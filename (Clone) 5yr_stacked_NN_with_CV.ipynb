{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d6a5ac-8bbf-47d4-822e-27b3f1b35460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NN MLP model baseline - 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-classifier\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets - custom join\n",
    "- get checkpoint data\n",
    "  - 5 year combined join, with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7b274b-c8d2-4949-9a8a-2b58e6a17564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/feature_eng\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/cv_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/training_splits/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/models/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/cv_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/feature_eng_ph3/training_splits/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4825eb9-9441-4e5d-a2c3-6adc6cea10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71052af-525f-4775-a2cc-79142b51f875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"DEP_DEL15\",\n",
    "    \"utc_timestamp\",\n",
    "    \"CRS_DEP_MINUTES\",            # feature eng start\n",
    "    \"prev_flight_delay_in_minutes\", \n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",        # feature eng end\n",
    "    'HourlyDryBulbTemperature',     # weather start\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed',               # weather end\n",
    "    # 'page_rank',               # phase 3 new features start\n",
    "    'out_degree',\n",
    "    'in_degree',\n",
    "    'weighted_out_degree',\n",
    "    'weighted_in_degree',\n",
    "    'N_RUNWAYS',\n",
    "    'betweenness_unweighted',\n",
    "    'closeness',\n",
    "    'betweenness',\n",
    "    'avg_origin_dep_delay',\n",
    "    'avg_dest_arr_delay',\n",
    "    'avg_daily_route_flights',\n",
    "    'avg_route_delay',\n",
    "    'avg_hourly_flights'               # phase 3 new features end\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff4688-854e-4a7f-9429-b74fdc925f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde39db-852d-4ce5-882d-a3f7f7cf3f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        # 'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights'               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70368be9-d8ed-4557-8841-52d5e1751acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be653215-8965-4f23-bc26-6db14cc0b802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Model Estimators ---\n",
    "preprocessing_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    assembler \n",
    "]\n",
    "\n",
    "# # A. XGBoost Regressor\n",
    "# xgb = SparkXGBRegressor(\n",
    "#     features_col=\"features\",\n",
    "#     label_col=\"DEP_DELAY_NEW\",\n",
    "#     num_workers=2, \n",
    "#     max_depth=6,\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.05\n",
    "# )\n",
    "\n",
    "# B. MLP Classifier\n",
    "# num_columns = 32\n",
    "# num_classes = 2\n",
    "\n",
    "# mlp = MultilayerPerceptronClassifier(\n",
    "#     featuresCol=\"features\",\n",
    "#     labelCol=\"DEP_DEL15\",\n",
    "#     # predictionCol=\"prediction\",\n",
    "#     maxIter=100,\n",
    "#     layers=[num_columns, num_columns//2, num_classes],\n",
    "#     blockSize=128,\n",
    "#     stepSize=0.03\n",
    "# )\n",
    "\n",
    "# --- Evaluator (Use one metric for optimization) ---\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" \n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"    \n",
    ")\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f2_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedFMeasure\"\n",
    ")\n",
    "f2_evaluator.setBeta(2.0)\n",
    "\n",
    "f2_evaluator_label = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"fMeasureByLabel\"\n",
    ")\n",
    "f2_evaluator_label.setMetricLabel(1.0).setBeta(2.0)\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36887f53-3436-46c9-bc0e-aaca9fe12246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    \"\"\"\n",
    "    Read a specific fold from partitioned parquet data.\n",
    "    Falls back to filtering if direct partition read fails.\n",
    "    \"\"\"\n",
    "    fold_path = f\"{path}/fold_id={fold_id}/split_type={split_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Try direct partition read\n",
    "        return spark.read.parquet(fold_path)\n",
    "    except:\n",
    "        # Fallback: read all data and filter\n",
    "        print(f\"Direct read failed for fold {fold_id}, using filter method...\")\n",
    "        all_data = spark.read.parquet(path)\n",
    "        return all_data.filter(\n",
    "            (all_data.fold_id == fold_id) & \n",
    "            (all_data.split_type == split_type)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e7ba9c-de95-4b40-8ee1-a0c4713f8f0b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765167891201}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_or_year = \"5_year_custom_joined\"\n",
    "# cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/cv_splits\" \n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\" \n",
    "val_fold = read_specific_fold(cv_path, 1, \"validation\").cache()\n",
    "import pyspark.sql.functions as F\n",
    "display(val_fold.groupby(\"DEP_DEL15\").agg(F.count(\"*\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2e3116-3f08-4b5b-ac61-f64ea4f5e339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # --- Training Loop for XGBoost with best hyperparams ---\n",
    "# n_folds = 10\n",
    "# month_or_year = \"5_year_custom_joined\"\n",
    "# cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\" \n",
    "\n",
    "# xgb = SparkXGBRegressor(\n",
    "#     features_col=\"features\",\n",
    "#     label_col=\"DEP_DELAY_NEW\",\n",
    "#     num_workers=12, \n",
    "#     tree_method=\"hist\",           \n",
    "#     max_depth=6,\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.05\n",
    "# )\n",
    "\n",
    "# with mlflow.start_run(run_name=\"XGB_BEST_HP_3_MNTH_CV\") as run:\n",
    "\n",
    "    \n",
    "#     # 1. Log Parameters\n",
    "#     mlflow.log_param(\"model\", \"XGBoost\")\n",
    "#     mlflow.log_param(\"max_depth\", 6)\n",
    "#     mlflow.log_param(\"n_estimators\", 100)\n",
    "#     mlflow.log_param(\"learning_rate\", 0.05)\n",
    "#     mlflow.log_param(\"dataset\", month_or_year)\n",
    "\n",
    "#     # 2. Create Pipeline\n",
    "#     pipeline = Pipeline(stages=preprocessing_stages + [xgb])\n",
    "    \n",
    "#     fold_metrics = {'train_mae': [], 'val_mae': [], 'train_rmse': [], 'val_rmse': []}\n",
    "    \n",
    "#     # 3. CV Loop\n",
    "#     for fold_id in range(1, n_folds + 1):\n",
    "#         with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True):\n",
    "#             print(f\"  Processing Fold {fold_id}/{n_folds}...\")\n",
    "            \n",
    "#             # Load Data\n",
    "#             train_fold = read_specific_fold(cv_path, fold_id, \"train\").cache()\n",
    "#             val_fold = read_specific_fold(cv_path, fold_id, \"validation\").cache()\n",
    "#             display(val_fold)\n",
    "            \n",
    "#             # Fit & Predict\n",
    "#             model = pipeline.fit(train_fold)\n",
    "#             train_preds = model.transform(train_fold)\n",
    "#             val_preds = model.transform(val_fold)\n",
    "            \n",
    "#             # Evaluate\n",
    "#             metrics = {\n",
    "#                 \"train_mae\": mae_evaluator.evaluate(train_preds),\n",
    "#                 \"val_mae\": mae_evaluator.evaluate(val_preds),\n",
    "#                 \"train_rmse\": rmse_evaluator.evaluate(train_preds),\n",
    "#                 \"val_rmse\": rmse_evaluator.evaluate(val_preds)\n",
    "#             }\n",
    "            \n",
    "#             # Log & Print\n",
    "#             mlflow.log_metrics(metrics)\n",
    "#             print(f\"    Fold {fold_id}: Val MAE={metrics['val_mae']:.4f}, Val RMSE={metrics['val_rmse']:.4f}\")\n",
    "            \n",
    "#             for k, v in metrics.items():\n",
    "#                 fold_metrics[k].append(v)\n",
    "            \n",
    "#             # Cleanup\n",
    "#             train_fold.unpersist()\n",
    "#             val_fold.unpersist()\n",
    "\n",
    "#     # 4. Log Aggregates\n",
    "#     avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "#     std_metrics = {f\"std_{k}\": np.std(v) for k, v in fold_metrics.items()}\n",
    "    \n",
    "#     mlflow.log_metrics({**avg_metrics, **std_metrics})\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(f\"Average Val MAE:  {avg_metrics['avg_val_mae']:.4f} (+/- {std_metrics['std_val_mae']:.4f})\")\n",
    "#     print(f\"Average Val RMSE: {avg_metrics['avg_val_rmse']:.4f} (+/- {std_metrics['std_val_rmse']:.4f})\")\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59bc036-2de1-46fd-9d01-bcd2cd04dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Aggressive Optimization Config ---\n",
    "OPTIMAL_PARTITIONS = 480  # 48 cores * 10 tasks\n",
    "n_folds = 10               # 5 folds is sufficient for 5-year data\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\"\n",
    "\n",
    "# --- 2. Safety UDF (Prevent Crashing on Inf/NaN) ---\n",
    "@F.udf(returnType=BooleanType())\n",
    "def vector_is_valid(v):\n",
    "    if v is None: return False\n",
    "    if np.any(np.isinf(v.values)): return False\n",
    "    if np.any(np.isnan(v.values)): return False\n",
    "    if np.max(np.abs(v.values)) > 1e30: return False\n",
    "    return True\n",
    "\n",
    "# --- 3. Pipeline Definition ---\n",
    "# Since this is the BASELINE, we process raw features (no XGBoost input)\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        # 'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights'               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "baseline_pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder,\n",
    "    assembler, \n",
    "    scaler\n",
    "])\n",
    "\n",
    "# --- 4. Global Loading & Preprocessing ---\n",
    "print(f\"Loading 20% Sample from {month_or_year}...\")\n",
    "\n",
    "# Load + Sample + Repartition\n",
    "full_cv_df = spark.read.parquet(cv_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20, seed=42) \\\n",
    "    .repartition(OPTIMAL_PARTITIONS) \\\n",
    "    .cache()\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline...\")\n",
    "feat_model = baseline_pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming, Cleaning & Persisting Vectors...\")\n",
    "# Transform -> Filter Invalid Vectors -> Persist to Disk\n",
    "featurized_df = feat_model.transform(full_cv_df) \\\n",
    "    .select(\"scaled_features\", \"DEP_DEL15\", \"fold_id\", \"split_type\") \\\n",
    "    .filter(vector_is_valid(F.col(\"scaled_features\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {featurized_df.count()} rows for Baseline Training.\")\n",
    "\n",
    "# --- 5. Training Loop (With Class Balancing) ---\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"f1\")\n",
    "f2_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedFMeasure\", beta=2.0)\n",
    "f2_label_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"fMeasureByLabel\", metricLabel=1.0, beta=2.0)\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedRecall\")\n",
    "\n",
    "print(f\"Starting Baseline MLP Training (Balanced)...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"MLP_BASELINE_5_YR_BALANCED_10CV\") as run:\n",
    "    \n",
    "    mlflow.log_param(\"model\", \"MLP Classifier\")\n",
    "    mlflow.log_param(\"type\", \"Baseline (No Stack)\")\n",
    "    mlflow.log_param(\"strategy\", \"Balanced Training + Global Prep\")\n",
    "    \n",
    "    fold_metrics = {\n",
    "        'train_f1': [], 'train_f2': [], 'train_f2_label': [], 'train_precision': [], 'train_recall': [],\n",
    "        'val_f1': [], 'val_f2': [], 'val_f2_label': [], 'val_precision': [], 'val_recall': []\n",
    "    }\n",
    "    \n",
    "    input_dim = len(featurized_df.first()[\"scaled_features\"])\n",
    "    print(f\"Detected Input Dimension: {input_dim}\")\n",
    "    mlflow.log_param(\"input_dim\", input_dim)\n",
    "\n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True):\n",
    "            print(f\"  Processing Fold {fold_id}/{n_folds}...\")\n",
    "            \n",
    "            # 1. Split Data\n",
    "            # Note: For baseline, we use the pre-defined 'split_type' from your CV generation\n",
    "            train_raw = featurized_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"train\"))\n",
    "            val_vec = featurized_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"validation\"))\n",
    "            \n",
    "            # 2. BALANCE TRAINING DATA (Critical Step)\n",
    "            train_pos = train_raw.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "            train_neg = train_raw.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "            \n",
    "            pos_count = train_pos.count()\n",
    "            neg_count = train_neg.count()\n",
    "            \n",
    "            # Downsample negatives to match positives\n",
    "            fraction = pos_count / neg_count\n",
    "            train_neg_sampled = train_neg.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "            \n",
    "            train_balanced = train_pos.union(train_neg_sampled).repartition(OPTIMAL_PARTITIONS)\n",
    "            \n",
    "            print(f\"    Fold {fold_id}: Training on Balanced Data ({pos_count} Pos)\")\n",
    "            \n",
    "            # 3. Define & Train MLP\n",
    "            layers = [input_dim, 64, 2] \n",
    "            mlp = MultilayerPerceptronClassifier(\n",
    "                featuresCol=\"scaled_features\",\n",
    "                labelCol=\"DEP_DEL15\",\n",
    "                maxIter=100,\n",
    "                layers=layers,\n",
    "                blockSize=128,\n",
    "                stepSize=0.03\n",
    "            )\n",
    "            \n",
    "            mlp_model = mlp.fit(train_balanced)\n",
    "            \n",
    "            # 4. Predict & Evaluate\n",
    "            # Predict on Training Set (Balanced)\n",
    "            train_preds = mlp_model.transform(train_balanced).select(\"prediction\", \"DEP_DEL15\").cache()\n",
    "            train_preds.count() # Materialize\n",
    "            \n",
    "            # Predict on Validation Set (Unbalanced/Real)\n",
    "            val_preds = mlp_model.transform(val_vec).select(\"prediction\", \"DEP_DEL15\").cache()\n",
    "            val_preds.count() # Materialize\n",
    "            \n",
    "            metrics = {\n",
    "                # Training Metrics\n",
    "                \"train_f1\": f1_evaluator.evaluate(train_preds),\n",
    "                \"train_f2\": f2_evaluator.evaluate(train_preds),\n",
    "                \"train_f2_label\": f2_label_evaluator.evaluate(train_preds),\n",
    "                \"train_precision\": precision_evaluator.evaluate(train_preds),\n",
    "                \"train_recall\": recall_evaluator.evaluate(train_preds),\n",
    "                \n",
    "                # Validation Metrics\n",
    "                \"val_f1\": f1_evaluator.evaluate(val_preds),\n",
    "                \"val_f2\": f2_evaluator.evaluate(val_preds),\n",
    "                \"val_f2_label\": f2_label_evaluator.evaluate(val_preds),\n",
    "                \"val_precision\": precision_evaluator.evaluate(val_preds),\n",
    "                \"val_recall\": recall_evaluator.evaluate(val_preds),\n",
    "            }\n",
    "            \n",
    "            mlflow.log_metrics(metrics)\n",
    "            print(f\"    Fold {fold_id}: Train F2-Delay={metrics['train_f2_label']:.4f}, Val F2-Delay={metrics['val_f2_label']:.4f}\")\n",
    "            \n",
    "            for k in fold_metrics.keys():\n",
    "                fold_metrics[k].append(metrics[k])\n",
    "            \n",
    "            val_preds.unpersist()\n",
    "\n",
    "    # Log Averages\n",
    "    avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "    mlflow.log_metrics(avg_metrics)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Average Train Delay-F2: {avg_metrics['avg_train_f2_label']:.4f}\")\n",
    "    print(f\"Average Val Delay-F2:   {avg_metrics['avg_val_f2_label']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Cleanup\n",
    "featurized_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9afbeb4-4a8e-444f-b880-c63561c9d05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# stacked approach\n",
    "- build XGBoost model and do hyperparamter tuning to find the best hyperparams\n",
    "- generate the XGBoost regression delay field and output it using the held out data\n",
    "- use the XGBoost delay field as the input for the NN/MLP model while doing hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b532a244-29b9-41e6-8b0e-63f353ba165b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "import itertools\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# --- 1. Config ---\n",
    "TRAIN_PARTITIONS = 6 \n",
    "n_folds = 10\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\"\n",
    "temp_materialize_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/temp_xgb_training\"\n",
    "\n",
    "# --- 2. Robust Safety Checks ---\n",
    "\n",
    "# UDF to check Feature Vectors for Inf/NaN\n",
    "@F.udf(returnType=BooleanType())\n",
    "def vector_is_valid(v):\n",
    "    if v is None: return False\n",
    "    # Check for Infinite values\n",
    "    if np.any(np.isinf(v.values)): return False\n",
    "    # Check for NaN\n",
    "    if np.any(np.isnan(v.values)): return False\n",
    "    # Check for extreme outliers (float32 stability)\n",
    "    if np.max(np.abs(v.values)) > 1e30: return False\n",
    "    return True\n",
    "\n",
    "# --- 3. Data Loading & Pipeline Definition ---\n",
    "print(f\"Loading and optimizing data from {month_or_year}...\")\n",
    "\n",
    "# Sample 10% (Stable)\n",
    "full_cv_df = spark.read.parquet(cv_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20) \\\n",
    "    .repartition(480) \\\n",
    "    .cache()\n",
    "\n",
    "# RE-DEFINE Assembler to ensure we output \"features\" (Standardizing name)\n",
    "assembler_xgb = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        # 'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights'               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"features\" # Explicit output name\n",
    ")\n",
    "\n",
    "# Re-build pipeline stages list to use this specific assembler\n",
    "# We exclude the Scaler (not needed for XGBoost)\n",
    "xgb_pipeline_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder,\n",
    "    assembler_xgb \n",
    "]\n",
    "\n",
    "global_pipeline = Pipeline(stages=xgb_pipeline_stages)\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline...\")\n",
    "feat_model = global_pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming & Cleaning...\")\n",
    "raw_transformed = feat_model.transform(full_cv_df)\n",
    "\n",
    "# CRITICAL FIX: Filter BOTH Features and Labels\n",
    "clean_df = raw_transformed \\\n",
    "    .select(\"features\", \"DEP_DELAY_NEW\", \"fold_id\", \"split_type\") \\\n",
    "    .filter(F.col(\"DEP_DELAY_NEW\").isNotNull()) \\\n",
    "    .filter(~F.isnan(F.col(\"DEP_DELAY_NEW\"))) \\\n",
    "    .filter(vector_is_valid(F.col(\"features\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {clean_df.count()} CLEAN rows for Tuning.\")\n",
    "\n",
    "# --- 4. Define Parameter Grid ---\n",
    "grid_search_params = {\n",
    "    \"max_depth\": [6], \n",
    "    \"n_estimators\": [100], \n",
    "    \"learning_rate\": [0.1]\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid_search_params.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "print(f\"Total Parameter Combinations: {len(param_combinations)}\")\n",
    "\n",
    "# --- 5. Run Tuning Loop (Fold-First) ---\n",
    "results_list = []\n",
    "fold_scores = {i: {\n",
    "    'train_mae': [], 'train_rmse': [],\n",
    "    'val_mae': [], 'val_rmse': []\n",
    "} for i in range(len(param_combinations))}\n",
    "\n",
    "print(f\"Starting Robust XGBoost Tuning...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGB_GRID_SEARCH_5_YR_10CV\") as parent_run:\n",
    "    mlflow.log_param(\"n_combinations\", len(param_combinations))\n",
    "    \n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        print(f\"\\n=== Processing Fold {fold_id}/{n_folds} ===\")\n",
    "        \n",
    "        # A. Materialize Clean Training Data\n",
    "        fold_train_path = f\"{temp_materialize_path}/fold_{fold_id}\"\n",
    "        print(f\"  Materializing training data to {fold_train_path}...\")\n",
    "        \n",
    "        # Write clean data\n",
    "        clean_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"train\")) \\\n",
    "            .repartition(TRAIN_PARTITIONS) \\\n",
    "            .write.mode(\"overwrite\").parquet(fold_train_path)\n",
    "            \n",
    "        # Read back\n",
    "        train_vec = spark.read.parquet(fold_train_path).repartition(TRAIN_PARTITIONS)\n",
    "        \n",
    "        val_vec = clean_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"validation\"))\n",
    "        \n",
    "        # INNER LOOP: Parameters\n",
    "        for idx, params in enumerate(param_combinations):\n",
    "            param_str = f\"depth{params['max_depth']}_est{params['n_estimators']}_lr{params['learning_rate']}\"\n",
    "            \n",
    "            xgb = SparkXGBRegressor(\n",
    "                features_col=\"features\",\n",
    "                label_col=\"DEP_DELAY_NEW\",\n",
    "                num_workers=6, \n",
    "                tree_method=\"hist\", \n",
    "                max_depth=params['max_depth'],\n",
    "                n_estimators=params['n_estimators'],\n",
    "                learning_rate=params['learning_rate'],\n",
    "                missing=0.0  # FIX: Explicitly handle sparse zeros/missing\n",
    "            )\n",
    "            \n",
    "            model = xgb.fit(train_vec)\n",
    "\n",
    "            # --- CALCULATE METRICS (Train vs Val) ---\n",
    "            \n",
    "            # 1. Training Metrics (Check for Overfitting)\n",
    "            train_preds = model.transform(train_vec)\n",
    "            t_mae = mae_evaluator.evaluate(train_preds)\n",
    "            t_rmse = rmse_evaluator.evaluate(train_preds)\n",
    "            \n",
    "            # 2. Validation Metrics (Generalization)\n",
    "            val_preds = model.transform(val_vec)\n",
    "            v_mae = mae_evaluator.evaluate(val_preds)\n",
    "            v_rmse = rmse_evaluator.evaluate(val_preds)\n",
    "            \n",
    "            # Store\n",
    "            fold_scores[idx]['train_mae'].append(t_mae)\n",
    "            fold_scores[idx]['train_rmse'].append(t_rmse)\n",
    "            \n",
    "            fold_scores[idx]['val_mae'].append(v_mae)\n",
    "            fold_scores[idx]['val_rmse'].append(v_rmse)\n",
    "            \n",
    "            print(f\"  Combo {idx+1}: Train MAE={t_mae:.2f} / Val MAE={v_mae:.2f}\")\n",
    "            print(f\"  Combo {idx+1}: Train RMSE={t_rmse:.2f} / Val RMSE={v_rmse:.2f}\")\n",
    "\n",
    "    # --- 6. Aggregate Results ---\n",
    "    print(\"\\n=== Aggregating Results ===\")\n",
    "    for idx, params in enumerate(param_combinations):\n",
    "        # Calculate Averages\n",
    "        avg_scores = {k: np.mean(v) for k, v in fold_scores[idx].items()}\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"combo_{idx}_summary\", nested=True):\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metrics(avg_scores)\n",
    "        \n",
    "        results_list.append({\n",
    "            **params,\n",
    "            \"avg_train_mae\": avg_scores['train_mae'],\n",
    "            \"avg_val_mae\": avg_scores['val_mae'],\n",
    "            \"avg_train_rmse\": avg_scores['train_rmse'],\n",
    "            \"avg_val_rmse\": avg_scores['val_rmse']\n",
    "        })\n",
    "        print(f\"Combo {idx+1} Final: Val MAE={avg_scores['val_mae']:.4f}, Train MAE={avg_scores['train_mae']:.4f}\")\n",
    "        print(f\"Combo {idx+1} Final: Val RMSE={avg_scores['val_rmse']:.4f}, Train RMSE={avg_scores['train_rmse']:.4f}\")\n",
    "\n",
    "# --- 8. Identify Best Parameters ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "best_row = results_df.loc[results_df['avg_val_mae'].idxmin()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"WINNER FOUND: {best_row.to_dict()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_depth = int(best_row['max_depth'])\n",
    "best_estimators = int(best_row['n_estimators'])\n",
    "best_lr = float(best_row['learning_rate'])\n",
    "\n",
    "# --- 9. Retrain & Log BEST Model ---\n",
    "print(\"Retraining Final Model with Best Parameters...\")\n",
    "\n",
    "# Define Final Model\n",
    "final_xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=6,\n",
    "    tree_method=\"hist\",\n",
    "    max_depth=best_depth,\n",
    "    n_estimators=best_estimators,\n",
    "    learning_rate=best_lr,\n",
    "    missing=0.0\n",
    ")\n",
    "\n",
    "# Pipeline includes the Assembler + Model (Indexers were pre-applied, but safer to include if needed for inference)\n",
    "# Since we used 'clean_df' which already has vectors, we fit on that directly.\n",
    "# BUT for a reusable model, we usually want the whole pipeline.\n",
    "# For simplicity here, we log the model trained on the processed vectors.\n",
    "final_model = final_xgb.fit(clean_df.repartition(TRAIN_PARTITIONS))\n",
    "\n",
    "with mlflow.start_run(run_name=\"FINAL_BEST_XGB_MODEL\") as run:\n",
    "    mlflow.log_params(best_row.to_dict())\n",
    "    mlflow.spark.log_model(final_model, \"model\")\n",
    "    print(f\"Final model saved to MLflow Run: {run.info.run_id}\")\n",
    "\n",
    "# Cleanup\n",
    "dbutils.fs.rm(temp_materialize_path, recurse=True)\n",
    "clean_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f492fbe-b9b0-47fe-88b1-69c48f9b24ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from functools import reduce\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# --- 1. Config ---\n",
    "# CRITICAL: Keep 1 task per worker (6 partitions)\n",
    "TRAIN_PARTITIONS = 6 \n",
    "n_folds = 10\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\"\n",
    "output_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/stacked_input_optimized\"\n",
    "\n",
    "# REPLACE WITH YOUR WINNING PARAMS FROM PHASE 1\n",
    "# best_depth = 6        \n",
    "# best_estimators = 100 \n",
    "# best_lr = 0.05        \n",
    "\n",
    "# --- 2. Safety UDF ---\n",
    "@F.udf(returnType=BooleanType())\n",
    "def vector_is_valid(v):\n",
    "    if v is None: return False\n",
    "    if np.any(np.isinf(v.values)): return False\n",
    "    if np.any(np.isnan(v.values)): return False\n",
    "    if np.max(np.abs(v.values)) > 1e30: return False\n",
    "    return True\n",
    "\n",
    "# --- 3. Evaluators ---\n",
    "# mae_evaluator = RegressionEvaluator(labelCol=\"DEP_DELAY_NEW\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "# rmse_evaluator = RegressionEvaluator(labelCol=\"DEP_DELAY_NEW\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# r2_evaluator = RegressionEvaluator(labelCol=\"DEP_DELAY_NEW\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# --- 4. Global Data Loading ---\n",
    "print(f\"Loading 5% Sample from {month_or_year}...\")\n",
    "\n",
    "# 5% Sample (0.05) is the proven stable size\n",
    "full_cv_df = spark.read.parquet(cv_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20) \\\n",
    "    .repartition(480) \\\n",
    "    .cache()\n",
    "\n",
    "# --- 5. Feature Pipeline ---\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "\n",
    "assembler_xgb = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\", \"MONTH\", \"YEAR\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\", \"origin_vec\", \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"CRS_DEP_MINUTES\",\n",
    "        \"prev_flight_delay_in_minutes\", \"prev_flight_delay\", \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\", \"delay_origin_carrier_7d\", \"delay_route_7d\",\n",
    "        \"flight_count_24h\", \"LANDING_TIME_DIFF_MINUTES\", \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\", 'HourlyDryBulbTemperature', 'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity', 'HourlyAltimeterSetting', 'HourlyVisibility',\n",
    "        'HourlyStationPressure', 'HourlyWetBulbTemperature', 'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage', 'HourlyCloudElevation', 'HourlyWindSpeed',\n",
    "        'out_degree', 'in_degree', 'weighted_out_degree', 'weighted_in_degree',\n",
    "        'N_RUNWAYS', 'betweenness_unweighted', 'closeness', 'betweenness',\n",
    "        'avg_origin_dep_delay', 'avg_dest_arr_delay', 'avg_daily_route_flights',\n",
    "        'avg_route_delay', 'avg_hourly_flights'\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder,\n",
    "    assembler_xgb \n",
    "])\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline...\")\n",
    "feat_model = pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming & Cleaning...\")\n",
    "clean_df = feat_model.transform(full_cv_df) \\\n",
    "    .filter(F.col(\"DEP_DELAY_NEW\").isNotNull()) \\\n",
    "    .filter(~F.isnan(F.col(\"DEP_DELAY_NEW\"))) \\\n",
    "    .filter(vector_is_valid(F.col(\"features\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {clean_df.count()} CLEAN rows for Stacking.\")\n",
    "\n",
    "# --- 6. Stacking Generation Loop ---\n",
    "out_of_fold_predictions = []\n",
    "# Updated history to track both Train and Val\n",
    "metrics_history = {\n",
    "    'train_mae': [], 'train_rmse': [],\n",
    "    'val_mae': [], 'val_rmse': []\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting Stacked Feature Generation (Depth={best_depth}, Est={best_estimators})...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGB_BEST_5_YR_10CV\") as run:\n",
    "    # Log Parameters\n",
    "    mlflow.log_param(\"xgb_max_depth\", best_depth)\n",
    "    mlflow.log_param(\"xgb_n_estimators\", best_estimators)\n",
    "    mlflow.log_param(\"xgb_learning_rate\", best_lr)\n",
    "    mlflow.log_param(\"sample_fraction\", 0.20)\n",
    "    \n",
    "    # Define Base Model\n",
    "    xgb = SparkXGBRegressor(\n",
    "        features_col=\"features\",\n",
    "        label_col=\"DEP_DELAY_NEW\",\n",
    "        num_workers=6,\n",
    "        tree_method=\"hist\", \n",
    "        max_depth=best_depth,\n",
    "        n_estimators=best_estimators,\n",
    "        learning_rate=best_lr,\n",
    "        missing=0.0\n",
    "    )\n",
    "\n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        print(f\"\\nProcessing Fold {fold_id}/{n_folds}...\")\n",
    "        \n",
    "        # 1. Prepare Training Data\n",
    "        train_vec = clean_df.filter((F.col(\"fold_id\") != fold_id)) \\\n",
    "            .sample(withReplacement=False, fraction=0.20) \\\n",
    "            .repartition(TRAIN_PARTITIONS)\n",
    "        \n",
    "        # 2. Prepare Validation Data\n",
    "        val_vec = clean_df.filter((F.col(\"fold_id\") == fold_id))\n",
    "        \n",
    "        # 3. Fit Model\n",
    "        model = xgb.fit(train_vec)\n",
    "        \n",
    "        # 4. TRAINING METRICS (New)\n",
    "        train_preds = model.transform(train_vec)\n",
    "        t_mae = mae_evaluator.evaluate(train_preds)\n",
    "        t_rmse = rmse_evaluator.evaluate(train_preds)\n",
    "        \n",
    "        metrics_history['train_mae'].append(t_mae)\n",
    "        metrics_history['train_rmse'].append(t_rmse)\n",
    "        \n",
    "        # 5. VALIDATION METRICS\n",
    "        val_preds = model.transform(val_vec)\n",
    "        v_mae = mae_evaluator.evaluate(val_preds)\n",
    "        v_rmse = rmse_evaluator.evaluate(val_preds)\n",
    "        \n",
    "        metrics_history['val_mae'].append(v_mae)\n",
    "        metrics_history['val_rmse'].append(v_rmse)\n",
    "        \n",
    "        print(f\"  Fold {fold_id}: Train MAE={t_mae:.2f} | Val MAE={v_mae:.2f}\")\n",
    "        print(f\"  Fold {fold_id}: Train RMSE={t_rmse:.2f} | Val RMSE={v_rmse:.2f}\")\n",
    "        \n",
    "        # Log Fold Metrics\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_train_mae\", t_mae)\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_val_mae\", v_mae)\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_train_rmse\", t_rmse)\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_val_rmse\", v_rmse)\n",
    "        \n",
    "        # 6. Format Output (Drop features to save space)\n",
    "        val_preds_clean = val_preds \\\n",
    "            .withColumnRenamed(\"prediction\", \"xgb_predicted_delay\") \\\n",
    "            .drop(\"features\") \n",
    "        \n",
    "        out_of_fold_predictions.append(val_preds_clean)\n",
    "\n",
    "    # Log Average Metrics\n",
    "    mlflow.log_metric(\"avg_train_mae\", np.mean(metrics_history['train_mae']))\n",
    "    mlflow.log_metric(\"avg_train_rmse\", np.mean(metrics_history['train_rmse']))\n",
    "    mlflow.log_metric(\"avg_val_mae\", np.mean(metrics_history['val_mae']))\n",
    "    mlflow.log_metric(\"avg_val_rmse\", np.mean(metrics_history['val_rmse']))\n",
    "    \n",
    "    # --- 7. Train & Log FINAL Model ---\n",
    "    print(\"\\nTraining Final XGBoost Model on Full Data...\")\n",
    "    \n",
    "    final_train = clean_df.repartition(TRAIN_PARTITIONS)\n",
    "    final_model = xgb.fit(final_train)\n",
    "    \n",
    "    mlflow.spark.log_model(final_model, \"stacked_xgb_model\")\n",
    "    print(f\"Final Model Saved to MLflow Run: {run.info.run_id}\")\n",
    "\n",
    "    # --- 8. Save Stacked Dataset ---\n",
    "    print(\"\\nUnioning and Saving Stacked Dataset...\")\n",
    "    \n",
    "    stacked_dataset = reduce(lambda df1, df2: df1.union(df2), out_of_fold_predictions)\n",
    "\n",
    "    cols_to_drop = [\"carrier_vec\", \"origin_vec\", \"dest_vec\"]\n",
    "    final_output = stacked_dataset.drop(*cols_to_drop)\n",
    "\n",
    "    final_output.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"SUCCESS: Stacked dataset saved to {output_path}\")\n",
    "\n",
    "# Cleanup\n",
    "clean_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cbc07f9-23ac-4edb-875d-a6b54be1c0a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Optimization Config ---\n",
    "# TUNING: 48 cores * 10 tasks = 480 Partitions\n",
    "OPTIMAL_PARTITIONS = 480 \n",
    "n_folds = 10  # 5 Folds is the sweet spot for this volume of data\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "stacked_input_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/stacked_input_optimized\"\n",
    "\n",
    "# --- 2. Global Data Loading & Preprocessing ---\n",
    "print(f\"Loading and optimizing data from {month_or_year}...\")\n",
    "\n",
    "# Sample 20% to speed up architecture search (approx 25M rows)\n",
    "# We use the same seed=42 to match previous steps\n",
    "full_cv_df = spark.read.parquet(stacked_input_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20) \\\n",
    "    .repartition(OPTIMAL_PARTITIONS) \\\n",
    "    .cache()\n",
    "\n",
    "# Define Pipeline Stages\n",
    "# carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "# origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "# dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        # 'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights',\n",
    "        \"xgb_predicted_delay\"  # <--- The Stacking Feature\n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Build & Fit Pipeline ONCE\n",
    "global_pipeline = Pipeline(stages=[\n",
    "    # carrier_indexer, origin_indexer, dest_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder,\n",
    "    assembler,\n",
    "    scaler \n",
    "])\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline (One-Time Fit)...\")\n",
    "feat_model = global_pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming & Caching Vectors...\")\n",
    "# Persist to DISK to play it safe with memory -- I'm going to cache actually\n",
    "featurized_df = feat_model.transform(full_cv_df) \\\n",
    "    .select(\"scaled_features\", \"DEP_DEL15\", \"fold_id\", \"split_type\") \\\n",
    "    .cache()\n",
    "\n",
    "print(f\"Materialized {featurized_df.count()} rows for Architecture Search.\")\n",
    "\n",
    "# --- 3. Define Architectures ---\n",
    "hidden_layer_grid = [\n",
    "    [32],               \n",
    "    [64, 32],           \n",
    "    [128, 64],          \n",
    "    [64, 32, 16]\n",
    "]\n",
    "\n",
    "# --- 4. Evaluators ---\n",
    "# TARGET: F2 Score for the DELAY Class (1.0)\n",
    "# f2_label_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"fMeasureByLabel\")\n",
    "# f2_label_evaluator.setMetricLabel(1.0)\n",
    "# f2_label_evaluator.setBeta(2.0)\n",
    "\n",
    "# --- 5. Tuning Loop ---\n",
    "results_list = []\n",
    "\n",
    "print(f\"Starting MLP Architecture Search (Target: Delay-Class F2)...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"MLP_5YR_STACKED_TUNING_10CV\") as parent_run:\n",
    "    \n",
    "    # Calculate Input Dimension once\n",
    "    input_dim = len(featurized_df.first()[\"scaled_features\"])\n",
    "    print(f\"Detected Input Dimension: {input_dim}\")\n",
    "    \n",
    "    for idx, hidden_config in enumerate(hidden_layer_grid):\n",
    "        \n",
    "        config_str = \"-\".join(map(str, hidden_config)) \n",
    "        print(f\"\\n--- Testing Arch {idx+1}/{len(hidden_layer_grid)}: Hidden=[{config_str}] ---\")\n",
    "        \n",
    "        # Track all metrics for this architecture\n",
    "        fold_metrics = {\n",
    "            'val_f1': [], 'val_f2': [], 'val_f2_label': [], \n",
    "            'val_precision': [], 'val_recall': []\n",
    "        }\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"arch_{config_str}\", nested=True) as child_run:\n",
    "            mlflow.log_param(\"hidden_layers\", str(hidden_config))\n",
    "            \n",
    "            for fold_id in range(1, n_folds + 1):\n",
    "                # FILTER instead of FIT (Fast!)\n",
    "                # We use the 'fold_id' column to grab the pre-processed chunks\n",
    "                train_vec = featurized_df.filter((F.col(\"fold_id\") != fold_id))\n",
    "                val_vec = featurized_df.filter((F.col(\"fold_id\") == fold_id))\n",
    "                \n",
    "                # Dynamic Layers\n",
    "                full_layers = [input_dim] + hidden_config + [2]\n",
    "                \n",
    "                # Define MLP\n",
    "                mlp = MultilayerPerceptronClassifier(\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    labelCol=\"DEP_DEL15\",\n",
    "                    layers=full_layers,\n",
    "                    blockSize=128,\n",
    "                    maxIter=100,\n",
    "                    stepSize=0.03\n",
    "                )\n",
    "                \n",
    "                # Train\n",
    "                mlp_model = mlp.fit(train_vec)\n",
    "                \n",
    "                # Predict\n",
    "                val_preds = mlp_model.transform(val_vec).select(\"prediction\", \"DEP_DEL15\")\n",
    "                \n",
    "                # Evaluate (All Metrics)\n",
    "                metrics = {\n",
    "                    \"val_f1\": f1_evaluator.evaluate(val_preds),\n",
    "                    \"val_f2\": f2_evaluator.evaluate(val_preds),\n",
    "                    \"val_f2_label\": f2_evaluator_label.evaluate(val_preds),\n",
    "                    \"val_precision\": precision_evaluator.evaluate(val_preds),\n",
    "                    \"val_recall\": recall_evaluator.evaluate(val_preds),\n",
    "                }\n",
    "                \n",
    "                # Store\n",
    "                for k in fold_metrics.keys():\n",
    "                    fold_metrics[k].append(metrics[k])\n",
    "                \n",
    "                print(f\"    Fold {fold_id}: F1={metrics['val_f1']:.4f}, Delay-F2={metrics['val_f2_label']:.4f}\")\n",
    "            \n",
    "            # Aggregate Results\n",
    "            avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "            mlflow.log_metrics(avg_metrics)\n",
    "            \n",
    "            # We select the winner based on Delay-Class F2\n",
    "            results_list.append({\n",
    "                \"hidden_config\": str(hidden_config),\n",
    "                \"avg_val_f2_label\": avg_metrics['avg_val_f2_label'], # Winner metric\n",
    "                \"avg_val_f1\": avg_metrics['avg_val_f1'],\n",
    "                \"avg_val_f2\": avg_metrics['avg_val_f2']\n",
    "            })\n",
    "            \n",
    "            print(f\"  Arch [{config_str}] Result: Avg Delay F2 = {avg_metrics['avg_val_f2_label']:.4f}\")\n",
    "\n",
    "# --- 6. Results ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "# Winner selected by F2 Label (Delay Class)\n",
    "best_row = results_df.loc[results_df['avg_val_f2_label'].idxmax()] \n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOP 3 ARCHITECTURES (Sorted by Delay-Class F2):\")\n",
    "print(results_df.sort_values(\"avg_val_f2_label\", ascending=False).head(3))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cleanup\n",
    "featurized_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcc6ee5-06bd-48e2-a877-355009bd9514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Optimization Config ---\n",
    "OPTIMAL_PARTITIONS = 480 \n",
    "# We use this for repartitioning inside the loop to keep tasks small and fast for MLP\n",
    "TRAIN_PARTITIONS = 480 \n",
    "n_folds = 10 \n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "stacked_input_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/stacked_input_optimized\"\n",
    "\n",
    "# --- 2. Safety UDF ---\n",
    "@F.udf(returnType=BooleanType())\n",
    "def vector_is_valid(v):\n",
    "    if v is None: return False\n",
    "    if np.any(np.isinf(v.values)): return False\n",
    "    if np.any(np.isnan(v.values)): return False\n",
    "    if np.max(np.abs(v.values)) > 1e30: return False\n",
    "    return True\n",
    "\n",
    "# --- 3. Global Data Loading & Preprocessing ---\n",
    "print(f\"Loading and optimizing data from {month_or_year}...\")\n",
    "\n",
    "# Sample 20%\n",
    "full_cv_df = spark.read.parquet(stacked_input_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20) \\\n",
    "    .repartition(OPTIMAL_PARTITIONS) \\\n",
    "    .cache()\n",
    "\n",
    "# Define Pipeline Stages\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\", \"MONTH\", \"YEAR\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\", \"origin_vec\", \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"CRS_DEP_MINUTES\",\n",
    "        \"prev_flight_delay_in_minutes\", \"prev_flight_delay\", \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\", \"delay_origin_carrier_7d\", \"delay_route_7d\",\n",
    "        \"flight_count_24h\", \"LANDING_TIME_DIFF_MINUTES\", \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\", 'HourlyDryBulbTemperature', 'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity', 'HourlyAltimeterSetting', 'HourlyVisibility',\n",
    "        'HourlyStationPressure', 'HourlyWetBulbTemperature', 'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage', 'HourlyCloudElevation', 'HourlyWindSpeed',\n",
    "        'out_degree', 'in_degree', 'weighted_out_degree', 'weighted_in_degree',\n",
    "        'N_RUNWAYS', 'betweenness_unweighted', 'closeness', 'betweenness',\n",
    "        'avg_origin_dep_delay', 'avg_dest_arr_delay', 'avg_daily_route_flights',\n",
    "        'avg_route_delay', 'avg_hourly_flights',\n",
    "        \"xgb_predicted_delay\" \n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Build & Fit Pipeline ONCE\n",
    "global_pipeline = Pipeline(stages=[\n",
    "    carrier_encoder, origin_encoder, dest_encoder,\n",
    "    assembler,\n",
    "    scaler \n",
    "])\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline (One-Time Fit)...\")\n",
    "feat_model = global_pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming & Caching Vectors...\")\n",
    "featurized_df = feat_model.transform(full_cv_df) \\\n",
    "    .select(\"scaled_features\", \"DEP_DEL15\", \"fold_id\", \"split_type\") \\\n",
    "    .filter(vector_is_valid(F.col(\"scaled_features\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {featurized_df.count()} rows for Architecture Search.\")\n",
    "\n",
    "# --- 4. Define Architectures ---\n",
    "hidden_layer_grid = [\n",
    "    # [32],               \n",
    "    [64, 32],           \n",
    "    [128, 64],          \n",
    "    [64, 32, 16]\n",
    "]\n",
    "\n",
    "# --- 5. Evaluators ---\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"f1\")\n",
    "f2_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedFMeasure\", beta=2.0)\n",
    "f2_label_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"fMeasureByLabel\", metricLabel=1.0, beta=2.0)\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedRecall\")\n",
    "\n",
    "# --- 6. Tuning Loop ---\n",
    "results_list = []\n",
    "\n",
    "print(f\"Starting MLP Architecture Search (Target: Delay-Class F2)...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"MLP_5YR_STACKED_TUNING_BALANCED_FULL_METRICS\") as parent_run:\n",
    "    \n",
    "    input_dim = len(featurized_df.first()[\"scaled_features\"])\n",
    "    print(f\"Detected Input Dimension: {input_dim}\")\n",
    "    mlflow.log_param(\"input_dim\", input_dim)\n",
    "    \n",
    "    for idx, hidden_config in enumerate(hidden_layer_grid):\n",
    "        \n",
    "        config_str = \"-\".join(map(str, hidden_config)) \n",
    "        print(f\"\\n--- Testing Arch {idx+1}/{len(hidden_layer_grid)}: Hidden=[{config_str}] ---\")\n",
    "        \n",
    "        # Track both Train and Val metrics\n",
    "        fold_metrics = {\n",
    "            'train_f1': [], 'train_f2': [], 'train_f2_label': [], 'train_precision': [], 'train_recall': [],\n",
    "            'val_f1': [], 'val_f2': [], 'val_f2_label': [], 'val_precision': [], 'val_recall': []\n",
    "        }\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"arch_{config_str}\", nested=True) as child_run:\n",
    "            mlflow.log_param(\"hidden_layers\", str(hidden_config))\n",
    "            \n",
    "            for fold_id in range(1, n_folds + 1):\n",
    "                # 1. Split Data\n",
    "                train_raw = featurized_df.filter((F.col(\"fold_id\") != fold_id))\n",
    "                val_vec = featurized_df.filter((F.col(\"fold_id\") == fold_id))\n",
    "                \n",
    "                # 2. BALANCE TRAINING DATA\n",
    "                train_pos = train_raw.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "                train_neg = train_raw.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "                \n",
    "                pos_count = train_pos.count()\n",
    "                neg_count = train_neg.count()\n",
    "                fraction = pos_count / neg_count\n",
    "                \n",
    "                train_neg_sampled = train_neg.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "                train_balanced = train_pos.union(train_neg_sampled).repartition(TRAIN_PARTITIONS)\n",
    "                \n",
    "                print(f\"    Fold {fold_id}: Training on Balanced Data ({pos_count} Pos)\")\n",
    "                \n",
    "                # 3. Define & Fit\n",
    "                full_layers = [input_dim] + hidden_config + [2]\n",
    "                \n",
    "                mlp = MultilayerPerceptronClassifier(\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    labelCol=\"DEP_DEL15\",\n",
    "                    layers=full_layers,\n",
    "                    blockSize=128,\n",
    "                    maxIter=100,\n",
    "                    stepSize=0.03\n",
    "                )\n",
    "                \n",
    "                mlp_model = mlp.fit(train_balanced)\n",
    "                \n",
    "                # 4. Predict & Evaluate (TRAIN - Check Overfitting)\n",
    "                train_preds = mlp_model.transform(train_balanced).select(\"prediction\", \"DEP_DEL15\")\n",
    "                \n",
    "                # 5. Predict & Evaluate (VAL - Check Performance)\n",
    "                val_preds = mlp_model.transform(val_vec).select(\"prediction\", \"DEP_DEL15\")\n",
    "                \n",
    "                metrics = {\n",
    "                    \"val_f1\": f1_evaluator.evaluate(val_preds),\n",
    "                    \"val_f2\": f2_evaluator.evaluate(val_preds),\n",
    "                    \"val_f2_label\": f2_label_evaluator.evaluate(val_preds),\n",
    "                    \"val_precision\": precision_evaluator.evaluate(val_preds),\n",
    "                    \"val_recall\": recall_evaluator.evaluate(val_preds),\n",
    "                    \"train_f1\": f1_evaluator.evaluate(train_preds),\n",
    "                    \"train_f2\": f2_evaluator.evaluate(train_preds),\n",
    "                    \"train_f2_label\": f2_label_evaluator.evaluate(train_preds),\n",
    "                    \"train_precision\": precision_evaluator.evaluate(train_preds),\n",
    "                    \"train_recall\": recall_evaluator.evaluate(train_preds)\n",
    "                }\n",
    "                \n",
    "                # Log fold metrics\n",
    "                mlflow.log_metrics(metrics)\n",
    "                \n",
    "                print(f\"    Result: Train F2-Delay={metrics['train_f2_label']:.4f} | Val F2-Delay={metrics['val_f2_label']:.4f}\")\n",
    "                \n",
    "                for k in fold_metrics.keys():\n",
    "                    fold_metrics[k].append(metrics[k])\n",
    "            \n",
    "            # Aggregate Results\n",
    "            avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "            mlflow.log_metrics(avg_metrics)\n",
    "            \n",
    "            results_list.append({\n",
    "                \"hidden_config\": str(hidden_config),\n",
    "                \"avg_train_f2_label\": avg_metrics['avg_train_f2_label'],\n",
    "                \"avg_val_f2_label\": avg_metrics['avg_val_f2_label']\n",
    "            })\n",
    "            \n",
    "            print(f\"  Arch [{config_str}] Final Train Delay F2 = {avg_metrics['avg_train_f2_label']:.4f}\")\n",
    "            print(f\"  Arch [{config_str}] Final Val Delay F2 = {avg_metrics['avg_val_f2_label']:.4f}\")\n",
    "\n",
    "# --- 7. Select Winner & Log Final Model ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "best_row = results_df.loc[results_df['avg_val_f2_label'].idxmax()] \n",
    "best_config_str = best_row['hidden_config'] # e.g. \"[64, 32]\"\n",
    "best_config_list = eval(best_config_str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"WINNER FOUND: {best_config_str}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Retraining Final Stacked Model...\")\n",
    "\n",
    "# 1. Balance Full Dataset (Downsample global majority)\n",
    "full_pos = featurized_df.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "full_neg = featurized_df.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "fraction = full_pos.count() / full_neg.count()\n",
    "\n",
    "# Re-combine to get the Training Set for the Final Model\n",
    "full_balanced = full_pos.union(full_neg.sample(False, fraction, seed=42)).repartition(OPTIMAL_PARTITIONS)\n",
    "\n",
    "# 2. Train Final Model\n",
    "final_layers = [input_dim] + best_config_list + [2]\n",
    "final_mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    layers=final_layers,\n",
    "    blockSize=128,\n",
    "    maxIter=100,\n",
    "    stepSize=0.03\n",
    ")\n",
    "final_model = final_mlp.fit(full_balanced)\n",
    "\n",
    "# --- NEW: Calculate Final Training Metrics ---\n",
    "print(\"Calculating Final Training Metrics...\")\n",
    "final_train_preds = final_model.transform(full_balanced).select(\"prediction\", \"DEP_DEL15\").cache()\n",
    "final_train_preds.count() # Materialize\n",
    "\n",
    "final_metrics = {\n",
    "    \"final_train_f1\": f1_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_f2\": f2_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_f2_label\": f2_label_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_precision\": precision_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_recall\": recall_evaluator.evaluate(final_train_preds)\n",
    "}\n",
    "\n",
    "print(f\"Final Model Training F2 (Delay Class): {final_metrics['final_train_f2_label']:.4f}\")\n",
    "\n",
    "# 3. Log to MLflow\n",
    "with mlflow.start_run(run_name=\"FINAL_BEST_STACKED_MLP\") as run:\n",
    "    # Log Params\n",
    "    mlflow.log_param(\"hidden_layers\", best_config_str)\n",
    "    \n",
    "    # Log Metrics (NEW)\n",
    "    mlflow.log_metrics(final_metrics)\n",
    "    \n",
    "    # Log Model Artifact\n",
    "    mlflow.spark.log_model(final_model, \"model\")\n",
    "    print(f\"Final Stacked Model & Metrics saved to Run: {run.info.run_id}\")\n",
    "\n",
    "# Cleanup\n",
    "final_train_preds.unpersist()\n",
    "featurized_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb8d698-0b97-4352-8a5a-36eb3aca083b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# running on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c94aff-d33e-44f4-8c12-730e1c0f6cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.regression import GBTRegressor # Using GBT as SparkXGB placeholder if needed\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, StringIndexerModel\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Update this to your actual Test Data path\n",
    "TEST_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet/\" \n",
    "TRAIN_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/cv_splits\"\n",
    "STACKED_TRAIN_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/stacked_input_optimized\"\n",
    "\n",
    "# BEST HYPERPARAMETERS (Fill these in from your previous runs!)\n",
    "BEST_XGB_DEPTH = 6\n",
    "BEST_XGB_ESTIMATORS = 100\n",
    "BEST_XGB_LR = 0.1\n",
    "BEST_MLP_LAYERS = [128, 64] \n",
    "\n",
    "# CLUSTER CONFIG\n",
    "OPTIMAL_PARTITIONS = 480\n",
    "\n",
    "print(\"--- STARTING FINAL EVALUATION ---\")\n",
    "\n",
    "# --- 2. Load & Prepare Data ---\n",
    "print(\"Loading Training Data (for Pipeline fitting)...\")\n",
    "# We use the same 20% sample seed to ensure indices match your previous work\n",
    "train_df = spark.read.parquet(TRAIN_PATH).sample(False, 0.20)\n",
    "\n",
    "print(f\"Loading Test Data from {TEST_PATH}...\")\n",
    "test_df = spark.read.parquet(TEST_PATH).repartition(OPTIMAL_PARTITIONS).cache()\n",
    "\n",
    "# --- 3. Build & Fit Feature Pipeline (Indexers/Encoders) ---\n",
    "# We must fit this on TRAIN data, then apply to TEST data.\n",
    "print(\"Fitting Feature Engineering Pipeline...\")\n",
    "\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "\n",
    "# Base Assembler (For XGBoost & Baseline)\n",
    "base_assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        # 'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights'               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Fit pipeline on TRAIN\n",
    "fe_pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder,\n",
    "    base_assembler\n",
    "])\n",
    "fe_model = fe_pipeline.fit(train_df)\n",
    "\n",
    "# Transform BOTH Train and Test\n",
    "print(\"Transforming Data...\")\n",
    "train_vec = fe_model.transform(train_df).select(\"features\", \"DEP_DELAY_NEW\", \"DEP_DEL15\")\n",
    "test_vec = fe_model.transform(test_df).cache()\n",
    "\n",
    "# --- 4. Final Evaluation: STACKED MODEL ---\n",
    "print(\"\\n--- EVALUATING STACKED MODEL (XGB + MLP) ---\")\n",
    "\n",
    "# A. Train Final XGBoost on Full Training Data\n",
    "print(\"Training Final XGBoost Regressor...\")\n",
    "xgb_final = SparkXGBRegressor(\n",
    "    features_col=\"features\", \n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=6, \n",
    "    tree_method=\"hist\",\n",
    "    max_depth=BEST_XGB_DEPTH, \n",
    "    n_estimators=BEST_XGB_ESTIMATORS, \n",
    "    learning_rate=BEST_XGB_LR,\n",
    "    missing=0.0\n",
    ")\n",
    "xgb_model = xgb_final.fit(train_vec)\n",
    "\n",
    "# B. Generate XGB Feature for TEST Data\n",
    "print(\"Generating XGB Predictions for Test Set...\")\n",
    "test_with_xgb = xgb_model.transform(test_vec) \\\n",
    "    .withColumnRenamed(\"prediction\", \"xgb_predicted_delay\")\n",
    "\n",
    "# C. Assemble MLP Features (Original + XGB Prediction)\n",
    "mlp_assembler = VectorAssembler(\n",
    "    inputCols=[\"features\", \"xgb_predicted_delay\"], \n",
    "    outputCol=\"raw_mlp_features\"\n",
    ")\n",
    "# We need to assemble, then scale\n",
    "test_mlp_ready = mlp_assembler.transform(test_with_xgb)\n",
    "\n",
    "# D. Train Final MLP on Stacked Training Data (Loaded from Phase 2)\n",
    "print(\"Loading Stacked Training Data...\")\n",
    "stacked_train = spark.read.parquet(STACKED_TRAIN_PATH).repartition(OPTIMAL_PARTITIONS)\n",
    "\n",
    "# E. Fit Scaler on Stacked Train\n",
    "print(\"Fitting Scaler...\")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "# Note: stacked_train already has 'raw_features' from Phase 2? \n",
    "# Wait, Phase 2 output had 'features' dropped and kept 'carrier_idx' etc.\n",
    "# We need to ensure schema matches.\n",
    "# SIMPLIFICATION: We will refit the pipeline on stacked_train to be safe.\n",
    "# Actually, stacked_input_optimized usually contains indices.\n",
    "# Let's assume we need to rebuild the vector for stacked_train too.\n",
    "\n",
    "# CORRECT APPROACH:\n",
    "# 1. Load Phase 2 output (has indices + xgb_predicted_delay)\n",
    "# 2. Run Encoder + Assembler + Scaler\n",
    "stacked_pipeline = Pipeline(stages=[\n",
    "    carrier_encoder, origin_encoder, dest_encoder, # Reuse encoders\n",
    "    # Assembler that includes xgb_predicted_delay\n",
    "    VectorAssembler(\n",
    "        inputCols=base_assembler.getInputCols() + [\"xgb_predicted_delay\"],\n",
    "        outputCol=\"raw_mlp_features\"\n",
    "    ),\n",
    "    scaler\n",
    "])\n",
    "stacked_model = stacked_pipeline.fit(stacked_train)\n",
    "stacked_train_vec = stacked_model.transform(stacked_train)\n",
    "\n",
    "# Transform Test Data using the SAME pipeline (Scaler fitted on train)\n",
    "# We need to prepare test data to match stacked_train schema first\n",
    "# Test data has indices (from fe_model) and xgb_predicted_delay.\n",
    "test_mlp_final = stacked_model.transform(test_with_xgb)\n",
    "\n",
    "# F. Train MLP\n",
    "print(\"Training Final MLP Classifier...\")\n",
    "input_dim = len(stacked_train_vec.first()[\"scaled_features\"])\n",
    "full_layers = [input_dim] + BEST_MLP_LAYERS + [2]\n",
    "\n",
    "mlp_final = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    layers=full_layers, \n",
    "    blockSize=128, \n",
    "    maxIter=100\n",
    ")\n",
    "mlp_model = mlp_final.fit(stacked_train_vec)\n",
    "\n",
    "# G. Evaluate\n",
    "print(\"Evaluating Stacked Model on Test Set...\")\n",
    "preds = mlp_model.transform(test_mlp_final)\n",
    "\n",
    "f2_eval = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"fMeasureByLabel\", metricLabel=1.0, beta=2.0)\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"f1\")\n",
    "\n",
    "print(f\"FINAL TEST RESULTS (Stacked):\")\n",
    "print(f\"  Delay-Class F2: {f2_eval.evaluate(preds):.4f}\")\n",
    "print(f\"  Weighted F1:    {f1_eval.evaluate(preds):.4f}\")\n",
    "\n",
    "# --- 5. Evaluation: BASELINE MLP ---\n",
    "print(\"\\n--- EVALUATING BASELINE MLP ---\")\n",
    "\n",
    "# A. Prepare Data (Raw Features -> Scaled)\n",
    "# Reuse the scaler from stacked pipeline but applied to base features\n",
    "base_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features_base\", withStd=True, withMean=False)\n",
    "base_pipe = Pipeline(stages=[base_scaler])\n",
    "base_model = base_pipe.fit(train_vec)\n",
    "\n",
    "train_base_vec = base_model.transform(train_vec)\n",
    "test_base_vec = base_model.transform(test_vec)\n",
    "\n",
    "# B. Train Baseline MLP\n",
    "print(\"Training Baseline MLP...\")\n",
    "input_dim_base = len(train_base_vec.first()[\"scaled_features_base\"])\n",
    "base_layers = [input_dim_base] + BEST_MLP_LAYERS + [2]\n",
    "\n",
    "mlp_base = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features_base\", \n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    layers=base_layers, \n",
    "    blockSize=128, \n",
    "    maxIter=100\n",
    ")\n",
    "mlp_base_model = mlp_base.fit(train_base_vec)\n",
    "\n",
    "# C. Evaluate\n",
    "print(\"Evaluating Baseline Model on Test Set...\")\n",
    "base_preds = mlp_base_model.transform(test_base_vec)\n",
    "\n",
    "print(f\"FINAL TEST RESULTS (Baseline):\")\n",
    "print(f\"  Delay-Class F2: {f2_eval.evaluate(base_preds):.4f}\")\n",
    "print(f\"  Weighted F1:    {f1_eval.evaluate(base_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e02e94-0e53-47b6-a1c3-e8f21ba3550b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet/\").limit(100))\n",
    "\n",
    "display(spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/feature_eng_ph3/training_splits/test.parquet/\").limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4fe8142-6b7b-4e2f-b7ac-99a610bad397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) 5yr_stacked_NN_with_CV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
