{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d6a5ac-8bbf-47d4-822e-27b3f1b35460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NN MLP model baseline - 3 month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-classifier\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets - custom join\n",
    "- get checkpoint data\n",
    "  - 3 month combined join, with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7b274b-c8d2-4949-9a8a-2b58e6a17564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/feature_eng\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/\")) # feature_eng and cv_splits\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/cv_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/training_splits/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/models/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4825eb9-9441-4e5d-a2c3-6adc6cea10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71052af-525f-4775-a2cc-79142b51f875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"DEP_DEL15\",\n",
    "    \"utc_timestamp\",\n",
    "    \"CRS_DEP_MINUTES\",            # feature eng start\n",
    "    \"prev_flight_delay_in_minutes\", \n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",        # feature eng end\n",
    "    'HourlyDryBulbTemperature',     # weather start\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed'               # weather end\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff4688-854e-4a7f-9429-b74fdc925f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde39db-852d-4ce5-882d-a3f7f7cf3f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'                   # weather end\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70368be9-d8ed-4557-8841-52d5e1751acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be653215-8965-4f23-bc26-6db14cc0b802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Model Estimators ---\n",
    "preprocessing_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    assembler \n",
    "]\n",
    "\n",
    "# # A. XGBoost Regressor\n",
    "# xgb = SparkXGBRegressor(\n",
    "#     features_col=\"features\",\n",
    "#     label_col=\"DEP_DELAY_NEW\",\n",
    "#     num_workers=2, \n",
    "#     max_depth=6,\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.05\n",
    "# )\n",
    "\n",
    "# B. MLP Classifier\n",
    "# num_columns = 32\n",
    "# num_classes = 2\n",
    "\n",
    "# mlp = MultilayerPerceptronClassifier(\n",
    "#     featuresCol=\"features\",\n",
    "#     labelCol=\"DEP_DEL15\",\n",
    "#     # predictionCol=\"prediction\",\n",
    "#     maxIter=100,\n",
    "#     layers=[num_columns, num_columns//2, num_classes],\n",
    "#     blockSize=128,\n",
    "#     stepSize=0.03\n",
    "# )\n",
    "\n",
    "# --- Evaluator (Use one metric for optimization) ---\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" \n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"    \n",
    ")\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f2_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedFMeasure\"\n",
    ")\n",
    "f2_evaluator.setBeta(2.0)\n",
    "\n",
    "f2_evaluator_label = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"fMeasureByLabel\"\n",
    ")\n",
    "f2_evaluator_label.setMetricLabel(1).setBeta(2.0)\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "497a0f46-84c7-45ec-8609-3c0ba0b5f831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36887f53-3436-46c9-bc0e-aaca9fe12246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    \"\"\"\n",
    "    Read a specific fold from partitioned parquet data.\n",
    "    Falls back to filtering if direct partition read fails.\n",
    "    \"\"\"\n",
    "    fold_path = f\"{path}/fold_id={fold_id}/split_type={split_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Try direct partition read\n",
    "        return spark.read.parquet(fold_path)\n",
    "    except:\n",
    "        # Fallback: read all data and filter\n",
    "        print(f\"Direct read failed for fold {fold_id}, using filter method...\")\n",
    "        all_data = spark.read.parquet(path)\n",
    "        return all_data.filter(\n",
    "            (all_data.fold_id == fold_id) & \n",
    "            (all_data.split_type == split_type)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e7ba9c-de95-4b40-8ee1-a0c4713f8f0b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765167891201}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_or_year = \"3_month_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/cv_splits\" \n",
    "val_fold = read_specific_fold(cv_path, 1, \"validation\").cache()\n",
    "import pyspark.sql.functions as F\n",
    "display(val_fold.groupby(\"DEP_DEL15\").agg(F.count(\"*\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2e3116-3f08-4b5b-ac61-f64ea4f5e339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Training Loop for XGBoost with best hyperparams ---\n",
    "n_folds = 5\n",
    "month_or_year = \"3_month_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/cv_splits\" \n",
    "\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=12, \n",
    "    tree_method=\"hist\",           \n",
    "    max_depth=6,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGB_BEST_HP_3_MNTH_CV\") as run:\n",
    "\n",
    "    \n",
    "    # 1. Log Parameters\n",
    "    mlflow.log_param(\"model\", \"XGBoost\")\n",
    "    mlflow.log_param(\"max_depth\", 6)\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"learning_rate\", 0.05)\n",
    "    mlflow.log_param(\"dataset\", month_or_year)\n",
    "\n",
    "    # 2. Create Pipeline\n",
    "    pipeline = Pipeline(stages=preprocessing_stages + [xgb])\n",
    "    \n",
    "    fold_metrics = {'train_mae': [], 'val_mae': [], 'train_rmse': [], 'val_rmse': []}\n",
    "    \n",
    "    # 3. CV Loop\n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True):\n",
    "            print(f\"  Processing Fold {fold_id}/{n_folds}...\")\n",
    "            \n",
    "            # Load Data\n",
    "            train_fold = read_specific_fold(cv_path, fold_id, \"train\").cache()\n",
    "            val_fold = read_specific_fold(cv_path, fold_id, \"validation\").cache()\n",
    "            display(val_fold)\n",
    "            \n",
    "            # Fit & Predict\n",
    "            model = pipeline.fit(train_fold)\n",
    "            train_preds = model.transform(train_fold)\n",
    "            val_preds = model.transform(val_fold)\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = {\n",
    "                \"train_mae\": mae_evaluator.evaluate(train_preds),\n",
    "                \"val_mae\": mae_evaluator.evaluate(val_preds),\n",
    "                \"train_rmse\": rmse_evaluator.evaluate(train_preds),\n",
    "                \"val_rmse\": rmse_evaluator.evaluate(val_preds)\n",
    "            }\n",
    "            \n",
    "            # Log & Print\n",
    "            mlflow.log_metrics(metrics)\n",
    "            print(f\"    Fold {fold_id}: Val MAE={metrics['val_mae']:.4f}, Val RMSE={metrics['val_rmse']:.4f}\")\n",
    "            \n",
    "            for k, v in metrics.items():\n",
    "                fold_metrics[k].append(v)\n",
    "            \n",
    "            # Cleanup\n",
    "            train_fold.unpersist()\n",
    "            val_fold.unpersist()\n",
    "\n",
    "    # 4. Log Aggregates\n",
    "    avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "    std_metrics = {f\"std_{k}\": np.std(v) for k, v in fold_metrics.items()}\n",
    "    \n",
    "    mlflow.log_metrics({**avg_metrics, **std_metrics})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Average Val MAE:  {avg_metrics['avg_val_mae']:.4f} (+/- {std_metrics['std_val_mae']:.4f})\")\n",
    "    print(f\"Average Val RMSE: {avg_metrics['avg_val_rmse']:.4f} (+/- {std_metrics['std_val_rmse']:.4f})\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59bc036-2de1-46fd-9d01-bcd2cd04dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Training Loop for basic MLP ---\n",
    "n_folds = 5\n",
    "month_or_year = \"3_month_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/cv_splits\"\n",
    "\n",
    "preprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "\n",
    "#with mlflow.start_run(run_name=\"MLP_BASELINE_3_MNTH_CV_64\") as run:\n",
    "with mlflow.start_run(run_name=\"DANIEL_TEST2_MLP_BASELINE_3_MNTH_CV_64\") as run:\n",
    "    \n",
    "    mlflow.log_param(\"model\", \"MLP Classifier\")\n",
    "    mlflow.log_param(\"dataset\", month_or_year)\n",
    "    mlflow.log_param(\"folds\", n_folds)\n",
    "    \n",
    "    fold_metrics = {'train_f2': [], 'val_f2': [], 'train_f1': [], 'val_f1': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [], 'train_auc': [], 'val_auc': [], 'train_acc': [], 'val_acc': [], 'train_f2_label': [], 'val_f2_label': []}\n",
    "    \n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True):\n",
    "            print(f\"  Processing Fold {fold_id}/{n_folds}...\")\n",
    "            \n",
    "            # 1. Load Data\n",
    "            train_fold = read_specific_fold(cv_path, fold_id, \"train\").cache()\n",
    "            val_fold = read_specific_fold(cv_path, fold_id, \"validation\").cache()\n",
    "            \n",
    "            # 2. Fit Feature Engineering Pipeline FIRST\n",
    "            # We need to do this to know the input vector size for the MLP layers\n",
    "            feat_model = preprocessing_pipeline.fit(train_fold)\n",
    "            \n",
    "            train_vec = feat_model.transform(train_fold)\n",
    "            val_vec = feat_model.transform(val_fold)\n",
    "            \n",
    "            # 3. Calculate Input Dimension dynamically\n",
    "            # Grab one row to check the length of the 'features' vector\n",
    "            input_dim = len(train_vec.select(\"features\").first()[0])\n",
    "            print(f\"    Detected Input Dimension: {input_dim}\")\n",
    "            \n",
    "            # 4. Define MLP Architecture\n",
    "            # Layers: [Input, Hidden, Output]\n",
    "            layers = [input_dim, 64, 2] \n",
    "\n",
    "            mlp = MultilayerPerceptronClassifier(\n",
    "                featuresCol=\"features\",\n",
    "                labelCol=\"DEP_DEL15\",\n",
    "                maxIter=100,\n",
    "                layers=layers,\n",
    "                blockSize=128,\n",
    "                stepSize=0.03\n",
    "            )\n",
    "\n",
    "            # 5. Train MLP\n",
    "            mlp_model = mlp.fit(train_vec)\n",
    "            \n",
    "            # 6. Predict\n",
    "            train_preds = mlp_model.transform(train_vec)\n",
    "            val_preds = mlp_model.transform(val_vec)\n",
    "            \n",
    "            # 7. Evaluate\n",
    "            metrics = {\n",
    "                \"train_f2\": f2_evaluator.evaluate(train_preds),\n",
    "                \"val_f2\": f2_evaluator.evaluate(val_preds),\n",
    "                \"train_f2_label\": f2_evaluator_label.evaluate(train_preds),\n",
    "                \"val_f2_label\": f2_evaluator_label.evaluate(val_preds),\n",
    "                \"train_f1\": f1_evaluator.evaluate(train_preds),\n",
    "                \"val_f1\": f1_evaluator.evaluate(val_preds),\n",
    "                \"train_precision\": precision_evaluator.evaluate(train_preds),\n",
    "                \"val_precision\": precision_evaluator.evaluate(val_preds),\n",
    "                \"train_recall\": recall_evaluator.evaluate(train_preds),\n",
    "                \"val_recall\": recall_evaluator.evaluate(val_preds),\n",
    "                \"train_auc\": auc_evaluator.evaluate(train_preds),\n",
    "                \"val_auc\": auc_evaluator.evaluate(val_preds),\n",
    "                \"train_acc\": acc_evaluator.evaluate(train_preds),\n",
    "                \"val_acc\": acc_evaluator.evaluate(val_preds)\n",
    "            }\n",
    "            \n",
    "            # Log & Store\n",
    "            mlflow.log_metrics(metrics)\n",
    "            mlflow.log_param(\"input_dim\", input_dim)\n",
    "            mlflow.log_param(\"layers\", str(layers))\n",
    "            \n",
    "            print(f\"    Fold {fold_id}: Val AUC={metrics['val_auc']:.4f}, Val Acc={metrics['val_acc']:.4f}, Val Precision={metrics['val_precision']:.4f}, Val Recall={metrics['val_recall']:.4f}, Val F1={metrics['val_f1']:.4f}, Val F2={metrics['val_f2']:.4f}, Val F2_label={metrics['val_f2_label']:.4f}\")\n",
    "            \n",
    "            for k, v in metrics.items():\n",
    "                fold_metrics[k].append(v)\n",
    "            \n",
    "            # Cleanup\n",
    "            train_fold.unpersist()\n",
    "            val_fold.unpersist()\n",
    "\n",
    "    # 8. Log Averages\n",
    "    avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "    std_metrics = {f\"std_{k}\": np.std(v) for k, v in fold_metrics.items()}\n",
    "    \n",
    "    mlflow.log_metrics({**avg_metrics, **std_metrics})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Average Val AUC:      {avg_metrics['avg_val_auc']:.4f} (+/- {std_metrics['std_val_auc']:.4f})\")\n",
    "    print(f\"Average Val Accuracy: {avg_metrics['avg_val_acc']:.4f} (+/- {std_metrics['std_val_acc']:.4f})\")\n",
    "    print(f\"Average Val Precision:  {avg_metrics['avg_val_precision']:.4f} (+/- {std_metrics['std_val_precision']:.4f})\")\n",
    "    print(f\"Average Val Recall:   {avg_metrics['avg_val_recall']:.4f} (+/- {std_metrics['std_val_recall']:.4f})\")\n",
    "    print(f\"Average Val F1:       {avg_metrics['avg_val_f1']:.4f} (+/- {std_metrics['std_val_f1']:.4f})\")\n",
    "    print(f\"Average Val F2:       {avg_metrics['avg_val_f2']:.4f} (+/- {std_metrics['std_val_f2']:.4f})\")\n",
    "    print(f\"Average Val F2_label: {avg_metrics['avg_val_f2_label']:.4f} (+/- {std_metrics['std_val_f2_label']:.4f})\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9afbeb4-4a8e-444f-b880-c63561c9d05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# stacked approach\n",
    "- build XGBoost model and do hyperparamter tuning to find the best hyperparams\n",
    "- generate the XGBoost regression delay field and output it using the held out data\n",
    "- use the XGBoost delay field as the input for the NN/MLP model while doing hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b532a244-29b9-41e6-8b0e-63f353ba165b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import itertools\n",
    "\n",
    "# --- 1. Define the Parameter Grid ---\n",
    "# We use lists for the values we want to test\n",
    "grid_search_params = {\n",
    "    \"max_depth\": [6], # [4, 6]\n",
    "    \"n_estimators\": [50, 100], # [20, 50, 100]\n",
    "    \"learning_rate\": [0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Create a list of all combinations (Cartesian Product)\n",
    "keys, values = zip(*grid_search_params.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"Total Parameter Combinations to Test: {len(param_combinations)}\")\n",
    "\n",
    "# --- 2. Run Tuning Loop ---\n",
    "n_folds = 5\n",
    "month_or_year = \"3_month_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/cv_splits\"\n",
    "\n",
    "# Store results to find the winner later\n",
    "results_list = []\n",
    "\n",
    "print(f\"Starting Hyperparameter Tuning on {month_or_year}...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGB_GRID_SEARCH_3_MNTH\") as parent_run:\n",
    "    mlflow.log_param(\"n_combinations\", len(param_combinations))\n",
    "    \n",
    "    for idx, params in enumerate(param_combinations):\n",
    "        param_str = f\"depth{params['max_depth']}_est{params['n_estimators']}_lr{params['learning_rate']}\"\n",
    "        print(f\"\\n--- Testing Combo {idx+1}/{len(param_combinations)}: {param_str} ---\")\n",
    "        \n",
    "        # Define Model with CURRENT params\n",
    "        current_xgb = SparkXGBRegressor(\n",
    "            features_col=\"features\",\n",
    "            label_col=\"DEP_DELAY_NEW\",\n",
    "            num_workers=6, \n",
    "            tree_method=\"hist\",  \n",
    "            max_depth=params['max_depth'],\n",
    "            n_estimators=params['n_estimators'],\n",
    "            learning_rate=params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline(stages=preprocessing_stages + [current_xgb])\n",
    "        \n",
    "        fold_maes = []\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"combo_{idx}_{param_str}\", nested=True) as child_run:\n",
    "            # Log Params\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "            # CV Loop for this combo\n",
    "            for fold_id in range(1, n_folds + 1):\n",
    "                # Load Data\n",
    "                train_fold = read_specific_fold(cv_path, fold_id, \"train\").cache()\n",
    "                val_fold = read_specific_fold(cv_path, fold_id, \"validation\").cache()\n",
    "                \n",
    "                # Fit & Predict\n",
    "                model = pipeline.fit(train_fold)\n",
    "                val_preds = model.transform(val_fold)\n",
    "                \n",
    "                # Metric\n",
    "                mae = mae_evaluator.evaluate(val_preds)\n",
    "                fold_maes.append(mae)\n",
    "                \n",
    "                # Cleanup\n",
    "                train_fold.unpersist()\n",
    "                val_fold.unpersist()\n",
    "            \n",
    "            # Aggregate Results\n",
    "            avg_mae = np.mean(fold_maes)\n",
    "            mlflow.log_metric(\"avg_val_mae\", avg_mae)\n",
    "            \n",
    "            print(f\"  Combo {idx+1} Result: Avg MAE = {avg_mae:.4f}\")\n",
    "            \n",
    "            # Add to results list\n",
    "            results_list.append({\n",
    "                **params,\n",
    "                \"avg_val_mae\": avg_mae\n",
    "            })\n",
    "\n",
    "# --- 3. Identify Best Parameters ---\n",
    "# Convert to Pandas for easy sorting\n",
    "results_df = pd.DataFrame(results_list)\n",
    "best_row = results_df.loc[results_df['avg_val_mae'].idxmin()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"WINNER FOUND: {best_row.to_dict()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract best params for Phase 2\n",
    "best_depth = int(best_row['max_depth'])\n",
    "best_estimators = int(best_row['n_estimators'])\n",
    "best_lr = float(best_row['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f492fbe-b9b0-47fe-88b1-69c48f9b24ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "print(f\"\\nGenerating Stacked Features using Best Params: Depth={best_depth}, Est={best_estimators}, LR={best_lr}\")\n",
    "\n",
    "best_xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=6,   \n",
    "    tree_method=\"hist\",            \n",
    "    max_depth=best_depth,\n",
    "    n_estimators=best_estimators,\n",
    "    learning_rate=best_lr\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=preprocessing_stages + [best_xgb])\n",
    "\n",
    "out_of_fold_predictions = []\n",
    "\n",
    "for fold_id in range(1, n_folds + 1):\n",
    "    print(f\"  Processing Fold {fold_id}/{n_folds} for Stacking...\")\n",
    "    \n",
    "    train_fold = read_specific_fold(cv_path, fold_id, \"train\").cache()\n",
    "    val_fold = read_specific_fold(cv_path, fold_id, \"validation\").cache()\n",
    "    \n",
    "    model = pipeline.fit(train_fold)\n",
    "    val_preds = model.transform(val_fold)\n",
    "    \n",
    "    val_preds_clean = val_preds \\\n",
    "        .withColumnRenamed(\"prediction\", \"xgb_predicted_delay\") \\\n",
    "        .withColumn(\"fold_id\", F.lit(fold_id))  # <--- Force add fold_id\n",
    "    out_of_fold_predictions.append(val_preds_clean)\n",
    "    \n",
    "    train_fold.unpersist()\n",
    "    val_fold.unpersist()\n",
    "\n",
    "stacked_dataset = reduce(DataFrame.union, out_of_fold_predictions)\n",
    "\n",
    "output_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/stacked_input_optimized\"\n",
    "cols_to_drop = [\"features\", \"carrier_vec\", \"origin_vec\", \"dest_vec\", \"carrier_idx\", \"origin_idx\", \"dest_idx\"]\n",
    "stacked_dataset.drop(*cols_to_drop).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"SUCCESS: Optimized stacked dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cbc07f9-23ac-4edb-875d-a6b54be1c0a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# # --- 1. Experiment Setup ---\n",
    "# EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-classifier\"\n",
    "# mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# # Load the OPTIMIZED stacked dataset (Output from previous XGBoost step)\n",
    "# month_or_year = \"3_month_custom_joined\"\n",
    "stacked_input_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/stacked_input_optimized\"\n",
    "\n",
    "# Load & Repartition for parallelism\n",
    "stacked_df = spark.read.parquet(stacked_input_path).repartition(12).cache()\n",
    "print(f\"Loaded {stacked_df.count()} rows for MLP tuning.\")\n",
    "\n",
    "# --- 2. Define Hyperparameter Grid (Hidden Layers Only) ---\n",
    "# We define the \"middle\" of the network. The code will auto-add Input and Output layers.\n",
    "# Structure: List of lists representing hidden layer sizes\n",
    "hidden_layer_grid = [\n",
    "    [32],               # Shallow, narrow \n",
    "    [64, 32],           # Medium depth \n",
    "    [128, 64],          # Wider \n",
    "    [64, 32, 16],       # Deep\n",
    "    # [128, 64, 32]       # Deep & Wide\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(hidden_layer_grid)} different network architectures.\")\n",
    "\n",
    "# --- 3. Feature Pipeline Definition ---\n",
    "# (Same as before, re-defining since we load from Parquet)\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\", \"MONTH\", \"YEAR\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\", \"origin_vec\", \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"CRS_DEP_MINUTES\",\n",
    "        \"prev_flight_delay_in_minutes\", \"prev_flight_delay\", \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\", \"delay_origin_carrier_7d\", \"delay_route_7d\",\n",
    "        \"flight_count_24h\", \"LANDING_TIME_DIFF_MINUTES\", \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\", 'HourlyDryBulbTemperature', 'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity', 'HourlyAltimeterSetting', 'HourlyVisibility',\n",
    "        'HourlyStationPressure', 'HourlyWetBulbTemperature', 'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage', 'HourlyCloudElevation', 'HourlyWindSpeed',\n",
    "        \"xgb_predicted_delay\"  # <--- Ensure this is included!\n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# withMean=False is important to handle the sparse OneHot vectors efficiently\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Update pipeline to include scaler\n",
    "preprocessing_pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder,\n",
    "    assembler,\n",
    "    scaler \n",
    "])\n",
    "\n",
    "# --- 4. Evaluators ---\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"areaUnderROC\")\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"f1\")\n",
    "f2_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedFMeasure\")\n",
    "f2_evaluator.setBeta(2.0)\n",
    "\n",
    "# --- 5. Tuning Loop ---\n",
    "n_folds = 5\n",
    "results_list = []\n",
    "\n",
    "print(f\"Starting MLP Architecture Search...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"MLP_3M_ARCHITECTURE_SEARCH_F2\") as parent_run:\n",
    "    \n",
    "    for idx, hidden_config in enumerate(hidden_layer_grid):\n",
    "        \n",
    "        config_str = \"-\".join(map(str, hidden_config)) \n",
    "        print(f\"\\n--- Testing Arch {idx+1}/{len(hidden_layer_grid)}: Hidden=[{config_str}] ---\")\n",
    "        \n",
    "        fold_metrics = {'val_f2': []}\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"arch_{config_str}\", nested=True) as child_run:\n",
    "            mlflow.log_param(\"hidden_layers\", str(hidden_config))\n",
    "            \n",
    "            for fold_id in range(1, n_folds + 1):\n",
    "                print(f\"  Processing Fold {fold_id}/{n_folds}...\")\n",
    "                \n",
    "                # A. Split Data\n",
    "                train_fold = stacked_df.filter(F.col(\"fold_id\") != fold_id)\n",
    "                val_fold = stacked_df.filter(F.col(\"fold_id\") == fold_id)\n",
    "                \n",
    "                # B. Fit Feature Pipeline\n",
    "                feat_model = preprocessing_pipeline.fit(train_fold)\n",
    "                train_vec = feat_model.transform(train_fold)\n",
    "                val_vec = feat_model.transform(val_fold)\n",
    "                \n",
    "                # C. Dynamic Layer Construction\n",
    "                input_dim = len(train_vec.select(\"scaled_features\").first()[0])\n",
    "                output_dim = 2 \n",
    "                \n",
    "                full_layers = [input_dim] + hidden_config + [output_dim]\n",
    "                \n",
    "                # D. Train MLP\n",
    "                mlp = MultilayerPerceptronClassifier(\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    labelCol=\"DEP_DEL15\",\n",
    "                    layers=full_layers,\n",
    "                    blockSize=128,\n",
    "                    maxIter=100,\n",
    "                    stepSize=0.03\n",
    "                )\n",
    "                \n",
    "                mlp_model = mlp.fit(train_vec)\n",
    "                \n",
    "                # E. Evaluate\n",
    "                val_preds = mlp_model.transform(val_vec)\n",
    "                \n",
    "                # Calculate F2\n",
    "                f2 = f2_evaluator.evaluate(val_preds)\n",
    "                fold_metrics['val_f2'].append(f2)\n",
    "                \n",
    "                print(f\"    Fold {fold_id}: F2={f2:.4f}\")\n",
    "            \n",
    "            # Aggregate Results\n",
    "            avg_f2 = np.mean(fold_metrics['val_f2'])\n",
    "            \n",
    "            mlflow.log_metric(\"avg_val_f2\", avg_f2)\n",
    "            \n",
    "            results_list.append({\n",
    "                \"hidden_config\": str(hidden_config),\n",
    "                \"avg_val_f2\": avg_f2\n",
    "            })\n",
    "            \n",
    "            print(f\"  Arch [{config_str}] Result: Avg F2 = {avg_f2:.4f}\")\n",
    "\n",
    "# --- 6. Select Best Architecture ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Select best based on F2 score\n",
    "best_row = results_df.loc[results_df['avg_val_f2'].idxmax()] \n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOP 3 ARCHITECTURES (Sorted by F2):\")\n",
    "print(results_df.sort_values(\"avg_val_f2\", ascending=False).head(3))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Unpersist\n",
    "stacked_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bcc6ee5-06bd-48e2-a877-355009bd9514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3mth_NN_with_CV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
