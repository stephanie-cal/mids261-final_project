{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d6a5ac-8bbf-47d4-822e-27b3f1b35460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NN MLP stacked model - 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-classifier\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets - custom join\n",
    "- get checkpoint data\n",
    "  - 5 year combined join, with feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4825eb9-9441-4e5d-a2c3-6adc6cea10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71052af-525f-4775-a2cc-79142b51f875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"DEP_DEL15\",\n",
    "    \"utc_timestamp\",\n",
    "    \"CRS_DEP_MINUTES\",            # feature eng start\n",
    "    \"prev_flight_delay_in_minutes\", \n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",        # feature eng end\n",
    "    'HourlyDryBulbTemperature',     # weather start\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed',               # weather end\n",
    "    # 'page_rank',               # phase 3 new features start\n",
    "    'out_degree',\n",
    "    'in_degree',\n",
    "    'weighted_out_degree',\n",
    "    'weighted_in_degree',\n",
    "    'N_RUNWAYS',\n",
    "    'betweenness_unweighted',\n",
    "    'closeness',\n",
    "    'betweenness',\n",
    "    'avg_origin_dep_delay',\n",
    "    'avg_dest_arr_delay',\n",
    "    'avg_daily_route_flights',\n",
    "    'avg_route_delay',\n",
    "    'avg_hourly_flights',\n",
    "    \"IS_HOLIDAY\",\n",
    "    \"IS_HOLIDAY_WINDOW\",\n",
    "    \"AIRPORT_HUB_CLASS\",\n",
    "    \"RATING\",\n",
    "    \"AIRLINE_CATEGORY\",\n",
    "    \"ground_flights_last_hour\",\n",
    "    \"arrivals_last_hour\",\n",
    "    \"dow_sin\",\n",
    "    \"dow_cos\",\n",
    "    \"doy_sin\",\n",
    "    \"doy_cos\" # phase 3 new features end\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff4688-854e-4a7f-9429-b74fdc925f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "holiday_indexer = StringIndexer(inputCol=\"IS_HOLIDAY\", outputCol=\"holiday_idx\", handleInvalid=\"keep\")\n",
    "holiday_window_indexer = StringIndexer(inputCol=\"IS_HOLIDAY_WINDOW\", outputCol=\"holiday_window_idx\", handleInvalid=\"keep\")\n",
    "airport_hub_indexer = StringIndexer(inputCol=\"AIRPORT_HUB_CLASS\", outputCol=\"airport_hub_idx\", handleInvalid=\"keep\")\n",
    "airline_category_indexer = StringIndexer(inputCol=\"AIRLINE_CATEGORY\", outputCol=\"airline_category_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_idx\", outputCol=\"holiday_vec\")\n",
    "holiday_window_encoder = OneHotEncoder(inputCol=\"holiday_window_idx\", outputCol=\"holiday_window_vec\")\n",
    "airport_hub_encoder = OneHotEncoder(inputCol=\"airport_hub_idx\", outputCol=\"airport_hub_vec\")\n",
    "airline_category_encoder = OneHotEncoder(inputCol=\"airline_category_idx\", outputCol=\"airline_category_vec\")\n",
    "\n",
    "\n",
    "# # Inside ResFiLMMLP class:\n",
    "\n",
    "# def __init__(self, ...):\n",
    "#     # Create a list of embedding layers, one for each categorical column\n",
    "#     # cat_dims = [Number of unique categories + 1 for Unknowns]\n",
    "#     # emb_dims = [Vector size, e.g., 64]\n",
    "#     self.embeddings = nn.ModuleList([\n",
    "#         nn.Embedding(d, e) for d, e in zip(cat_dims, emb_dims)\n",
    "#     ])\n",
    "\n",
    "# def forward(self, x_cat, ...):\n",
    "#     # Lookup the vector for each column's integer index\n",
    "#     # x_cat[:, i] is the column of integers for category 'i'\n",
    "#     emb = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "    \n",
    "#     # Concatenate all vectors into one long feature vector\n",
    "#     emb = torch.cat(emb, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde39db-852d-4ce5-882d-a3f7f7cf3f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        # 'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights',\n",
    "        \"holiday_vec\",\n",
    "        \"holiday_window_vec\",\n",
    "        \"airport_hub_vec\",\n",
    "        \"RATING\",\n",
    "        \"airline_category_vec\",\n",
    "        \"ground_flights_last_hour\",\n",
    "        \"arrivals_last_hour\",\n",
    "        \"dow_sin\",\n",
    "        \"dow_cos\",\n",
    "        \"doy_sin\",\n",
    "        \"doy_cos\" # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70368be9-d8ed-4557-8841-52d5e1751acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be653215-8965-4f23-bc26-6db14cc0b802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Evaluator (Use one metric for optimization) ---\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" \n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"    \n",
    ")\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f2_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedFMeasure\"\n",
    ")\n",
    "f2_evaluator.setBeta(2.0)\n",
    "\n",
    "f2_evaluator_label = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"fMeasureByLabel\"\n",
    ")\n",
    "f2_evaluator_label.setMetricLabel(1.0).setBeta(2.0)\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36887f53-3436-46c9-bc0e-aaca9fe12246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    \"\"\"\n",
    "    Read a specific fold from partitioned parquet data.\n",
    "    Falls back to filtering if direct partition read fails.\n",
    "    \"\"\"\n",
    "    fold_path = f\"{path}/fold_id={fold_id}/split_type={split_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Try direct partition read\n",
    "        return spark.read.parquet(fold_path)\n",
    "    except:\n",
    "        # Fallback: read all data and filter\n",
    "        print(f\"Direct read failed for fold {fold_id}, using filter method...\")\n",
    "        all_data = spark.read.parquet(path)\n",
    "        return all_data.filter(\n",
    "            (all_data.fold_id == fold_id) & \n",
    "            (all_data.split_type == split_type)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9afbeb4-4a8e-444f-b880-c63561c9d05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# stacked approach (attempt 1)\n",
    "- build XGBoost model and do hyperparamter tuning to find the best hyperparams\n",
    "- generate the XGBoost regression delay field and output it using the held out data\n",
    "- use the XGBoost delay field as the input for the NN/MLP model while doing hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c053254e-a901-4613-bd67-093c3e9d1be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import BooleanType\n",
    "import itertools\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# --- 1. Config ---\n",
    "TRAIN_PARTITIONS = 6 \n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "full_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday_nnfeat/training_splits/\"\n",
    "train_path = full_path + \"train.parquet\"\n",
    "val_path = full_path + \"val.parquet\"\n",
    "test_path = full_path + \"test.parquet\"\n",
    "\n",
    "# --- 2. Robust Safety Checks ---\n",
    "# This REPLACES bad values instead of checking for them\n",
    "@F.udf(returnType=VectorUDT())\n",
    "def sanitize_vector(v):\n",
    "    if v is None: return Vectors.dense([])\n",
    "    \n",
    "    # Convert to numpy for fast processing\n",
    "    arr = v.toArray()\n",
    "    \n",
    "    # Replace NaN, Infinity, -Infinity with 0.0\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Clip huge values to prevent float32 overflow in XGBoost\n",
    "    arr = np.clip(arr, -1e30, 1e30)\n",
    "    \n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "# --- 3. Data Loading & Pipeline Definition ---\n",
    "print(f\"Loading and optimizing data from {month_or_year}...\")\n",
    "\n",
    "# Read in data\n",
    "# full_df = spark.read.parquet(full_path).repartition(480).cache()\n",
    "train_df = spark.read.parquet(train_path).repartition(480).cache()\n",
    "val_df = spark.read.parquet(val_path).repartition(480).cache()\n",
    "train_val_df = train_df.union(val_df).cache()\n",
    "test_df = spark.read.parquet(test_path).repartition(480).cache()\n",
    "\n",
    "\n",
    "# Define Final Model\n",
    "best_depth = 6\n",
    "best_estimators = 100\n",
    "best_lr = 0.1   \n",
    "final_xgb = SparkXGBRegressor(\n",
    "    features_col=\"raw_features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=6,\n",
    "    tree_method=\"hist\",\n",
    "    max_depth=best_depth,\n",
    "    n_estimators=best_estimators,\n",
    "    learning_rate=best_lr,\n",
    "    missing=0.0\n",
    ")\n",
    "\n",
    "\n",
    "# build pipeline stages list to use this specific assembler\n",
    "xgb_pipeline_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    tail_num_indexer, holiday_indexer, holiday_window_indexer, \n",
    "    airport_hub_indexer, airline_category_indexer,\n",
    "    \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    tail_num_encoder, holiday_encoder, holiday_window_encoder, \n",
    "    airport_hub_encoder, airline_category_encoder,\n",
    "    \n",
    "    assembler,\n",
    "    final_xgb\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f474370d-2f09-48a7-957c-93fe27cf9efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with mlflow.start_run(run_name=\"FINAL_XGB_STACKED_5_YR\") as parent_run:\n",
    "global_pipeline = Pipeline(stages=xgb_pipeline_stages)\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline...\")\n",
    "final_model = global_pipeline.fit(train_val_df)\n",
    "\n",
    "print(\"Transforming & Cleaning...\")\n",
    "train_val_predictions = final_model.transform(train_val_df)\n",
    "test_predictions = final_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb29eb1-5ae0-4b1a-9d3c-c88cfc7190df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "mae_train_val = mae_evaluator.evaluate(train_val_predictions)\n",
    "mae_test = mae_evaluator.evaluate(test_predictions)\n",
    "# Calculate RMSE\n",
    "rmse_train_val = rmse_evaluator.evaluate(train_val_predictions)\n",
    "rmse_test = rmse_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Define DBFS paths\n",
    "train_val_pred_path = full_path + \"train_val_predictions\"\n",
    "test_pred_path = full_path + \"test_predictions\"\n",
    "# Save as Parquet to DBFS\n",
    "train_val_predictions.withColumnRenamed(\"prediction\", \"xgb_predicted_delay\").write.mode(\"overwrite\").parquet(train_val_pred_path)\n",
    "test_predictions.withColumnRenamed(\"prediction\", \"xgb_predicted_delay\").write.mode(\"overwrite\").parquet(test_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f67887e0-eb86-4b03-b90b-a15779f5d5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "with mlflow.start_run(run_name=\"FINAL_XGB_STACKED_5_YR\"):\n",
    "\n",
    "    # mlflow.log_artifacts(train_val_pred_path, \"train_val_predictions\")\n",
    "    # mlflow.log_artifact(test_pred_path, \"test_predictions\")\n",
    "\n",
    "    mlflow.log_metric(\"train_val_mae\", mae_train_val)\n",
    "    mlflow.log_metric(\"train_val_rmse\", rmse_train_val)\n",
    "    mlflow.log_metric(\"test_mae\", mae_test)\n",
    "    mlflow.log_metric(\"test_rmse\", rmse_test)\n",
    "\n",
    "    mlflow.log_param(\"max_depth\", best_depth)\n",
    "    mlflow.log_param(\"n_estimators\", best_estimators)\n",
    "    mlflow.log_param(\"learning_rate\", best_lr)\n",
    "\n",
    "    signature = infer_signature(train_val_df, train_val_predictions)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        final_model, \n",
    "        \"final_stacked_xgb_model_5yr\",\n",
    "        input_example=train_val_df.limit(1).toPandas(),\n",
    "        signature=signature,\n",
    "        registered_model_name=\"final_stacked_xgb_model_5yr\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14ca58b-31e7-400c-8d73-9ad49c272be5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":196},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765508922108}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d458a050-f3f2-4ae4-ba85-6cb75a710190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/cv_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/cv_splits2/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/cv_splits2/fold_id=1/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b4cb51-869a-40db-9652-909ee9a40e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# stacked approach (attempt 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e64893-6ee0-4f10-8e2f-23e8d5acfc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "import mlflow\n",
    "import time\n",
    "import functools\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- 1. Config ---\n",
    "TRAIN_PARTITIONS = 6 \n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "\n",
    "# Base Paths\n",
    "base_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday_nnfeat/\"\n",
    "cv_splits_path = base_path + \"cv_splits/\"\n",
    "training_splits_path = base_path + \"training_splits/\"\n",
    "\n",
    "# Input Files (Final Model)\n",
    "train_path = training_splits_path + \"train.parquet\"\n",
    "val_path = training_splits_path + \"val.parquet\"\n",
    "test_path = training_splits_path + \"test.parquet\"\n",
    "\n",
    "# Output Paths\n",
    "train_val_pred_path = training_splits_path + \"train_val_predictions\"\n",
    "test_pred_path = training_splits_path + \"test_predictions\"\n",
    "\n",
    "# --- 2. Feature Definitions (Ensuring availability) ---\n",
    "xgb_stages_base = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    tail_num_indexer, holiday_indexer, holiday_window_indexer, \n",
    "    airport_hub_indexer, airline_category_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    tail_num_encoder, holiday_encoder, holiday_window_encoder, \n",
    "    airport_hub_encoder, airline_category_encoder,\n",
    "    assembler\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "best_depth = 6\n",
    "best_estimators = 100\n",
    "best_lr = 0.1   \n",
    "\n",
    "# --- 3. Execution ---\n",
    "print(\"Starting Final Pipeline...\")\n",
    "\n",
    "oof_predictions = []\n",
    "training_times = []\n",
    "cv_train_mae = []\n",
    "cv_train_rmse = []\n",
    "cv_val_mae = []\n",
    "cv_val_rmse = []\n",
    "\n",
    "# Detect folds from file system or assume 1-10\n",
    "folds = range(1, 11) \n",
    "\n",
    "with mlflow.start_run(run_name=\"FINAL_XGB_STACKED_OOF_5_YR\") as run:\n",
    "    \n",
    "    # --- PART A: Generate Out-of-Fold Predictions (Using cv_splits2) ---\n",
    "    print(\"Generating OOF Predictions from cv_splits2...\")\n",
    "    \n",
    "    # Load all CV data once (lazy) to filter later\n",
    "    cv_data = spark.read.parquet(cv_splits_path)\n",
    "    \n",
    "    for fold_id in folds:\n",
    "        print(f\"Processing Fold {fold_id}...\")\n",
    "        \n",
    "        # 1. Filter Data for this Fold\n",
    "        # Train on 'train', Predict on 'validation'\n",
    "        train_fold = cv_data.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"train\")).repartition(480).cache()\n",
    "        val_fold = cv_data.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"validation\")).repartition(480).cache()\n",
    "        \n",
    "        # 2. Define Model\n",
    "        xgb = SparkXGBRegressor(\n",
    "            features_col=\"raw_features\",\n",
    "            label_col=\"DEP_DELAY_NEW\",\n",
    "            num_workers=6,\n",
    "            tree_method=\"hist\",\n",
    "            max_depth=best_depth,\n",
    "            n_estimators=best_estimators,\n",
    "            learning_rate=best_lr,\n",
    "            missing=0.0\n",
    "        )\n",
    "        \n",
    "        pipeline = Pipeline(stages=xgb_stages_base + [xgb])\n",
    "        \n",
    "        # 3. Train & Time\n",
    "        start_time = time.time()\n",
    "        model = pipeline.fit(train_fold)\n",
    "        duration = time.time() - start_time\n",
    "        training_times.append(duration)\n",
    "        \n",
    "        print(f\"  Training took {duration:.2f} seconds\")\n",
    "        \n",
    "        # 4. Predict & Evaluate (TRAIN)\n",
    "        train_preds = model.transform(train_fold)\n",
    "        t_mae = mae_evaluator.evaluate(train_preds)\n",
    "        t_rmse = rmse_evaluator.evaluate(train_preds)\n",
    "        \n",
    "        cv_train_mae.append(t_mae)\n",
    "        cv_train_rmse.append(t_rmse)\n",
    "        \n",
    "        # 5. Predict & Evaluate (VALIDATION)\n",
    "        val_preds = model.transform(val_fold)\n",
    "        v_mae = mae_evaluator.evaluate(val_preds)\n",
    "        v_rmse = rmse_evaluator.evaluate(val_preds)\n",
    "        \n",
    "        cv_val_mae.append(v_mae)\n",
    "        cv_val_rmse.append(v_rmse)\n",
    "        \n",
    "        print(f\"  Fold {fold_id}: Time={duration:.2f}s\")\n",
    "        print(f\"    Train: MAE={t_mae:.2f} | RMSE={t_rmse:.2f}\")\n",
    "        print(f\"    Val:   MAE={v_mae:.2f} | RMSE={v_rmse:.2f}\")\n",
    "        \n",
    "        # Log fold metrics\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_train_mae\", t_mae)\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_val_mae\", v_mae)\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_train_rmse\", t_rmse)\n",
    "        mlflow.log_metric(f\"fold_{fold_id}_val_rmse\", v_rmse)\n",
    "        \n",
    "        # Keep predictions\n",
    "        preds_clean = val_preds.withColumnRenamed(\"prediction\", \"xgb_predicted_delay\")\n",
    "        oof_predictions.append(preds_clean)\n",
    "\n",
    "    # --- PART B: Train Final Model (For Test Set) ---\n",
    "    print(\"\\nTraining Final Model on ALL Train+Val Data...\")\n",
    "    \n",
    "    # Load separate Train/Val files and union them\n",
    "    train_df = spark.read.parquet(train_path)\n",
    "    val_df = spark.read.parquet(val_path)\n",
    "    train_val_df = train_df.union(val_df).repartition(480).cache()\n",
    "    \n",
    "    # Load Test\n",
    "    test_df = spark.read.parquet(test_path).repartition(480).cache()\n",
    "    \n",
    "    final_xgb = SparkXGBRegressor(\n",
    "        features_col=\"raw_features\",\n",
    "        label_col=\"DEP_DELAY_NEW\",\n",
    "        num_workers=6,\n",
    "        tree_method=\"hist\",\n",
    "        max_depth=best_depth,\n",
    "        n_estimators=best_estimators,\n",
    "        learning_rate=best_lr,\n",
    "        missing=0.0\n",
    "    )\n",
    "    final_pipeline = Pipeline(stages=xgb_stages_base + [final_xgb])\n",
    "    \n",
    "    # Train & Time\n",
    "    start_time = time.time()\n",
    "    final_model = final_pipeline.fit(train_val_df)\n",
    "    final_duration = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Final training took {final_duration:.2f} seconds\")\n",
    "    \n",
    "    # 1. Final Training Performance (Did we learn?)\n",
    "    final_train_preds = final_model.transform(train_val_df)\n",
    "    final_train_mae = mae_evaluator.evaluate(final_train_preds)\n",
    "    final_train_rmse = rmse_evaluator.evaluate(final_train_preds)\n",
    "    \n",
    "    # 2. Test Set Performance (Did we generalize?)\n",
    "    test_preds = final_model.transform(test_df)\n",
    "    final_test_mae = mae_evaluator.evaluate(test_preds)\n",
    "    final_test_rmse = rmse_evaluator.evaluate(test_preds)\n",
    "    \n",
    "    print(f\"  Final Train: MAE={final_train_mae:.2f} | RMSE={final_train_rmse:.2f}\")\n",
    "    print(f\"  Final Test:  MAE={final_test_mae:.2f}  | RMSE={final_test_rmse:.2f}\")\n",
    "\n",
    "    # --- PART C: Logging ---\n",
    "    avg_train_time = sum(training_times) / len(training_times)\n",
    "    \n",
    "    mlflow.log_metric(\"avg_fold_training_time_sec\", sum(training_times) / len(training_times))\n",
    "    mlflow.log_metric(\"final_model_training_time_sec\", final_duration)\n",
    "    \n",
    "    mlflow.log_metric(\"avg_cv_train_mae\", np.mean(cv_train_mae))\n",
    "    mlflow.log_metric(\"avg_cv_train_rmse\", np.mean(cv_train_rmse))\n",
    "    \n",
    "    mlflow.log_metric(\"avg_cv_val_mae\", np.mean(cv_val_mae))\n",
    "    mlflow.log_metric(\"avg_cv_val_rmse\", np.mean(cv_val_rmse))\n",
    "    \n",
    "    mlflow.log_metric(\"final_train_mae\", final_train_mae)\n",
    "    mlflow.log_metric(\"final_train_rmse\", final_train_rmse)\n",
    "    \n",
    "    mlflow.log_metric(\"final_test_mae\", final_test_mae)\n",
    "    mlflow.log_metric(\"final_test_rmse\", final_test_rmse)\n",
    "    \n",
    "    mlflow.log_param(\"max_depth\", best_depth)\n",
    "    mlflow.log_param(\"n_estimators\", best_estimators)\n",
    "    mlflow.log_param(\"learning_rate\", best_lr)\n",
    "    \n",
    "    # Log Model\n",
    "    signature = infer_signature(train_val_df, final_train_preds)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        final_model, \n",
    "        \"final_stacked_xgb_model_5yr\",\n",
    "        input_example=train_val_df.limit(1).toPandas(),\n",
    "        signature=signature,\n",
    "        registered_model_name=\"final_stacked_xgb_model_5yr\"\n",
    "        )\n",
    "\n",
    "    # --- PART D: Saving ---\n",
    "    print(\"Saving Datasets...\")\n",
    "    \n",
    "    # Union all OOF folds back together (These are your \"Stacked\" training features)\n",
    "    full_train_val_with_preds = functools.reduce(DataFrame.union, oof_predictions)\n",
    "    \n",
    "    full_train_val_with_preds.write.mode(\"overwrite\").parquet(train_val_pred_path)\n",
    "    print(f\"Saved OOF predictions to {train_val_pred_path}\")\n",
    "    \n",
    "    test_preds_clean = test_preds.withColumnRenamed(\"prediction\", \"xgb_predicted_delay\")\n",
    "    test_preds_clean.write.mode(\"overwrite\").parquet(test_pred_path)\n",
    "    print(f\"Saved Test predictions to {test_pred_path}\")\n",
    "    \n",
    "    print(\"Success! Pipeline complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d151cbec-02cf-409a-8207-45eb93f6d31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 1. Config ---\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "base_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday_nnfeat/training_splits/\"\n",
    "\n",
    "# Input Paths (The outputs from your XGBoost step)\n",
    "train_val_path = base_path + \"train_val_predictions\"\n",
    "test_path = base_path + \"test_predictions\"\n",
    "\n",
    "# Output Paths for Final Classification Results\n",
    "final_train_output = base_path + \"mlp_classification_train_val_predictions\"\n",
    "final_test_output = base_path + \"mlp_classification_test_predictions\"\n",
    "\n",
    "# # --- 2. Safety UDF ---\n",
    "# @F.udf(returnType=BooleanType())\n",
    "# def vector_is_valid(v):\n",
    "#     if v is None: return False\n",
    "#     if np.any(np.isinf(v.values)): return False\n",
    "#     if np.any(np.isnan(v.values)): return False\n",
    "#     if np.max(np.abs(v.values)) > 1e30: return False\n",
    "#     return True\n",
    "\n",
    "SAFE_PARTITIONS = 2400\n",
    "\n",
    "# --- 3. Data Loading ---\n",
    "print(\"Loading Stacked Datasets...\")\n",
    "\n",
    "# Load Train/Val (Has OOF XGB predictions)\n",
    "# We sample slightly just to infer schema/partitions quickly, but we use full data for training\n",
    "train_df = spark.read.parquet(train_val_path).repartition(SAFE_PARTITIONS).cache()\n",
    "\n",
    "# Load Test (Has standard XGB predictions)\n",
    "test_df = spark.read.parquet(test_path).repartition(SAFE_PARTITIONS).cache()\n",
    "\n",
    "print(f\"Train Count: {train_df.count()}\")\n",
    "print(f\"Test Count:  {test_df.count()}\")\n",
    "\n",
    "# --- 4. Pipeline Definition ---\n",
    "# A. Define Indexers with NEW Output Names (_mlp)\n",
    "# This avoids the \"already exists\" error and forces re-calculation\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx_mlp\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx_mlp\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx_mlp\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx_mlp\", handleInvalid=\"keep\")\n",
    "holiday_indexer = StringIndexer(inputCol=\"IS_HOLIDAY\", outputCol=\"holiday_idx_mlp\", handleInvalid=\"keep\")\n",
    "holiday_window_indexer = StringIndexer(inputCol=\"IS_HOLIDAY_WINDOW\", outputCol=\"holiday_window_idx_mlp\", handleInvalid=\"keep\")\n",
    "airport_hub_indexer = StringIndexer(inputCol=\"AIRPORT_HUB_CLASS\", outputCol=\"airport_hub_idx_mlp\", handleInvalid=\"keep\")\n",
    "airline_category_indexer = StringIndexer(inputCol=\"AIRLINE_CATEGORY\", outputCol=\"airline_category_idx_mlp\", handleInvalid=\"keep\")\n",
    "\n",
    "# B. Define Encoders (Mapping _mlp indices to _mlp vectors)\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx_mlp\", outputCol=\"carrier_vec_mlp\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx_mlp\", outputCol=\"origin_vec_mlp\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx_mlp\", outputCol=\"dest_vec_mlp\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx_mlp\", outputCol=\"tail_num_vec_mlp\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_idx_mlp\", outputCol=\"holiday_vec_mlp\")\n",
    "holiday_window_encoder = OneHotEncoder(inputCol=\"holiday_window_idx_mlp\", outputCol=\"holiday_window_vec_mlp\")\n",
    "airport_hub_encoder = OneHotEncoder(inputCol=\"airport_hub_idx_mlp\", outputCol=\"airport_hub_vec_mlp\")\n",
    "airline_category_encoder = OneHotEncoder(inputCol=\"airline_category_idx_mlp\", outputCol=\"airline_category_vec_mlp\")\n",
    "\n",
    "# C. Assembler - Updates to use the NEW vectors (_mlp)\n",
    "# INCLUDES 'xgb_predicted_delay'\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\", \"MONTH\", \"YEAR\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec_mlp\", \"origin_vec_mlp\", \"dest_vec_mlp\",\n",
    "        \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"CRS_DEP_MINUTES\",\n",
    "        \"prev_flight_delay_in_minutes\", \"prev_flight_delay\", \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\", \"delay_origin_carrier_7d\", \"delay_route_7d\",\n",
    "        \"flight_count_24h\", \"LANDING_TIME_DIFF_MINUTES\", \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\", 'HourlyDryBulbTemperature', 'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity', 'HourlyAltimeterSetting', 'HourlyVisibility',\n",
    "        'HourlyStationPressure', 'HourlyWetBulbTemperature', 'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage', 'HourlyCloudElevation', 'HourlyWindSpeed',\n",
    "        'out_degree', 'in_degree', 'weighted_out_degree', 'weighted_in_degree',\n",
    "        'N_RUNWAYS', 'betweenness_unweighted', 'closeness', 'betweenness',\n",
    "        'avg_origin_dep_delay', 'avg_dest_arr_delay', 'avg_daily_route_flights',\n",
    "        'avg_route_delay', 'avg_hourly_flights',\n",
    "        \"holiday_vec_mlp\", \"holiday_window_vec_mlp\", \"airport_hub_vec_mlp\",\n",
    "        \"RATING\", \"airline_category_vec_mlp\",\n",
    "        \"ground_flights_last_hour\", \"arrivals_last_hour\", \"dow_sin\", \"dow_cos\", \"doy_sin\", \"doy_cos\",\n",
    "        \"xgb_predicted_delay\" # <--- The Stacking Feature\n",
    "    ],\n",
    "    outputCol=\"raw_mlp_features\"\n",
    ")\n",
    "\n",
    "# Scaler (Critical for Neural Networks)\n",
    "scaler = StandardScaler(inputCol=\"raw_mlp_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "prep_pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    tail_num_indexer, holiday_indexer, holiday_window_indexer, \n",
    "    airport_hub_indexer, airline_category_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    tail_num_encoder, holiday_encoder, holiday_window_encoder, \n",
    "    airport_hub_encoder, airline_category_encoder,\n",
    "    assembler, \n",
    "    scaler\n",
    "])\n",
    "\n",
    "print(\"Fitting Preprocessing Pipeline...\")\n",
    "prep_model = prep_pipeline.fit(train_df)\n",
    "\n",
    "print(\"Transforming Data...\")\n",
    "full_train_vec = prep_model.transform(train_df).persist(StorageLevel.DISK_ONLY)\n",
    "test_vec = prep_model.transform(test_df).persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# --- 5. FIX: Class Balancing (The Solution to F2=0) ---\n",
    "print(\"Balancing Training Data...\")\n",
    "# Separate classes\n",
    "positives = full_train_vec.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "negatives = full_train_vec.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "\n",
    "pos_count = positives.count()\n",
    "neg_count = negatives.count()\n",
    "\n",
    "print(f\"  Found {pos_count} Positives and {neg_count} Negatives.\")\n",
    "\n",
    "# Undersample Negatives to match Positives\n",
    "fraction = pos_count / neg_count\n",
    "negatives_sampled = negatives.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "\n",
    "# Combine and Repartition for training\n",
    "train_balanced = positives.union(negatives_sampled).repartition(SAFE_PARTITIONS)\n",
    "print(f\"  Balanced Training Set: {train_balanced.count()} rows.\")\n",
    "\n",
    "# --- 6. MLP Definition & Training ---\n",
    "input_dim = len(full_train_vec.first()[\"scaled_features\"])\n",
    "print(f\"MLP Input Dimension: {input_dim}\")\n",
    "\n",
    "layers = [input_dim, 128, 64, 2]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    layers=layers,\n",
    "    solver=\"gd\",     # Gradient Descent for low memory usage\n",
    "    blockSize=128,   \n",
    "    stepSize=0.03,\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "# Evaluators\n",
    "f2_label_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"fMeasureByLabel\", metricLabel=1.0, beta=2.0)\n",
    "\n",
    "print(\"Training Final MLP...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"FINAL_STACKED_MLP_5_YR\") as run:\n",
    "    mlflow.log_param(\"layers\", str(layers))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # TRAIN ON BALANCED DATA\n",
    "    mlp_model = mlp.fit(train_balanced)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    mlflow.log_metric(\"training_duration_seconds\", training_time)\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # --- 7. Evaluation ---\n",
    "    print(\"Evaluating on Train (Balanced)...\")\n",
    "    train_preds = mlp_model.transform(train_balanced)\n",
    "    t_f2 = f2_label_evaluator.evaluate(train_preds)\n",
    "    print(f\"  Train (Bal): Delay-F2={t_f2:.4f}\")\n",
    "    \n",
    "    print(\"Evaluating on Test (Full/Imbalanced Holdout)...\")\n",
    "    test_preds = mlp_model.transform(test_vec)\n",
    "    test_f2 = f2_label_evaluator.evaluate(test_preds)\n",
    "    print(f\"  Test:        Delay-F2={test_f2:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"train_f2_delay\", t_f2)\n",
    "    mlflow.log_metric(\"test_f2_delay\", test_f2)\n",
    "    \n",
    "    # --- 7b. FIX: Threshold Tuning (The Magic Step) ---\n",
    "    print(\"\\n--- Tuning Threshold on Full Training Data ---\")\n",
    "\n",
    "    # 1. Get predictions on the FULL (Imbalanced) training set\n",
    "    # We need to tune on the real class distribution, not the balanced one.\n",
    "    train_full_preds = mlp_model.transform(full_train_vec)\n",
    "\n",
    "    # UDF to extract the probability of Class 1 (Delay)\n",
    "    from pyspark.sql.types import DoubleType\n",
    "    extract_prob = F.udf(lambda v: float(v[1]), DoubleType())\n",
    "\n",
    "    # Add probability column\n",
    "    train_with_prob = train_full_preds.withColumn(\"prob_delay\", extract_prob(F.col(\"probability\")))\n",
    "    test_with_prob = test_preds.withColumn(\"prob_delay\", extract_prob(F.col(\"probability\")))\n",
    "\n",
    "    # 2. Grid Search for Best Threshold\n",
    "    best_threshold = 0.5\n",
    "    best_f2 = 0.0\n",
    "    thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "    print(\"Searching for best threshold...\")\n",
    "    for t in thresholds:\n",
    "        # Apply threshold dynamically\n",
    "        preds_t = train_with_prob.withColumn(\"prediction\", F.when(F.col(\"prob_delay\") >= t, 1.0).otherwise(0.0))\n",
    "        \n",
    "        # Calculate F2\n",
    "        f2 = f2_label_evaluator.evaluate(preds_t)\n",
    "        print(f\"  Threshold {t}: F2 = {f2:.4f}\")\n",
    "        \n",
    "        if f2 > best_f2:\n",
    "            best_f2 = f2\n",
    "            best_threshold = t\n",
    "\n",
    "    print(f\"\\nWINNER: Best Threshold = {best_threshold} (Train F2={best_f2:.4f})\")\n",
    "    mlflow.log_param(\"best_threshold\", best_threshold)\n",
    "\n",
    "    # 3. Apply Winner to TEST Set (Final Evaluation)\n",
    "    print(f\"Applying threshold {best_threshold} to Test Set...\")\n",
    "    final_test_preds = test_with_prob.withColumn(\"prediction\", F.when(F.col(\"prob_delay\") >= best_threshold, 1.0).otherwise(0.0))\n",
    "\n",
    "    final_test_f2 = f2_label_evaluator.evaluate(final_test_preds)\n",
    "    # final_test_prec = prec_evaluator.evaluate(final_test_preds)\n",
    "    # final_test_rec = rec_evaluator.evaluate(final_test_preds)\n",
    "\n",
    "    print(f\"  FINAL TEST SCORE: Delay-F2 = {final_test_f2:.4f}\")\n",
    "    # print(f\"  (Precision={final_test_prec:.4f}, Recall={final_test_rec:.4f})\")\n",
    "\n",
    "    mlflow.log_metric(\"final_test_f2_optimized\", final_test_f2)\n",
    "\n",
    "    # Update the dataframes to save so they include the optimized prediction\n",
    "    train_preds = train_full_preds.withColumn(\"prediction\", F.when(extract_prob(F.col(\"probability\")) >= best_threshold, 1.0).otherwise(0.0))\n",
    "    test_preds = final_test_preds\n",
    "\n",
    "    # --- 8. FIX: Input Example Serialization ---\n",
    "    # We must DROP vector columns from the example to avoid JSON errors\n",
    "    # We pick just a few scalar columns to define the schema safely\n",
    "    print(\"Logging Model...\")\n",
    "    \n",
    "    # Identify vector columns to exclude from the example\n",
    "    vector_cols = [c.name for c in train_df.schema if \"VectorUDT\" in str(c.dataType)]\n",
    "    # Also exclude the generated _mlp vectors\n",
    "    vector_cols += [\"raw_mlp_features\", \"scaled_features\"]\n",
    "    \n",
    "    # Create a clean example using only scalar columns\n",
    "    # We take 1 row from the original DF and drop known vector columns\n",
    "    example_pd = train_df.drop(*vector_cols).limit(1).toPandas()\n",
    "    \n",
    "    # Infer signature from the scalar inputs + prediction\n",
    "    signature = infer_signature(example_pd, test_preds.limit(1).select(\"prediction\").toPandas())\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        mlp_model, \n",
    "        \"final_stacked_mlp_model_5yr\",\n",
    "        input_example=example_pd, # Clean example\n",
    "        signature=signature,\n",
    "        registered_model_name=\"final_stacked_mlp_model_5yr\"\n",
    "    )\n",
    "    \n",
    "    # --- 9. Save Final Predictions ---\n",
    "    print(\"Saving Final Predictions...\")\n",
    "    train_preds.write.mode(\"overwrite\").parquet(final_train_output)\n",
    "    test_preds.write.mode(\"overwrite\").parquet(final_test_output)\n",
    "    print(\"Success! Final Stacked Pipeline Complete.\")\n",
    "\n",
    "# Cleanup\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "full_train_vec.unpersist()\n",
    "test_vec.unpersist()\n",
    "train_balanced.unpersist()\n",
    "\n",
    "# with mlflow.start_run(run_name=\"FINAL_STACKED_MLP_5_YR\") as run:\n",
    "#     mlflow.log_param(\"layers\", str(layers))\n",
    "    \n",
    "#     # Train\n",
    "#     start_time = time.time()\n",
    "#     mlp_model = mlp.fit(train_vec)\n",
    "#     training_time = time.time() - start_time\n",
    "    \n",
    "#     mlflow.log_metric(\"training_duration_seconds\", training_time)\n",
    "#     print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "#     # --- 6. Predictions & Evaluation ---\n",
    "#     print(\"Evaluating on Train/Val (Self)...\")\n",
    "#     train_preds = mlp_model.transform(train_vec)\n",
    "    \n",
    "#     train_f2_delay = f2_label_evaluator.evaluate(train_preds)\n",
    "    \n",
    "#     print(f\"  Delay-F2={train_f2_delay:.4f}\")\n",
    "    \n",
    "#     print(\"Evaluating on Test (Holdout)...\")\n",
    "#     test_preds = mlp_model.transform(test_vec)\n",
    "    \n",
    "#     test_f2_delay = f2_label_evaluator.evaluate(test_preds)\n",
    "    \n",
    "#     print(f\"  Delay-F2={test_f2_delay:.4f}\")\n",
    "    \n",
    "#     # Log Metrics\n",
    "#     mlflow.log_metric(\"train_f2_delay\", train_f2_delay)\n",
    "\n",
    "#     mlflow.log_metric(\"test_f2_delay\", test_f2_delay)\n",
    "    \n",
    "#     # Log Model\n",
    "#     mlflow.spark.log_model(mlp_model, \"final_mlp_model\")\n",
    "#     signature = infer_signature(train_df, train_preds)\n",
    "\n",
    "#     mlflow.spark.log_model(\n",
    "#         mlp_model, \n",
    "#         \"final_stacked_mlp_model_5yr\",\n",
    "#         input_example=train_df.limit(1).toPandas(),\n",
    "#         signature=signature,\n",
    "#         registered_model_name=\"final_stacked_mlp_model_5yr\"\n",
    "#         )\n",
    "    \n",
    "#     # --- 7. Save Final Predictions ---\n",
    "#     print(\"Saving Final Predictions...\")\n",
    "    \n",
    "#     # Join predictions back with original features if needed, or just save preds\n",
    "#     # Here we save the simplified prediction output\n",
    "#     train_preds.write.mode(\"overwrite\").parquet(final_train_output)\n",
    "#     test_preds.write.mode(\"overwrite\").parquet(final_test_output)\n",
    "    \n",
    "#     print(\"Success! Final Stacked Pipeline Complete.\")\n",
    "\n",
    "# # Cleanup\n",
    "# train_df.unpersist()\n",
    "# test_df.unpersist()\n",
    "# train_vec.unpersist()\n",
    "# test_vec.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c642052-0ba7-463e-aad4-437084c57385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom thresholding UDF or logic\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Function to extract probability of Class 1 and apply threshold\n",
    "# Vector index 1 is the probability of Positive (Delay)\n",
    "extract_prob = F.udf(lambda v: float(v[1]), DoubleType())\n",
    "\n",
    "test_preds_prob = test_preds.withColumn(\"prob_delay\", extract_prob(F.col(\"probability\")))\n",
    "\n",
    "# Check F2 at different thresholds\n",
    "for t in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    # Manually classify based on threshold\n",
    "    preds_thresholded = test_preds_prob.withColumn(\"prediction\", F.when(F.col(\"prob_delay\") >= t, 1.0).otherwise(0.0))\n",
    "    \n",
    "    f2 = f2_label_evaluator.evaluate(preds_thresholded)\n",
    "    print(f\"Threshold {t}: F2 Score = {f2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a92b793-f5c0-4d71-af6d-ace0ca3e9023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5yr_stacked_NN_with_CV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
