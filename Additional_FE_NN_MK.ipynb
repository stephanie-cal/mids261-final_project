{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "348372bf-80e0-44c9-885b-7951db0851da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Add additional features needed for NN Tower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ca42b2-1faa-46f0-9daf-b5c20567f1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05bfc82-fd4a-491a-a017-28f3ccd80642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bdd5143-0a0a-469d-87ea-62b5179d2d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b76c216-b170-417d-826f-bb549cc3b4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "112314a8-b1f3-4bc6-9195-61aacb0148fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets (Custom Join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04830b3-263b-4b1f-8c67-6ef6ccb53dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet/</td><td>test.parquet/</td><td>0</td><td>1765424905930</td></tr><tr><td>dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/train.parquet/</td><td>train.parquet/</td><td>0</td><td>1765424905930</td></tr><tr><td>dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/validation.parquet/</td><td>validation.parquet/</td><td>0</td><td>1765424905930</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet/",
         "test.parquet/",
         0,
         1765424905930
        ],
        [
         "dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/train.parquet/",
         "train.parquet/",
         0,
         1765424905930
        ],
        [
         "dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/validation.parquet/",
         "validation.parquet/",
         0,
         1765424905930
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b138b5c6-57d7-4a06-b26c-e6cf0d893987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/train.parquet/\"\n",
    ") # or 1_year_custom_joined\n",
    "val_df = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/validation.parquet/\"\n",
    ") # or 1_year_custom_joined\n",
    "test_df = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet/\"\n",
    ") # or 1_year_custom_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182cd0a4-0261-4477-aed3-f7464f58937a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f447807a-8b8b-40bb-ba75-f68b1deedcb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Time-derived cyclic features\n",
    "# -----------------------------\n",
    "def add_time_features(df):\n",
    "    # Hour of departure as float\n",
    "    df = df.withColumn(\"dep_hour\", F.col(\"CRS_DEP_MINUTES\") / 60.0)\n",
    "\n",
    "    # Day of year\n",
    "    df = df.withColumn(\"day_of_year\", F.dayofyear(\"utc_timestamp\").cast(\"double\"))\n",
    "\n",
    "    # Cyclic transforms (double precision)\n",
    "    df = df.withColumn(\"dep_hour_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"dep_hour\") / 24))\n",
    "    df = df.withColumn(\"dep_hour_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"dep_hour\") / 24))\n",
    "\n",
    "    df = df.withColumn(\"dow_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"DAY_OF_WEEK\") / 7))\n",
    "    df = df.withColumn(\"dow_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"DAY_OF_WEEK\") / 7))\n",
    "\n",
    "    df = df.withColumn(\"doy_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"day_of_year\") / 365))\n",
    "    df = df.withColumn(\"doy_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"day_of_year\") / 365))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0642d3fa-fcc0-4061-83c8-8f49fa0ab7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Weather delta features (3-hour changes)\n",
    "# -----------------------------\n",
    "def add_weather_deltas(df):\n",
    "    w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_timestamp\")\n",
    "    \n",
    "    for col in [\n",
    "        \"HourlyVisibility\", \"HourlyStationPressure\",\n",
    "        \"HourlyDryBulbTemperature\", \"HourlyWindSpeed\",\n",
    "        \"HourlyPrecipitation\"\n",
    "    ]:\n",
    "        lag_col = F.lag(col, 3).over(w)\n",
    "        delta_col = F.col(col) - lag_col\n",
    "        # Use lag value if missing instead of 0 to avoid small bias\n",
    "        df = df.withColumn(\n",
    "            f\"{col}_3h_change\",\n",
    "            F.when(lag_col.isNull(), F.lit(None)).otherwise(delta_col)\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb79ceb-9ff4-4fc9-872c-fca2778a0a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Origin congestion features\n",
    "# -----------------------------\n",
    "def add_congestion_features(df):\n",
    "    # Rolling window: 1 hour before current event\n",
    "    df = df.withColumn(\"utc_ts_sec\", F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "\n",
    "    w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_ts_sec\").rangeBetween(-3600, 0)\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"ground_flights_last_hour\",\n",
    "        F.count(\"utc_ts_sec\").over(w) - 1\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1df849-d374-444a-a875-635fb5e14107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Destination congestion features\n",
    "# -----------------------------\n",
    "def add_dest_congestion_features(df):\n",
    "    # Convert timestamp to seconds\n",
    "    df = df.withColumn(\"utc_ts_sec\", F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "    \n",
    "    # Rolling window: 1 hour (3600 seconds) before current row\n",
    "    w = Window.partitionBy(\"DEST_AIRPORT_SEQ_ID\") \\\n",
    "              .orderBy(\"utc_ts_sec\") \\\n",
    "              .rangeBetween(-3600, 0)\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"arrivals_last_hour\",\n",
    "        F.count(\"utc_ts_sec\").over(w) - 1  # exclude current row\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406642bf-7462-46bf-a8f8-a9b06e290a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Apply additional feature engineering\n",
    "train_df_fe = (train_df\n",
    "               .transform(add_time_features)\n",
    "               .transform(add_weather_deltas)\n",
    "               .transform(add_congestion_features)\n",
    "               .transform(add_dest_congestion_features))\n",
    "\n",
    "val_df_fe   = (val_df\n",
    "               .transform(add_time_features)\n",
    "               .transform(add_weather_deltas)\n",
    "               .transform(add_congestion_features)\n",
    "               .transform(add_dest_congestion_features))\n",
    "\n",
    "test_df_fe  = (test_df\n",
    "               .transform(add_time_features)\n",
    "               .transform(add_weather_deltas)\n",
    "               .transform(add_congestion_features)\n",
    "               .transform(add_dest_congestion_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39127673-372a-4bc1-84e3-4b3286df2e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointed 5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train\nCheckpointed 5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val\nCheckpointed 5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test\n"
     ]
    }
   ],
   "source": [
    "## Checkpoint updated data\n",
    "\n",
    "checkpoint_dataset(train_df_fe, \"5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train\")\n",
    "checkpoint_dataset(val_df_fe, \"5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val\")\n",
    "checkpoint_dataset(test_df_fe, \"5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Additional_FE_NN_MK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}