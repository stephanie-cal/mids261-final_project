{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PIN_THREAD'] = 'false'\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-mlp-layers\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87f01ff-92b7-46da-bb8f-79a4bba3cef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38fde98-90c8-474d-8199-574441d019ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_root = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/nn_layers\"\n",
    "\n",
    "# Load in PyTorch\n",
    "train_df = spark.read.parquet(f\"{output_root}/train_pytorch.parquet\")\n",
    "validation_df   = spark.read.parquet(f\"{output_root}/val_pytorch.parquet\")\n",
    "test_df  = spark.read.parquet(f\"{output_root}/test_pytorch.parquet\")\n",
    "\n",
    "# # Convert to torch tensors\n",
    "# x_cat_train = torch.tensor(train_df[categorical_idx_cols].values, dtype=torch.long)\n",
    "# x_num_train = torch.tensor(train_df[numerical_cols].values, dtype=torch.float)\n",
    "# x_time_train = torch.tensor(train_df[[time_col]].values, dtype=torch.float)\n",
    "# y_train = torch.tensor(train_df[target_col].values, dtype=torch.float).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efed1655-84c8-4947-934c-0b7956a52912",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763839566213}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Combine train + validation\n",
    "df = train_df.unionByName(validation_df)\n",
    "\n",
    "# Filter out cancelled flights\n",
    "# df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "\n",
    "print(f\"Number of rows after filtering: {df.count()}\")\n",
    "display(df.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7bbd64-8024-444a-9b1c-60e5a6abd229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # combine date and scheduled departure time\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Combine date and scheduled departure time into a timestamp\n",
    "# df = df.withColumn(\n",
    "#     \"utc_timestamp\",\n",
    "#     F.to_timestamp(\n",
    "#         F.concat(\n",
    "#             F.col(\"FL_DATE\"),               # Flight date\n",
    "#             F.lit(\" \"),                      # Space separator\n",
    "#             F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")  # Ensure 4-digit HHmm\n",
    "#         ),\n",
    "#         \"yyyy-MM-dd HHmm\"                   # Format to parse\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# display(df.select(\"FL_DATE\", \"CRS_DEP_TIME\", \"utc_timestamp\").limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c11636-0f97-4444-ae07-2f093109b2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Splits for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bfe8bf3-76f2-44da-8527-e63e29e68ade",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765146377344}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Truncate timestamp to hour level\n",
    "df_indexed = df.withColumn(\n",
    "    \"hour\", \n",
    "    F.date_trunc(\"hour\", F.col(\"utc_timestamp\"))\n",
    ")\n",
    "\n",
    "# Create time index based on unique hours\n",
    "window_spec = Window.orderBy(\"hour\")\n",
    "df_indexed = df_indexed.withColumn(\n",
    "    \"time_idx\", \n",
    "    F.dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "df_indexed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e6ac15a-407b-41b7-a670-a6c09ede6b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 M splits config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de439e60-8e63-4173-990e-96c348187267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_time_idx = df_indexed.agg(F.max(\"time_idx\")).collect()[0][0]\n",
    "print(f\"  Max time index: {max_time_idx}\")\n",
    "\n",
    "train_size = 720      # 30 days (720 hours)\n",
    "gap_size = 2          # 2 hours\n",
    "val_size = 168        # 7 days (168 hours)\n",
    "step_size = 85       # Calculated to get exactly 10 folds\n",
    "\n",
    "fold_window_size = train_size + gap_size + val_size\n",
    "n_folds = (max_time_idx - fold_window_size) // step_size + 1\n",
    "print(f\"Step 2: Calculated {n_folds} folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ace5659-c091-4095-b81f-c896fe02d42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 Year splits config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c017bd8e-4e69-47ba-8e42-aafc5361c4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_time_idx = df_indexed.agg(F.max(\"time_idx\")).collect()[0][0]\n",
    "print(f\"  Max time index: {max_time_idx}\")\n",
    "\n",
    "train_size = 720*4    # 30 days (720 hours)\n",
    "gap_size = 2          # 2 hours\n",
    "val_size = 168*4      # 7 days (168 hours)\n",
    "step_size = 90*4      # Calculated to get exactly 10 folds\n",
    "\n",
    "fold_window_size = train_size + gap_size + val_size\n",
    "n_folds = (max_time_idx - fold_window_size) // step_size + 1\n",
    "print(f\"Step 2: Calculated {n_folds} folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7245718-dde9-4742-8143-4bffc3b59a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5 Year splits config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a70bff7-db39-4892-9d38-e0647de70ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# max_time_idx = df_indexed.agg(F.max(\"time_idx\")).collect()[0][0]\n",
    "# print(f\"  Max time index: {max_time_idx}\")\n",
    "\n",
    "# train_size = 720*4*5      # 30 days (720 hours)\n",
    "# gap_size = 2          # 2 hours\n",
    "# val_size = 168*4*5        # 7 days (168 hours)\n",
    "# step_size = 90*4*5       # Calculated to get exactly 10 folds\n",
    "\n",
    "# fold_window_size = train_size + gap_size + val_size\n",
    "# n_folds = (max_time_idx - fold_window_size) // step_size + 1\n",
    "# print(f\"Step 2: Calculated {n_folds} folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf9f18a-7e97-4901-812e-6397b27b7ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_mapping = []\n",
    "\n",
    "for fold_id in range(1, n_folds + 1):\n",
    "    fold_start = 1 + (fold_id - 1) * step_size\n",
    "    # print(fold_id, fold_start)\n",
    "    for t in range(fold_start, fold_start + train_size):\n",
    "        fold_mapping.append((t, fold_id, \"train\"))\n",
    "    \n",
    "    for t in range(fold_start + train_size, fold_start + train_size + gap_size):\n",
    "        fold_mapping.append((t, fold_id, \"gap\"))\n",
    "\n",
    "    for t in range(fold_start + train_size + gap_size, fold_start + train_size + gap_size + val_size):\n",
    "        fold_mapping.append((t, fold_id, \"validation\"))\n",
    "\n",
    "fold_df = spark.createDataFrame(fold_mapping, [\"time_idx\", \"fold_id\", \"split_type\"])\n",
    "\n",
    "result = df_indexed.join(\n",
    "    F.broadcast(fold_df),\n",
    "    on='time_idx',\n",
    "    how='inner'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9d0f39-847c-4e38-acfe-0a5266787463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# month_or_year = \"3_month_\"\n",
    "if input(\"Careful! About to overwrite splits. If you want to continue, type y\") == \"y\":\n",
    "    result.write \\\n",
    "    .partitionBy(\"fold_id\", \"split_type\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/cv_splits_nn_layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7119ff85-a394-41b5-9644-caedb89f27fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to read the CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488c9531-9a7a-45b7-9837-ef369d4b230d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    return spark.read.parquet(f\"{path}/fold_id={fold_id}/split_type={split_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f5528e-6b64-4016-84b0-5228a19e0bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n",
    "\n",
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"tail_num_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44d6630-570b-4ad1-ac75-7fbf7fd8fda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Model architecture defined\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Residual Block\n",
    "# -----------------------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.gelu(self.fc1(self.ln(x)))\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Time2Vec\n",
    "# -----------------------------\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.wb = nn.Linear(1, 1)\n",
    "        self.ws = nn.Linear(1, k)\n",
    "\n",
    "    def forward(self, t):\n",
    "        b = self.wb(t)\n",
    "        s = torch.sin(self.ws(t))\n",
    "        return torch.cat([b, s], dim=-1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ResFiLM MLP (F2-optimized)\n",
    "# -----------------------------\n",
    "class ResFiLMMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_dims,\n",
    "        emb_dims,\n",
    "        num_numerical,\n",
    "        time_dim=8,\n",
    "        emb_dropout=0.05,\n",
    "        num_dropout=0.1,\n",
    "        film_dropout=0.1,\n",
    "        final_dropout=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Embedding tower ---\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, emb_dim)\n",
    "            for cat_dim, emb_dim in zip(cat_dims, emb_dims)\n",
    "        ])\n",
    "        self.emb_total = sum(emb_dims)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        # --- Numeric tower ---\n",
    "        self.fc_num = nn.Linear(num_numerical, 256)\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlock(256, dropout=num_dropout)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "\n",
    "        # --- FiLM for embeddings ---\n",
    "        self.film = nn.Linear(256, 2 * self.emb_total)\n",
    "        self.film_dropout = nn.Dropout(film_dropout)\n",
    "\n",
    "        # --- Time2Vec ---\n",
    "        self.t2v = Time2Vec(time_dim)\n",
    "\n",
    "        # --- Optional: classification-specific FiLM ---\n",
    "        self.clf_film = nn.Linear(256, 2 * self.emb_total)\n",
    "        self.clf_film_dropout = nn.Dropout(film_dropout)\n",
    "\n",
    "        # --- Final fusion dimension ---\n",
    "        fused_dim = 256 + self.emb_total + (time_dim + 1) + 1\n",
    "\n",
    "        # -------------------------------\n",
    "        # Multi-task heads\n",
    "        # -------------------------------\n",
    "\n",
    "        # Regression head (delay minutes)\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(128, 1)  # raw regression output\n",
    "        )\n",
    "\n",
    "        # Classification head (delay yes/no) – deeper\n",
    "        self.clf_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(128, 1)  # raw logit, no Sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num, x_time):\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        emb = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "        emb = torch.cat(emb, dim=-1)\n",
    "        emb = self.emb_dropout(emb)\n",
    "\n",
    "        # --- Numeric tower ---\n",
    "        h = F.gelu(self.fc_num(x_num))\n",
    "        for block in self.res_blocks:\n",
    "            h = block(h)\n",
    "\n",
    "        # --- FiLM modulation ---\n",
    "        gamma, beta = torch.chunk(self.film(h), 2, dim=-1)\n",
    "        gamma = self.film_dropout(gamma)\n",
    "        beta = self.film_dropout(beta)\n",
    "        emb_mod = gamma * emb + beta\n",
    "\n",
    "        # --- Time2Vec ---\n",
    "        t_feat = self.t2v(x_time)\n",
    "\n",
    "        # --- Fuse towers ---\n",
    "        z = torch.cat([emb_mod, h, t_feat, x_time], dim=-1)\n",
    "\n",
    "        # --- Output tasks ---\n",
    "        reg_out = self.reg_head(z)       # regression output\n",
    "        clf_out = self.clf_head(z)       # classification logit (no sigmoid)\n",
    "\n",
    "        return reg_out, clf_out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example F2-focused loss function\n",
    "# -----------------------------\n",
    "def f2_loss(logits, targets, pos_weight=4.0):\n",
    "    # logits: raw outputs from clf_head\n",
    "    # targets: binary labels (0/1)\n",
    "    weight = torch.tensor([pos_weight], device=logits.device)\n",
    "    return nn.BCEWithLogitsLoss(pos_weight=weight)(logits, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e732e7e-3b42-4296-85e1-5c3d78506174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    \"\"\"\n",
    "    Read a specific fold from partitioned parquet data.\n",
    "    Falls back to filtering if direct partition read fails.\n",
    "    \"\"\"\n",
    "    fold_path = f\"{path}/fold_id={fold_id}/split_type={split_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Try direct partition read\n",
    "        return spark.read.parquet(fold_path)\n",
    "    except:\n",
    "        # Fallback: read all data and filter\n",
    "        print(f\"Direct read failed for fold {fold_id}, using filter method...\")\n",
    "        all_data = spark.read.parquet(path)\n",
    "        return all_data.filter(\n",
    "            (all_data.fold_id == fold_id) & \n",
    "            (all_data.split_type == split_type)\n",
    "        )\n",
    "\n",
    "\n",
    "# Your original train function works as-is now\n",
    "def train_cv_resfilm(\n",
    "        n_folds=10,\n",
    "        month_or_year=\"1_year_custom_joined\",\n",
    "        params=params\n",
    "    ):\n",
    "    \n",
    "    cv_results = []\n",
    "    cv_models = []\n",
    "    \n",
    "    mlflow.pytorch.autolog(log_models=False)  # you will log manually\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"ResFiLM_CV_{month_or_year}\") as parent_run:\n",
    "        \n",
    "        mlflow.log_param(\"n_folds\", n_folds)\n",
    "        mlflow.log_param(\"model_type\", \"ResFiLMMLP\")\n",
    "        mlflow.log_param(\"dataset\", month_or_year)\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        for fold_id in range(1, n_folds + 1):\n",
    "\n",
    "            # ============================\n",
    "            # LOAD THIS FOLD'S TRAIN + VAL\n",
    "            # ============================\n",
    "            fold_train_df = read_specific_fold(\n",
    "                path=f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\",\n",
    "                fold_id=fold_id,\n",
    "                split_type=\"train\"\n",
    "            )\n",
    "            fold_val_df = read_specific_fold(\n",
    "                path=f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\",\n",
    "                fold_id=fold_id,\n",
    "                split_type=\"validation\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\n===== FOLD {fold_id} =====\")\n",
    "            print(f\"Train rows: {fold_train_df.count()}, Val rows: {fold_val_df.count()}\")\n",
    "\n",
    "            # Convert Spark → Pandas → PyTorch-ready numpy arrays\n",
    "            train_pd = fold_train_df.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "            val_pd   = fold_val_df.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "\n",
    "            # Build dataloaders\n",
    "            train_dl = make_dataloader(train_pd, params[\"batch_size\"], shuffle=True)\n",
    "            val_dl   = make_dataloader(val_pd, params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "            # ============================\n",
    "            # NEW Pytorch Model For This Fold\n",
    "            # ============================\n",
    "            model = ResFiLMMLP(\n",
    "                cat_dims,\n",
    "                emb_dims,\n",
    "                num_numerical=len(numerical_cols),\n",
    "                time_dim=params[\"time_dim\"]\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=params[\"lr\"],\n",
    "                weight_decay=params[\"weight_decay\"]\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=2\n",
    "            )\n",
    "            clf_loss_fn = nn.BCEWithLogitsLoss(\n",
    "                pos_weight=torch.tensor(params[\"pos_weight\"]).to(device)\n",
    "            )\n",
    "            reg_loss_fn = nn.L1Loss()\n",
    "\n",
    "            best_val_f2 = -1\n",
    "            best_threshold = 0.5\n",
    "            patience_counter = 0\n",
    "\n",
    "            # ============================\n",
    "            # MLflow nested run for this fold\n",
    "            # ============================\n",
    "            with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True):\n",
    "\n",
    "                for epoch in range(params[\"num_epochs\"]):\n",
    "\n",
    "                    # -----------------------------------------\n",
    "                    # TRAIN\n",
    "                    # -----------------------------------------\n",
    "                    model.train()\n",
    "                    y_true_reg_train = []\n",
    "                    y_pred_reg_train = []\n",
    "                    y_true_clf_train = []\n",
    "                    y_pred_clf_train = []\n",
    "\n",
    "                    for x_cat, x_num, x_time, y_reg in train_dl:\n",
    "                        x_cat = x_cat.to(device).long()\n",
    "                        x_num = x_num.to(device).float()\n",
    "                        x_time = x_time.to(device).float()\n",
    "                        y_reg = y_reg.to(device).float().view(-1, 1)\n",
    "                        y_clf = (y_reg > 15).float()\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        pred_reg, pred_clf_logits = model(x_cat, x_num, x_time)\n",
    "\n",
    "                        loss_reg = reg_loss_fn(pred_reg, y_reg)\n",
    "                        loss_clf = clf_loss_fn(pred_clf_logits, y_clf)\n",
    "                        loss = loss_reg + params[\"clf_loss_weight\"] * loss_clf\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        # collect preds\n",
    "                        y_true_reg_train.append(y_reg.cpu().numpy())\n",
    "                        y_pred_reg_train.append(pred_reg.detach().cpu().numpy())\n",
    "                        y_true_clf_train.append(y_clf.cpu().numpy())\n",
    "                        y_pred_clf_train.append(torch.sigmoid(pred_clf_logits).detach().cpu().numpy())\n",
    "\n",
    "                    # flatten train predictions\n",
    "                    y_true_reg_train = np.concatenate(y_true_reg_train)\n",
    "                    y_pred_reg_train = np.concatenate(y_pred_reg_train)\n",
    "                    y_true_clf_train = np.concatenate(y_true_clf_train).reshape(-1)\n",
    "                    y_pred_clf_train = np.concatenate(y_pred_clf_train).reshape(-1)\n",
    "                    train_pred_binary = (y_pred_clf_train >= 0.5).astype(int)\n",
    "\n",
    "                    train_f2 = fbeta_score(y_true_clf_train, train_pred_binary, beta=2, zero_division=0)\n",
    "                    train_mae = mean_absolute_error(y_true_reg_train, y_pred_reg_train)\n",
    "\n",
    "                    # -----------------------------------------\n",
    "                    # VALIDATION\n",
    "                    # -----------------------------------------\n",
    "                    model.eval()\n",
    "                    y_true_reg_val = []\n",
    "                    y_pred_reg_val = []\n",
    "                    y_true_clf_val = []\n",
    "                    y_pred_clf_val = []\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for x_cat, x_num, x_time, y_reg in val_dl:\n",
    "                            x_cat = x_cat.to(device).long()\n",
    "                            x_num = x_num.to(device).float()\n",
    "                            x_time = x_time.to(device).float()\n",
    "                            y_reg = y_reg.to(device).float().view(-1, 1)\n",
    "                            y_clf = (y_reg > 15).float()\n",
    "\n",
    "                            pred_reg, pred_clf_logits = model(x_cat, x_num, x_time)\n",
    "\n",
    "                            y_true_reg_val.append(y_reg.cpu().numpy())\n",
    "                            y_pred_reg_val.append(pred_reg.cpu().numpy())\n",
    "                            y_true_clf_val.append(y_clf.cpu().numpy())\n",
    "                            y_pred_clf_val.append(torch.sigmoid(pred_clf_logits).cpu().numpy())\n",
    "\n",
    "                    # flatten val predictions\n",
    "                    y_true_reg_val = np.concatenate(y_true_reg_val)\n",
    "                    y_pred_reg_val = np.concatenate(y_pred_reg_val)\n",
    "                    y_true_clf_val = np.concatenate(y_true_clf_val).reshape(-1)\n",
    "                    y_pred_clf_val = np.concatenate(y_pred_clf_val).reshape(-1)\n",
    "\n",
    "                    # --- Find best threshold for F2\n",
    "                    thresholds = np.linspace(0.05, 0.95, 40)\n",
    "                    f2_scores = [fbeta_score(y_true_clf_val,\n",
    "                                             (y_pred_clf_val >= t).astype(int),\n",
    "                                             beta=2,\n",
    "                                             zero_division=0)\n",
    "                                 for t in thresholds]\n",
    "                    best_idx = np.argmax(f2_scores)\n",
    "                    val_f2 = f2_scores[best_idx]\n",
    "                    val_thresh = thresholds[best_idx]\n",
    "\n",
    "                    val_mae = mean_absolute_error(y_true_reg_val, y_pred_reg_val)\n",
    "\n",
    "                    print(f\"[Fold {fold_id} | Epoch {epoch}] Val F2={val_f2:.4f}  | Best Thresh={val_thresh:.2f}  | Val MAE={val_mae:.4f}\")\n",
    "\n",
    "                    # MLflow\n",
    "                    mlflow.log_metrics({\n",
    "                        \"train_mae\": train_mae,\n",
    "                        \"train_f2\": train_f2,\n",
    "                        \"val_mae\": val_mae,\n",
    "                        \"val_f2\": val_f2,\n",
    "                        \"threshold\": val_thresh,\n",
    "                    }, step=epoch)\n",
    "\n",
    "                    scheduler.step(val_mae)\n",
    "\n",
    "                    # Early stopping\n",
    "                    if val_f2 > best_val_f2:\n",
    "                        best_val_f2 = val_f2\n",
    "                        best_threshold = val_thresh\n",
    "                        patience_counter = 0\n",
    "                        torch.save(model.state_dict(), f\"/tmp/best_model_fold_{fold_id}.pth\")\n",
    "                        mlflow.log_artifact(f\"/tmp/best_model_fold_{fold_id}.pth\")\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= params[\"patience\"]:\n",
    "                            print(f\"Early stopping at epoch {epoch}\")\n",
    "                            break\n",
    "\n",
    "            # store fold results\n",
    "            cv_results.append({\n",
    "                \"fold\": fold_id,\n",
    "                \"val_f2\": best_val_f2,\n",
    "                \"val_mae\": val_mae,\n",
    "                \"best_threshold\": best_threshold\n",
    "            })\n",
    "            cv_models.append(f\"/tmp/best_model_fold_{fold_id}.pth\")\n",
    "\n",
    "        # Log summary to parent run\n",
    "        df = pd.DataFrame(cv_results)\n",
    "        mlflow.log_table(df, \"cv_results.json\")\n",
    "\n",
    "        print(\"\\nCV COMPLETE:\")\n",
    "        print(df)\n",
    "\n",
    "    return cv_results, cv_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113edef5-63d2-49a7-979f-027cf684f374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_results, cv_models = train_cv_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23373f57-2d55-49d3-9a5d-28c25599ae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6978085643709185,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CV-1Y_MK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
