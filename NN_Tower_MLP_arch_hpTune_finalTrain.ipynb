{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881713f0-81ec-4c67-b173-2fbd107c5746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Neural Network Stack: Flight Delay Prediction (ResFiLM-MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f6cb6e-4e82-4c13-b282-c88b9a4aed83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Section | Description |\n",
    "| :--- | :--- |\n",
    "| **Model** | Multi-Towered Residual FiLM-MLP (Deep Learning) |\n",
    "| **Objective** | Model architecture code, multi-fold hyperparameter tuning, final training, and prediction generation for 5-Year dataset. |\n",
    "| **Target Metric** | Maximizing **F2 Score** (Primary), Minimizing **MAE** (Secondary). |\n",
    "| **Architecture TLDR** | Dual-Head **Multi-Task** Learning. Combines a **Categorical Tower** (Embeddings modulated by FiLM) and a **Numerical Tower** (Residual Blocks) to fuse features for Regression (MAE) and Classification (F2) heads. |\n",
    "| **Key Optimization** | Class imbalance handled with 4x positive weighting in the Classification Loss (BCE). |\n",
    "| **Output** | Parquet file of **Out-of-Sample Predictions** for the entire 5-year period. |\n",
    "| **Best Params Used** | `{'lr': 0.0001556, 'batch_size': 4096, 'alpha': 0.342, 'time_dim': 16, 'emb_drop': 0.046, 'num_drop': 0.324, 'final_drop': 0.100}` |"
]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "570129d5-9f85-4f29-998b-61c8e8dc5795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c0349d4-bd94-494d-95b9-d194fa55d669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /databricks/python3/lib/python3.12/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /databricks/python3/lib/python3.12/site-packages (from optuna) (1.14.1)\nRequirement already satisfied: colorlog in /databricks/python3/lib/python3.12/site-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from optuna) (24.1)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /databricks/python3/lib/python3.12/site-packages (from optuna) (2.0.30)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from optuna) (4.66.4)\nRequirement already satisfied: PyYAML in /databricks/python3/lib/python3.12/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.2.0)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.12/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /databricks/python3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nUsing device: cpu\nNumerical columns count: 34\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Imports & Global Configuration\n",
    "# ---------------------------------------------------------\n",
    "%pip install optuna\n",
    "\n",
    "import pyspark.sql.functions as sf \n",
    "import torch.nn.functional as F    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "import copy\n",
    "import mlflow\n",
    "import optuna\n",
    "\n",
    "# Enable Arrow for faster, lower-memory conversion from Spark to Pandas\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "def get_device(): \n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Feature Definitions\n",
    "categorical_cols = [\n",
    "    \"OP_UNIQUE_CARRIER\", \"ORIGIN_AIRPORT_SEQ_ID\", \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"route\", \"AIRPORT_HUB_CLASS\", \"AIRLINE_CATEGORY\"\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    \"DISTANCE\", \"CRS_ELAPSED_TIME\", \"prev_flight_delay_in_minutes\",\n",
    "    \"origin_delays_4h\", \"delay_origin_7d\", \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\", \"flight_count_24h\", \"AVG_TAXI_OUT_ORIGIN\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\", \"in_degree\", \"out_degree\",\n",
    "    \"weighted_in_degree\", \"weighted_out_degree\", \"betweenness\",\n",
    "    \"closeness\", \"N_RUNWAYS\", \"HourlyVisibility\", \"HourlyStationPressure\",\n",
    "    \"HourlyWindSpeed\", \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlyAltimeterSetting\", \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyPrecipitation\", \"HourlyCloudCoverage\", \"HourlyCloudElevation\",\n",
    "    \"ground_flights_last_hour\", \"arrivals_last_hour\",\n",
    "    \"dow_sin\", \"dow_cos\", \"doy_sin\", \"doy_cos\"\n",
    "]\n",
    "\n",
    "# Deduplicate numerical columns to prevent shape mismatches\n",
    "numerical_cols = list(dict.fromkeys(numerical_cols))\n",
    "print(f\"Numerical columns count: {len(numerical_cols)}\")\n",
    "\n",
    "time_col = \"CRS_DEP_MINUTES\"\n",
    "target_col = \"DEP_DELAY_NEW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb36c895-bd0b-4e2f-8275-4d2a9455ffc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02439ac2-c369-4e67-91d0-44120e882627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-tower-tuned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3d4c07-9704-4804-bcf1-9534470edcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Data Prep\n",
    "# ---------------------------------------------------------\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "def prepare_fold_data(fold_df, categorical_cols, numerical_cols, time_col, target_col):\n",
    "    \"\"\"\n",
    "    Prepares data for a specific fold with OOM protection and Safe Indexing.\n",
    "    \"\"\"\n",
    "    # Split Data\n",
    "    train_fe = fold_df.filter(sf.col(\"split_type\") == \"train\")\n",
    "    val_fe   = fold_df.filter(sf.col(\"split_type\") == \"validation\")\n",
    "\n",
    "    # Cast Numerics to Float32\n",
    "    for c in numerical_cols + [time_col, target_col]:\n",
    "        train_fe = train_fe.withColumn(c, sf.col(c).cast(FloatType()))\n",
    "        val_fe = val_fe.withColumn(c, sf.col(c).cast(FloatType()))\n",
    "\n",
    "    # Distributed String Indexing\n",
    "    indexers = []\n",
    "    encoded_cat_cols = []\n",
    "    \n",
    "    for c in categorical_cols:\n",
    "        output_col = f\"{c}_idx\"\n",
    "        encoded_cat_cols.append(output_col)\n",
    "        # handleInvalid='keep' creates a bucket for UNK values at the end\n",
    "        indexers.append(StringIndexer(inputCol=c, outputCol=output_col, \n",
    "                                      stringOrderType=\"alphabetAsc\", handleInvalid=\"keep\"))\n",
    "    \n",
    "    # Fit Indexers on TRAIN data only\n",
    "    for ind in indexers:\n",
    "        model = ind.fit(train_fe)\n",
    "        train_fe = model.transform(train_fe)\n",
    "        val_fe = model.transform(val_fe)\n",
    "\n",
    "    # Select & Cast Indices to Integer\n",
    "    final_cols = encoded_cat_cols + numerical_cols + [time_col, target_col]\n",
    "    \n",
    "    for c in encoded_cat_cols:\n",
    "        train_fe = train_fe.withColumn(c, sf.col(c).cast(IntegerType()))\n",
    "        val_fe = val_fe.withColumn(c, sf.col(c).cast(IntegerType()))\n",
    "\n",
    "    # Collect to Pandas\n",
    "    train_pd = train_fe.select(final_cols).toPandas()\n",
    "    val_pd   = val_fe.select(final_cols).toPandas()\n",
    "\n",
    "    # Rename columns\n",
    "    rename_map = {f\"{c}_idx\": c for c in categorical_cols}\n",
    "    train_pd = train_pd.rename(columns=rename_map)\n",
    "    val_pd = val_pd.rename(columns=rename_map)\n",
    "\n",
    "    # Fit Scaler (Train Only)\n",
    "    scaler = StandardScaler()\n",
    "    train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "    val_pd[numerical_cols]   = scaler.transform(val_pd[numerical_cols])\n",
    "\n",
    "    # Calculate Embedding Dimensions \n",
    "    cat_dims = [int(train_pd[c].max() + 2) for c in categorical_cols]\n",
    "    \n",
    "    emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
    "\n",
    "    return train_pd, val_pd, cat_dims, emb_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f45559dc-83e6-4e66-87a4-1ab182e9bb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db3a218-576e-4bb9-baf7-1536c2b60d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Model Definitions\n",
    "# ---------------------------------------------------------\n",
    "class FlightDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Data is already cast to correct types in prepare_fold_data\n",
    "        self.cat = torch.tensor(df[categorical_cols].values, dtype=torch.long)\n",
    "        self.num = torch.tensor(df[numerical_cols].values, dtype=torch.float32)\n",
    "        self.time = torch.tensor(df[time_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.cat[idx], self.num[idx], self.time[idx], self.y[idx]\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        h = F.gelu(self.fc1(self.ln(x)))\n",
    "        return x + self.fc2(self.dropout(h))\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.wb = nn.Linear(1, 1)\n",
    "        self.ws = nn.Linear(1, k)\n",
    "    def forward(self, t):\n",
    "        return torch.cat([self.wb(t), torch.sin(self.ws(t))], dim=-1)\n",
    "\n",
    "class ResFiLMMLP(nn.Module):\n",
    "    def __init__(self, cat_dims, emb_dims, num_numerical, time_dim=8, \n",
    "                 emb_dropout=0.1, num_dropout=0.1, film_dropout=0.1, final_dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding Tower\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(d, e) for d, e in zip(cat_dims, emb_dims)])\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        emb_total = sum(emb_dims)\n",
    "        \n",
    "        # Numeric Tower\n",
    "        self.fc_num = nn.Linear(num_numerical, 256)\n",
    "        self.res_blocks = nn.ModuleList([ResBlock(256, num_dropout) for _ in range(4)])\n",
    "        \n",
    "        # FiLM & Time\n",
    "        self.film = nn.Linear(256, 2 * emb_total)\n",
    "        self.film_dropout = nn.Dropout(film_dropout)\n",
    "        self.t2v = Time2Vec(time_dim)\n",
    "        \n",
    "        # Heads\n",
    "        fused_dim = 256 + emb_total + (time_dim + 1) + 1\n",
    "        self.reg_head = nn.Sequential(nn.Linear(fused_dim, 256), nn.GELU(), nn.Dropout(final_dropout),\n",
    "                                      nn.Linear(256, 128), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(128, 1))\n",
    "        self.clf_head = nn.Sequential(nn.Linear(fused_dim, 256), nn.GELU(), nn.Dropout(final_dropout),\n",
    "                                      nn.Linear(256, 128), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(128, 1))\n",
    "\n",
    "    def forward(self, x_cat, x_num, x_time):\n",
    "        emb = self.emb_dropout(torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)], dim=-1))\n",
    "        \n",
    "        h = F.gelu(self.fc_num(x_num))\n",
    "        for block in self.res_blocks: h = block(h)\n",
    "        \n",
    "        gamma, beta = torch.chunk(self.film(h), 2, dim=-1)\n",
    "        emb_mod = self.film_dropout(gamma) * emb + self.film_dropout(beta)\n",
    "        \n",
    "        z = torch.cat([emb_mod, h, self.t2v(x_time), x_time], dim=-1)\n",
    "        return self.reg_head(z), self.clf_head(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ff3349a-ef94-47cb-86ee-a0370459633a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f0a373-4404-4054-8a7e-713162f8ea02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, crit_reg, crit_clf, alpha):\n",
    "    model.train()\n",
    "    for cat, num, time, y in loader:\n",
    "        cat, num, time, y = cat.to(DEVICE), num.to(DEVICE), time.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        reg, clf = model(cat, num, time)\n",
    "        loss = alpha * crit_reg(reg, y) + (1-alpha) * crit_clf(clf, (y >= 15.0).float())\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39cb2a7c-f735-4753-8de0-687be7721db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, loader):\n",
    "    model.eval()\n",
    "    preds_reg, preds_clf = [], []\n",
    "    targets_reg, targets_clf = [], []\n",
    "    with torch.no_grad():\n",
    "        for cat, num, time, y in loader:\n",
    "            reg, clf = model(cat.to(DEVICE), num.to(DEVICE), time.to(DEVICE))\n",
    "            preds_reg.append(reg.cpu()); targets_reg.append(y.cpu())\n",
    "            preds_clf.append(torch.sigmoid(clf).cpu()); targets_clf.append((y >= 15.0).float().cpu())\n",
    "    \n",
    "    y_pred_reg, y_true_reg = torch.cat(preds_reg).numpy(), torch.cat(targets_reg).numpy()\n",
    "    y_pred_clf, y_true_clf = torch.cat(preds_clf).numpy(), torch.cat(targets_clf).numpy()\n",
    "    \n",
    "    mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "    f2 = fbeta_score(y_true_clf, (y_pred_clf > 0.5).astype(int), beta=2)\n",
    "    return f2, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ede7c887-2cc7-449e-aac6-6d7849e9fa14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [1024, 2048, 4096]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.3, 0.7),\n",
    "        \"time_dim\": trial.suggest_categorical(\"time_dim\", [4, 8, 16]),\n",
    "        \"emb_drop\": trial.suggest_float(\"emb_drop\", 0.0, 0.4),\n",
    "        \"num_drop\": trial.suggest_float(\"num_drop\", 0.0, 0.4),\n",
    "        \"final_drop\": trial.suggest_float(\"final_drop\", 0.0, 0.4)\n",
    "    }\n",
    "    \n",
    "    tuning_folds = [folds[0], folds[len(folds)//2], folds[-1]]\n",
    "    val_f2_list, val_mae_list = [], []\n",
    "    train_f2_list, train_mae_list = [], [] \n",
    "    \n",
    "    print(f\"\\n[Trial {trial.number}] START | Batch={params['batch_size']}, LR={params['lr']:.5f}\")\n",
    "    sys.stdout.flush() \n",
    "\n",
    "    for i, fold in enumerate(tuning_folds):\n",
    "        print(f\"  > Fold {fold} ({i+1}/3)...\", end=\" \")\n",
    "        sys.stdout.flush()\n",
    "        start_t = time.time()\n",
    "        \n",
    "        train_pd, val_pd, cat_dims, emb_dims = prepare_fold_data(\n",
    "            cv_full_df.filter(sf.col(\"fold_id\") == fold), \n",
    "            categorical_cols, numerical_cols, time_col, target_col\n",
    "        )\n",
    "        \n",
    "        # num_workers=0 to prevent multiprocessing crash\n",
    "        train_dl = DataLoader(FlightDataset(train_pd), batch_size=params[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
    "        val_dl = DataLoader(FlightDataset(val_pd), batch_size=params[\"batch_size\"], num_workers=0, pin_memory=True)\n",
    "        \n",
    "        model = ResFiLMMLP(cat_dims, emb_dims, len(numerical_cols), time_dim=params[\"time_dim\"],\n",
    "                           emb_dropout=params[\"emb_drop\"], num_dropout=params[\"num_drop\"], \n",
    "                           final_dropout=params[\"final_drop\"]).to(DEVICE)\n",
    "        opt = optim.AdamW(model.parameters(), lr=params[\"lr\"])\n",
    "        crit_reg = nn.L1Loss()\n",
    "        crit_clf = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]).to(DEVICE))\n",
    "        \n",
    "        for _ in range(3):\n",
    "            train_one_epoch(model, train_dl, opt, crit_reg, crit_clf, params[\"alpha\"])\n",
    "        \n",
    "        v_f2, v_mae = evaluate_metrics(model, val_dl)\n",
    "        val_f2_list.append(v_f2); val_mae_list.append(v_mae)\n",
    "        \n",
    "        t_f2, t_mae = evaluate_metrics(model, train_dl)\n",
    "        train_f2_list.append(t_f2); train_mae_list.append(t_mae)\n",
    "        \n",
    "        elapsed = (time.time() - start_t) / 60\n",
    "        print(f\"Done ({elapsed:.1f}m). Val F2={v_f2:.3f} | Train F2={t_f2:.3f}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        trial.report(np.mean(val_f2_list), i)\n",
    "        if trial.should_prune(): \n",
    "            print(f\"  > Pruned at Fold {fold}.\")\n",
    "            mlflow.set_tag(\"status\", \"pruned\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    with mlflow.start_run(nested=True, run_name=f\"Trial_{trial.number}\"):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"val_f2\", np.mean(val_f2_list))\n",
    "        mlflow.log_metric(\"val_mae\", np.mean(val_mae_list))\n",
    "        mlflow.log_metric(\"train_f2\", np.mean(train_f2_list)) \n",
    "        mlflow.log_metric(\"train_mae\", np.mean(train_mae_list)) \n",
    "    \n",
    "    return np.mean(val_f2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e42ba22-bf8b-43bc-b4d6-ff5e9f4e1838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca61e2a-fe02-4073-aa31-5d65eac3c4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Optuna Tuning (8 Trials, Stable Mode) ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 07:39:08,784] A new study created in memory with name: no-name-b9410282-c4aa-4b84-8474-d6581c428c56\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 0] START | Batch=2048, LR=0.00096\n  > Fold 1 (1/3)... Done (23.5m). Val F2=0.569 | Train F2=0.590\n  > Fold 6 (2/3)... Done (22.7m). Val F2=0.536 | Train F2=0.575\n  > Fold 10 (3/3)... Done (24.9m). Val F2=0.572 | Train F2=0.582\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 08:50:18,478] Trial 0 finished with value: 0.5588664642492351 and parameters: {'lr': 0.0009575058150175052, 'batch_size': 2048, 'alpha': 0.40065021283464114, 'time_dim': 4, 'emb_drop': 0.16404933909407846, 'num_drop': 0.16736632485412783, 'final_drop': 0.21101424331333254}. Best is trial 0 with value: 0.5588664642492351.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 1] START | Batch=1024, LR=0.00038\n  > Fold 1 (1/3)... Done (22.6m). Val F2=0.560 | Train F2=0.584\n  > Fold 6 (2/3)... Done (24.1m). Val F2=0.555 | Train F2=0.598\n  > Fold 10 (3/3)... Done (25.3m). Val F2=0.572 | Train F2=0.579\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 10:02:19,849] Trial 1 finished with value: 0.5625465733551208 and parameters: {'lr': 0.00038191325052377973, 'batch_size': 1024, 'alpha': 0.5817744495942692, 'time_dim': 4, 'emb_drop': 0.2973993187515431, 'num_drop': 0.01820144637246144, 'final_drop': 0.05244952735329807}. Best is trial 1 with value: 0.5625465733551208.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 2] START | Batch=4096, LR=0.00061\n  > Fold 1 (1/3)... Done (23.8m). Val F2=0.577 | Train F2=0.597\n  > Fold 6 (2/3)... Done (22.7m). Val F2=0.539 | Train F2=0.576\n  > Fold 10 (3/3)... Done (26.1m). Val F2=0.576 | Train F2=0.581\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 11:14:55,948] Trial 2 finished with value: 0.5638415356441168 and parameters: {'lr': 0.0006079442355415744, 'batch_size': 4096, 'alpha': 0.6171095826693376, 'time_dim': 16, 'emb_drop': 0.31252774208004025, 'num_drop': 0.08086622245692543, 'final_drop': 0.3506675733290697}. Best is trial 2 with value: 0.5638415356441168.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 3] START | Batch=4096, LR=0.00016\n  > Fold 1 (1/3)... Done (24.7m). Val F2=0.565 | Train F2=0.582\n  > Fold 6 (2/3)... Done (24.5m). Val F2=0.544 | Train F2=0.584\n  > Fold 10 (3/3)... Done (26.5m). Val F2=0.597 | Train F2=0.600\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 12:30:36,594] Trial 3 finished with value: 0.5687250006294451 and parameters: {'lr': 0.00015558999196709387, 'batch_size': 4096, 'alpha': 0.34214834331587723, 'time_dim': 16, 'emb_drop': 0.046257475013397054, 'num_drop': 0.32431529675298293, 'final_drop': 0.10010488301186182}. Best is trial 3 with value: 0.5687250006294451.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 4] START | Batch=4096, LR=0.00013\n  > Fold 1 (1/3)... Done (24.9m). Val F2=0.565 | Train F2=0.586\n  > Fold 6 (2/3)... Done (24.6m). Val F2=0.548 | Train F2=0.587\n  > Fold 10 (3/3)... Done (26.9m). Val F2=0.577 | Train F2=0.583\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 13:47:01,732] Trial 4 finished with value: 0.5634774039139566 and parameters: {'lr': 0.00012577372010664665, 'batch_size': 4096, 'alpha': 0.33059836514431606, 'time_dim': 16, 'emb_drop': 0.3265276151501351, 'num_drop': 0.37956164780145407, 'final_drop': 0.2627991690022928}. Best is trial 3 with value: 0.5687250006294451.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 5] START | Batch=4096, LR=0.00186\n  > Fold 1 (1/3)... Done (24.7m). Val F2=0.561 | Train F2=0.580\n  > Pruned at Fold 1.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 14:11:45,191] Trial 5 pruned. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 6] START | Batch=2048, LR=0.00042\n  > Fold 1 (1/3)... Done (25.0m). Val F2=0.577 | Train F2=0.596\n  > Fold 6 (2/3)... Done (28.7m). Val F2=0.543 | Train F2=0.584\n  > Fold 10 (3/3)... Done (28.5m). Val F2=0.571 | Train F2=0.580\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 15:33:51,876] Trial 6 finished with value: 0.5639208265448387 and parameters: {'lr': 0.0004174074924664462, 'batch_size': 2048, 'alpha': 0.6374870217158395, 'time_dim': 4, 'emb_drop': 0.17756553684120396, 'num_drop': 0.33457012601060093, 'final_drop': 0.091024878754628}. Best is trial 3 with value: 0.5687250006294451.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Trial 7] START | Batch=2048, LR=0.00130\n  > Fold 1 (1/3)... Done (25.2m). Val F2=0.555 | Train F2=0.577\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 15:59:01,277] Trial 7 pruned. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Pruned at Fold 1.\n\n>>> Best Params: {'lr': 0.00015558999196709387, 'batch_size': 4096, 'alpha': 0.34214834331587723, 'time_dim': 16, 'emb_drop': 0.046257475013397054, 'num_drop': 0.32431529675298293, 'final_drop': 0.10010488301186182}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Hyperparameter Tuning (Single Worker)\n",
    "# ---------------------------------------------------------\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import sys\n",
    "import time\n",
    "\n",
    "CV_DATA_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/cv_splits\"\n",
    "cv_full_df = spark.read.parquet(CV_DATA_PATH)\n",
    "folds = sorted([row['fold_id'] for row in cv_full_df.select(\"fold_id\").distinct().collect()])\n",
    "\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-tower-tuned\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"--- Starting Optuna Tuning (8 Trials, Stable Mode) ---\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Hyperparameter_Tuning_Session\"):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=8)\n",
    "\n",
    "BEST_PARAMS = study.best_params\n",
    "print(f\"\\n>>> Best Params: {BEST_PARAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0574d66-c1bd-48a5-a2fb-190aee68361f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training on full dataset + early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafeb849-3055-4a61-bc17-fdb8681caf99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- !!! FULL PRODUCTION RUN !!! ---\n  > Indexing...\n  > Train: 18066230 | Val: 4384854\n\n--- Starting Full Training (10 Epochs, Patience=4) ---\n    Ep 0 Batch 500/4411...\r    Ep 0 Batch 1000/4411...\r    Ep 0 Batch 1500/4411...\r    Ep 0 Batch 2000/4411...\r    Ep 0 Batch 2500/4411...\r    Ep 0 Batch 3000/4411...\r    Ep 0 Batch 3500/4411...\r    Ep 0 Batch 4000/4411...\r    Ep 0: Val F2=0.5935 MAE=10.97 | Train F2=0.5641 MAE=9.94\n    Ep 1 Batch 500/4411...\r    Ep 1 Batch 1000/4411...\r    Ep 1 Batch 1500/4411...\r    Ep 1 Batch 2000/4411...\r    Ep 1 Batch 2500/4411...\r    Ep 1 Batch 3000/4411...\r    Ep 1 Batch 3500/4411...\r    Ep 1 Batch 4000/4411...\r    Ep 1: Val F2=0.5876 MAE=10.90 | Train F2=0.5803 MAE=9.72\n       >> No improve. Patience 1/4\n    Ep 2 Batch 500/4411...\r    Ep 2 Batch 1000/4411...\r    Ep 2 Batch 1500/4411...\r    Ep 2 Batch 2000/4411...\r    Ep 2 Batch 2500/4411...\r    Ep 2 Batch 3000/4411...\r    Ep 2 Batch 3500/4411...\r    Ep 2 Batch 4000/4411...\r    Ep 2: Val F2=0.5987 MAE=10.90 | Train F2=0.5834 MAE=9.68\n    Ep 3 Batch 500/4411...\r    Ep 3 Batch 1000/4411...\r    Ep 3 Batch 1500/4411...\r    Ep 3 Batch 2000/4411...\r    Ep 3 Batch 2500/4411...\r    Ep 3 Batch 3000/4411...\r    Ep 3 Batch 3500/4411...\r    Ep 3 Batch 4000/4411...\r    Ep 3: Val F2=0.5945 MAE=10.86 | Train F2=0.5858 MAE=9.65\n       >> No improve. Patience 1/4\n    Ep 4 Batch 500/4411...\r    Ep 4 Batch 1000/4411...\r    Ep 4 Batch 1500/4411...\r    Ep 4 Batch 2000/4411...\r    Ep 4 Batch 2500/4411...\r    Ep 4 Batch 3000/4411...\r    Ep 4 Batch 3500/4411...\r    Ep 4 Batch 4000/4411...\r    Ep 4: Val F2=0.5998 MAE=10.84 | Train F2=0.5875 MAE=9.63\n    Ep 5 Batch 500/4411...\r    Ep 5 Batch 1000/4411...\r    Ep 5 Batch 1500/4411...\r    Ep 5 Batch 2000/4411...\r    Ep 5 Batch 2500/4411...\r    Ep 5 Batch 3000/4411...\r    Ep 5 Batch 3500/4411...\r    Ep 5 Batch 4000/4411...\r    Ep 5: Val F2=0.5949 MAE=10.84 | Train F2=0.5889 MAE=9.61\n       >> No improve. Patience 1/4\n    Ep 6 Batch 500/4411...\r    Ep 6 Batch 1000/4411...\r    Ep 6 Batch 1500/4411...\r    Ep 6 Batch 2000/4411...\r    Ep 6 Batch 2500/4411...\r    Ep 6 Batch 3000/4411...\r    Ep 6 Batch 3500/4411...\r    Ep 6 Batch 4000/4411...\r    Ep 6: Val F2=0.6005 MAE=10.84 | Train F2=0.5900 MAE=9.59\n    Ep 7 Batch 500/4411...\r    Ep 7 Batch 1000/4411...\r    Ep 7 Batch 1500/4411...\r    Ep 7 Batch 2000/4411...\r    Ep 7 Batch 2500/4411...\r    Ep 7 Batch 3000/4411...\r    Ep 7 Batch 3500/4411...\r    Ep 7 Batch 4000/4411...\r    Ep 7: Val F2=0.5968 MAE=10.83 | Train F2=0.5909 MAE=9.58\n       >> No improve. Patience 1/4\n    Ep 8 Batch 500/4411...\r    Ep 8 Batch 1000/4411...\r    Ep 8 Batch 1500/4411...\r    Ep 8 Batch 2000/4411...\r    Ep 8 Batch 2500/4411...\r    Ep 8 Batch 3000/4411...\r    Ep 8 Batch 3500/4411...\r    Ep 8 Batch 4000/4411...\r    Ep 8: Val F2=0.5943 MAE=10.81 | Train F2=0.5917 MAE=9.57\n       >> No improve. Patience 2/4\n    Ep 9 Batch 500/4411...\r    Ep 9 Batch 1000/4411...\r    Ep 9 Batch 1500/4411...\r    Ep 9 Batch 2000/4411...\r    Ep 9 Batch 2500/4411...\r    Ep 9 Batch 3000/4411...\r    Ep 9 Batch 3500/4411...\r    Ep 9 Batch 4000/4411...\r    Ep 9: Val F2=0.5907 MAE=10.79 | Train F2=0.5925 MAE=9.56\n       >> No improve. Patience 3/4\n\n--- Finalizing Best Model ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306bfde1794d426dbdd1035240286481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Generating predictions for final_val_preds...\n    >> Saving to DBFS: dbfs:/student-groups/Group_2_2/5_year_custom_joined/nn_predictions_final/final_val_preds_4e65b009\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div style=\"background-color:#e6f7ff;padding:10px;border:1px solid #91d5ff;\">\n",
       "    <b>final_val_preds Predictions:</b> <a href=\"/files/shared_uploads/predictions/final_val_preds_4e65b009.csv\" target=\"_blank\">Download CSV</a></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS. Run complete.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Final Training Run (FULL DATASET + EARLY STOPPING)\n",
    "# ---------------------------------------------------------\n",
    "import torch\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "import uuid\n",
    "import pyspark.sql.functions as sf \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        h = F.gelu(self.fc1(self.ln(x)))\n",
    "        return x + self.fc2(self.dropout(h))\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.wb = nn.Linear(1, 1)\n",
    "        self.ws = nn.Linear(1, k)\n",
    "    def forward(self, t):\n",
    "        return torch.cat([self.wb(t), torch.sin(self.ws(t))], dim=-1)\n",
    "\n",
    "class ResFiLMMLP(nn.Module):\n",
    "    def __init__(self, cat_dims, emb_dims, num_numerical, time_dim=8, \n",
    "                 emb_dropout=0.1, num_dropout=0.1, film_dropout=0.1, final_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(d, e) for d, e in zip(cat_dims, emb_dims)])\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        emb_total = sum(emb_dims)\n",
    "        self.fc_num = nn.Linear(num_numerical, 256)\n",
    "        self.res_blocks = nn.ModuleList([ResBlock(256, num_dropout) for _ in range(4)])\n",
    "        self.film = nn.Linear(256, 2 * emb_total)\n",
    "        self.film_dropout = nn.Dropout(film_dropout)\n",
    "        self.t2v = Time2Vec(time_dim)\n",
    "        fused_dim = 256 + emb_total + (time_dim + 1) + 1\n",
    "        self.reg_head = nn.Sequential(nn.Linear(fused_dim, 256), nn.GELU(), nn.Dropout(final_dropout),\n",
    "                                      nn.Linear(256, 128), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(128, 1))\n",
    "        self.clf_head = nn.Sequential(nn.Linear(fused_dim, 256), nn.GELU(), nn.Dropout(final_dropout),\n",
    "                                      nn.Linear(256, 128), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(128, 1))\n",
    "\n",
    "    def forward(self, x_cat, x_num, x_time):\n",
    "        emb = self.emb_dropout(torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)], dim=-1))\n",
    "        h = F.gelu(self.fc_num(x_num))\n",
    "        for block in self.res_blocks: h = block(h)\n",
    "        gamma, beta = torch.chunk(self.film(h), 2, dim=-1)\n",
    "        emb_mod = self.film_dropout(gamma) * emb + self.film_dropout(beta)\n",
    "        z = torch.cat([emb_mod, h, self.t2v(x_time), x_time], dim=-1)\n",
    "        return self.reg_head(z), self.clf_head(z)\n",
    "\n",
    "# DATASET & HELPERS\n",
    "class ProductionFlightDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, time_col, target_col, id_col=\"flight_uid\"):\n",
    "        self.cat = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "        self.num = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.time = torch.tensor(df[time_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.ids = df[id_col].values \n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.cat[idx], self.num[idx], self.time[idx], self.y[idx], self.ids[idx]\n",
    "\n",
    "def evaluate_metrics(model, loader):\n",
    "    model.eval()\n",
    "    all_y, all_reg, all_clf = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for cat, num, time_t, y, _ in loader:\n",
    "            reg, clf = model(cat, num, time_t)\n",
    "            all_y.append(y); all_reg.append(reg); all_clf.append(clf)\n",
    "    \n",
    "    y_true = torch.cat(all_y).cpu().numpy().flatten()\n",
    "    y_reg = torch.cat(all_reg).cpu().numpy().flatten()\n",
    "    y_prob = torch.sigmoid(torch.cat(all_clf)).cpu().numpy().flatten()\n",
    "    \n",
    "    mae = np.mean(np.abs(y_true - y_reg))\n",
    "    f2 = fbeta_score((y_true >= 15.0).astype(int), (y_prob > 0.5).astype(int), beta=2, zero_division=0)\n",
    "    return f2, mae\n",
    "\n",
    "def save_predictions_and_link(model, loader, fold_name, save_path_base):\n",
    "    print(f\"  > Generating predictions for {fold_name}...\")\n",
    "    model.eval()\n",
    "    res = {\"flight_uid\": [], \"target_delay\": [], \"pred_delay\": [], \"target_class\": [], \"pred_prob\": []}\n",
    "    with torch.no_grad():\n",
    "        for cat, num, time_t, y, ids in loader:\n",
    "            reg, clf = model(cat, num, time_t)\n",
    "            res[\"flight_uid\"].append(ids)\n",
    "            res[\"target_delay\"].append(y.numpy().flatten())\n",
    "            res[\"pred_delay\"].append(reg.numpy().flatten())\n",
    "            res[\"target_class\"].append((y >= 15.0).float().numpy().flatten())\n",
    "            res[\"pred_prob\"].append(torch.sigmoid(clf).numpy().flatten())\n",
    "    \n",
    "    flat_ids = np.concatenate(res[\"flight_uid\"]) if len(res[\"flight_uid\"]) > 0 else []\n",
    "    pdf = pd.DataFrame({\n",
    "        \"flight_uid\": flat_ids, \"target_delay\": np.concatenate(res[\"target_delay\"]),\n",
    "        \"pred_delay\": np.concatenate(res[\"pred_delay\"]), \"target_class\": np.concatenate(res[\"target_class\"]),\n",
    "        \"pred_prob\": np.concatenate(res[\"pred_prob\"]), \"split_name\": fold_name\n",
    "    })\n",
    "    \n",
    "    unique_id = str(uuid.uuid4())[:8]\n",
    "    # MLflow Upload\n",
    "    temp_file = f\"/tmp/{fold_name}_{unique_id}.parquet\"\n",
    "    try:\n",
    "        pdf.to_parquet(temp_file)\n",
    "        mlflow.log_artifact(temp_file, artifact_path=\"predictions\")\n",
    "    except: pass\n",
    "    \n",
    "    # DBFS Save\n",
    "    save_path = f\"{save_path_base}/{fold_name}_{unique_id}\"\n",
    "    print(f\"    >> Saving to DBFS: {save_path}\")\n",
    "    spark.createDataFrame(pdf).write.mode(\"overwrite\").parquet(save_path)\n",
    "    \n",
    "    # CSV Link\n",
    "    csv_name = f\"{fold_name}_{unique_id}.csv\"\n",
    "    local_csv = f\"/tmp/{csv_name}\"\n",
    "    pdf.to_csv(local_csv, index=False)\n",
    "    dbutils.fs.cp(f\"file:{local_csv}\", f\"dbfs:/FileStore/shared_uploads/predictions/{csv_name}\")\n",
    "    displayHTML(f\"\"\"<div style=\"background-color:#e6f7ff;padding:10px;border:1px solid #91d5ff;\">\n",
    "    <b>{fold_name} Predictions:</b> <a href=\"/files/shared_uploads/predictions/{csv_name}\" target=\"_blank\">Download CSV</a></div>\"\"\")\n",
    "    \n",
    "    if os.path.exists(temp_file): os.remove(temp_file)\n",
    "    if os.path.exists(local_csv): os.remove(local_csv)\n",
    "\n",
    "# CONFIG & DATA \n",
    "BASE_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/\"\n",
    "TRAIN_PATH = BASE_PATH + \"train.parquet/\"\n",
    "VAL_PATH   = BASE_PATH + \"val.parquet/\"\n",
    "PREDS_SAVE_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/nn_predictions_final\"\n",
    "\n",
    "params = {'lr': 0.0001556, 'batch_size': 4096, 'alpha': 0.342, 'time_dim': 16, \n",
    "          'emb_drop': 0.046, 'num_drop': 0.324, 'final_drop': 0.100}\n",
    "NUM_EPOCHS = 10  # Increased since we have early stopping\n",
    "PATIENCE = 4\n",
    "\n",
    "print(\"--- !!! FULL PRODUCTION RUN !!! ---\")\n",
    "# Load FULL Dataset\n",
    "train_spark = spark.read.parquet(TRAIN_PATH)\n",
    "val_spark = spark.read.parquet(VAL_PATH)\n",
    "\n",
    "# Leakage Check\n",
    "if \"xgb_predicted_delay\" in train_spark.columns: train_spark = train_spark.drop(\"xgb_predicted_delay\")\n",
    "if \"xgb_predicted_delay\" in val_spark.columns: val_spark = val_spark.drop(\"xgb_predicted_delay\")\n",
    "\n",
    "# Indexing\n",
    "print(\"  > Indexing...\")\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\").fit(train_spark) for c in categorical_cols]\n",
    "\n",
    "def spark_to_pandas_safe(df, indexers):\n",
    "    for ind in indexers: df = ind.transform(df)\n",
    "    select_expr = [sf.col(f\"{c}_idx\").cast(IntegerType()).alias(c) for c in categorical_cols] + \\\n",
    "                  [sf.col(c).cast(FloatType()) for c in numerical_cols] + \\\n",
    "                  [sf.col(time_col).cast(FloatType()), sf.col(target_col).cast(FloatType()), sf.col(\"flight_uid\")]\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    return df.select(*select_expr).toPandas()\n",
    "\n",
    "train_pd = spark_to_pandas_safe(train_spark, indexers)\n",
    "val_pd = spark_to_pandas_safe(val_spark, indexers)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "val_pd[numerical_cols] = scaler.transform(val_pd[numerical_cols])\n",
    "\n",
    "# Dims\n",
    "cat_dims = [int(max(train_pd[c].max(), val_pd[c].max()) + 2) for c in categorical_cols]\n",
    "emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
    "\n",
    "train_ds = ProductionFlightDataset(train_pd, categorical_cols, numerical_cols, time_col, target_col)\n",
    "val_ds = ProductionFlightDataset(val_pd, categorical_cols, numerical_cols, time_col, target_col)\n",
    "print(f\"  > Train: {len(train_ds)} | Val: {len(val_ds)}\")\n",
    "del train_pd, val_pd, train_spark, val_spark; gc.collect()\n",
    "\n",
    "# TRAINING LOOP\n",
    "torch.set_num_threads(60)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "train_dl = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=params[\"batch_size\"], num_workers=0)\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/team_2_2/mlflow-nn-tower-final\")\n",
    "print(f\"\\n--- Starting Full Training ({NUM_EPOCHS} Epochs, Patience={PATIENCE}) ---\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Final_Prod_Run_Full_Data\"):\n",
    "    mlflow.log_params(params)\n",
    "    model = ResFiLMMLP(cat_dims, emb_dims, len(numerical_cols), time_dim=params[\"time_dim\"],\n",
    "                       emb_dropout=params[\"emb_drop\"], num_dropout=params[\"num_drop\"], \n",
    "                       final_dropout=params[\"final_drop\"]).to(DEVICE)\n",
    "    opt = optim.AdamW(model.parameters(), lr=params[\"lr\"])\n",
    "    crit_reg = nn.L1Loss()\n",
    "    crit_clf = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]))\n",
    "\n",
    "    best_f2 = -1.0; best_state = None\n",
    "    patience_cnt = 0 # Early Stopping Counter\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        t_loss_sum, t_ae_sum, t_tp, t_fp, t_fn = 0.0, 0.0, 0, 0, 0\n",
    "        batch_cnt = 0; total_samples = 0\n",
    "        \n",
    "        for i, (cat, num, t, y, _) in enumerate(train_dl):\n",
    "            opt.zero_grad()\n",
    "            reg, clf = model(cat, num, t)\n",
    "            loss = params[\"alpha\"]*crit_reg(reg, y) + (1-params[\"alpha\"])*crit_clf(clf, (y>=15.0).float())\n",
    "            loss.backward(); opt.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_size = y.size(0)\n",
    "                t_loss_sum += loss.item(); t_ae_sum += torch.sum(torch.abs(reg - y)).item()\n",
    "                y_true = (y >= 15.0).long(); y_pred = (torch.sigmoid(clf) > 0.5).long()\n",
    "                t_tp += ((y_true == 1) & (y_pred == 1)).sum().item()\n",
    "                t_fp += ((y_true == 0) & (y_pred == 1)).sum().item()\n",
    "                t_fn += ((y_true == 1) & (y_pred == 0)).sum().item()\n",
    "                batch_cnt += 1; total_samples += batch_size\n",
    "\n",
    "            if i % 500 == 0 and i > 0: print(f\"    Ep {epoch} Batch {i}/{len(train_dl)}...\", end=\"\\r\")\n",
    "        \n",
    "        train_loss = t_loss_sum / batch_cnt\n",
    "        train_mae = t_ae_sum / total_samples\n",
    "        precision = t_tp / (t_tp + t_fp + 1e-8); recall = t_tp / (t_tp + t_fn + 1e-8); beta = 2\n",
    "        train_f2 = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + 1e-8)\n",
    "        \n",
    "        val_f2, val_mae = evaluate_metrics(model, val_dl)\n",
    "        print(f\"    Ep {epoch}: Val F2={val_f2:.4f} MAE={val_mae:.2f} | Train F2={train_f2:.4f} MAE={train_mae:.2f}\")\n",
    "        \n",
    "        mlflow.log_metrics({\"train_loss\": train_loss, \"train_mae\": train_mae, \"train_f2\": train_f2, \"val_mae\": val_mae, \"val_f2\": val_f2}, step=epoch)\n",
    "        \n",
    "        # --- EARLY STOPPING CHECK ---\n",
    "        if val_f2 > best_f2:\n",
    "            best_f2 = val_f2\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_cnt = 0 # Reset counter\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            print(f\"       >> No improve. Patience {patience_cnt}/{PATIENCE}\")\n",
    "            if patience_cnt >= PATIENCE:\n",
    "                print(\"       >> Early stopping triggered.\")\n",
    "                break\n",
    "            \n",
    "    print(\"\\n--- Finalizing Best Model ---\")\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        c, n, t, _, _ = next(iter(train_dl))\n",
    "        r, cl = model(c, n, t)\n",
    "        sig = infer_signature({\"cat\": c.numpy(), \"num\": n.numpy(), \"time\": t.numpy()}, {\"pred_delay\": r.numpy(), \"pred_class\": cl.numpy()})\n",
    "    mlflow.pytorch.log_model(model, \"model_final\", signature=sig)\n",
    "    \n",
    "    save_predictions_and_link(model, val_dl, \"final_val_preds\", PREDS_SAVE_PATH)\n",
    "    print(\"SUCCESS. Run complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fdae9b4-139b-46d7-abc3-097dc3371e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimal Classification Threshold Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8417a980-24b6-4044-ae8f-a776233a3b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Optimal Threshold Optimization ---\n  > Generating probabilities for Validation Set...\n  > Probabilities generated in 107.8 seconds.\n  > Sweeping thresholds and calculating F2...\n\nOptimization Complete:\n  > Optimal F2 Score: 0.62595\n  > Optimal Threshold: 0.360\n  > Detailed curve logged to MLflow run.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Optimal Classification Threshold Finder\n",
    "# ---------------------------------------------------------\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import fbeta_score\n",
    "import time\n",
    "\n",
    "print(\"--- Starting Optimal Threshold Optimization ---\")\n",
    "\n",
    "# --- 1. Generate Full Validation Probabilities and True Labels ---\n",
    "def get_validation_probabilities(model, loader):\n",
    "    \"\"\"Generates a single array of true labels and probabilities for the entire validation set.\"\"\"\n",
    "    model.eval()\n",
    "    all_y_true = []\n",
    "    all_y_prob = []\n",
    "    \n",
    "    # Disable gradient tracking as this is just inference\n",
    "    with torch.no_grad():\n",
    "        for cat, num, time_t, y, _ in loader:\n",
    "            # Note: We are only interested in the classification head (clf)\n",
    "            _, clf = model(cat, num, time_t)\n",
    "            \n",
    "            # Convert delay minutes (y) to binary class (0 or 1)\n",
    "            y_true_class = (y >= 15.0).long().cpu().numpy().flatten()\n",
    "            \n",
    "            # Convert logits (clf) to probabilities\n",
    "            y_prob = torch.sigmoid(clf).cpu().numpy().flatten()\n",
    "            \n",
    "            all_y_true.append(y_true_class)\n",
    "            all_y_prob.append(y_prob)\n",
    "            \n",
    "    return np.concatenate(all_y_true), np.concatenate(all_y_prob)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"  > Generating probabilities for Validation Set...\")\n",
    "y_true_val, y_prob_val = get_validation_probabilities(model, val_dl)\n",
    "print(f\"  > Probabilities generated in {time.time() - start_time:.1f} seconds.\")\n",
    "\n",
    "# --- 2. Define Threshold Sweep Range ---\n",
    "# Sweep thresholds from 0.05 to 0.95 with small steps\n",
    "threshold_range = np.arange(0.05, 0.95, 0.01)\n",
    "\n",
    "best_f2_score = -1.0\n",
    "optimal_threshold = 0.5\n",
    "f2_scores = []\n",
    "\n",
    "# --- 3. Sweep Thresholds and Calculate F2 Score ---\n",
    "print(\"  > Sweeping thresholds and calculating F2...\")\n",
    "for threshold in threshold_range:\n",
    "    # Classify based on the current threshold\n",
    "    y_pred_class = (y_prob_val >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F2 score (beta=2 is what we optimize for)\n",
    "    current_f2 = fbeta_score(y_true_val, y_pred_class, beta=2, zero_division=0)\n",
    "    f2_scores.append(current_f2)\n",
    "    \n",
    "    if current_f2 > best_f2_score:\n",
    "        best_f2_score = current_f2\n",
    "        optimal_threshold = threshold\n",
    "\n",
    "# --- 4. Log Results to MLflow ---\n",
    "mlflow.set_experiment(\"/Shared/team_2_2/mlflow-nn-tower-final\")\n",
    "with mlflow.start_run(run_name=\"Threshold_Optimization_F2_Score\", nested=True):\n",
    "    \n",
    "    mlflow.log_param(\"optimization_metric\", \"F2 Score (beta=2)\")\n",
    "    mlflow.log_param(\"sweep_range_start\", threshold_range[0])\n",
    "    mlflow.log_param(\"sweep_range_end\", threshold_range[-1])\n",
    "    \n",
    "    mlflow.log_metric(\"best_f2_score_found\", best_f2_score)\n",
    "    mlflow.log_metric(\"optimal_threshold_f2\", optimal_threshold)\n",
    "    \n",
    "    # Log the full curve for visualization\n",
    "    threshold_log_data = pd.DataFrame({\n",
    "        \"threshold\": threshold_range,\n",
    "        \"f2_score\": f2_scores\n",
    "    })\n",
    "    \n",
    "    # Create a local file to log as an artifact\n",
    "    temp_csv_path = f\"/tmp/threshold_curve_{str(uuid.uuid4())[:8]}.csv\"\n",
    "    threshold_log_data.to_csv(temp_csv_path, index=False)\n",
    "    mlflow.log_artifact(temp_csv_path, \"threshold_analysis\")\n",
    "    os.remove(temp_csv_path)\n",
    "\n",
    "    print(f\"\\nOptimization Complete:\")\n",
    "    print(f\"  > Optimal F2 Score: {best_f2_score:.5f}\")\n",
    "    print(f\"  > Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"  > Detailed curve logged to MLflow run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35f1b22c-5ce5-4c9a-be10-6511494fc211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Inferences using trained model + optimized threshold (Restart-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b57e8990-5f1e-47d9-a7b9-2f50fe54251b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "import uuid\n",
    "import pyspark.sql.functions as sf \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a06b81-101b-4c6d-a42c-6ccd4f1dcd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Recovering Environment and Data Scalers ---\n  > Re-fitting String Indexers on Train Set...\n  > Re-fitting StandardScaler on Train Set...\n  > Loading Model & Determining embedding dimensions...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af686314b8914761bf6f2eef56394b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- 2. Calculating and Logging TRAIN & VAL Set Metrics ---\n\n=======================================================\n| FINAL REPORTED METRICS (Threshold: 0.360) |\n=======================================================\n| TRAIN SET (Internal Fit) | F2: 0.6213 | MAE: 9.5335 |\n| VALIDATION SET (Tuning)  | F2: 0.6260 | MAE: 10.8419 |\n=======================================================\n\n--- 3. Saving VALIDATION Predictions ---\n  > Generating predictions for FINAL_VAL_OPTIMIZED...\n  > Saved to DBFS: dbfs:/student-groups/Group_2_2/5_year_custom_joined/nn_predictions_final/FINAL_VAL_OPTIMIZED_T36_93b24868\n  > Final F2 Score: 0.62595 | Final MAE: 10.8419\n  > Inference Time: 95.92 seconds\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div style=\"background-color:#ccffee;padding:10px;border:1px solid #91d5ff;\"><b>FINAL_VAL_OPTIMIZED PREDICTIONS (T=0.360):</b> <a href=\"/files/shared_uploads/predictions/FINAL_VAL_OPTIMIZED_T36_93b24868.csv\" target=\"_blank\">Download CSV</a></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- 4. Running Final Optimized Inference on TEST Set ---\n  > Generating predictions for FINAL_TEST_OPTIMIZED...\n  > Saved to DBFS: dbfs:/student-groups/Group_2_2/5_year_custom_joined/nn_predictions_final/FINAL_TEST_OPTIMIZED_T36_9fea73a1\n  > Final F2 Score: 0.61870 | Final MAE: 11.3363\n  > Inference Time: 142.73 seconds\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div style=\"background-color:#ccffee;padding:10px;border:1px solid #91d5ff;\"><b>FINAL_TEST_OPTIMIZED PREDICTIONS (T=0.360):</b> <a href=\"/files/shared_uploads/predictions/FINAL_TEST_OPTIMIZED_T36_9fea73a1.csv\" target=\"_blank\">Download CSV</a></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSUCCESS: All predictions and metrics generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Rebuild Environment and Run Final Inference\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# config\n",
    "MLFLOW_RUN_ID = \"8706b956e0bd4ed681234979ad86206b\"\n",
    "OPTIMAL_THRESHOLD = 0.36 \n",
    "\n",
    "# features\n",
    "categorical_cols = [\n",
    "    \"OP_UNIQUE_CARRIER\", \"ORIGIN_AIRPORT_SEQ_ID\", \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"route\", \"AIRPORT_HUB_CLASS\", \"AIRLINE_CATEGORY\"\n",
    "]\n",
    "numerical_cols = [\n",
    "    \"DISTANCE\", \"CRS_ELAPSED_TIME\", \"prev_flight_delay_in_minutes\",\n",
    "    \"origin_delays_4h\", \"delay_origin_7d\", \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\", \"flight_count_24h\", \"AVG_TAXI_OUT_ORIGIN\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\", \"in_degree\", \"out_degree\",\n",
    "    \"weighted_in_degree\", \"weighted_out_degree\", \"betweenness\",\n",
    "    \"closeness\", \"N_RUNWAYS\", \"HourlyVisibility\", \"HourlyStationPressure\",\n",
    "    \"HourlyWindSpeed\", \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlyAltimeterSetting\", \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyPrecipitation\", \"HourlyCloudCoverage\", \"HourlyCloudElevation\",\n",
    "    \"ground_flights_last_hour\", \"arrivals_last_hour\",\n",
    "    \"dow_sin\", \"dow_cos\", \"doy_sin\", \"doy_cos\"\n",
    "]\n",
    "numerical_cols = list(dict.fromkeys(numerical_cols))\n",
    "time_col = \"CRS_DEP_MINUTES\"\n",
    "target_col = \"DEP_DELAY_NEW\"\n",
    "\n",
    "# paths & params\n",
    "BASE_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/\"\n",
    "TRAIN_PATH = BASE_PATH + \"train.parquet/\"\n",
    "VAL_PATH   = BASE_PATH + \"val.parquet/\" \n",
    "TEST_PATH  = BASE_PATH + \"test.parquet/\"\n",
    "PREDS_SAVE_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined/nn_predictions_final\"\n",
    "\n",
    "# hyperparameters\n",
    "params = {'lr': 0.0001556, 'batch_size': 4096, 'alpha': 0.342, 'time_dim': 16, \n",
    "          'emb_drop': 0.046, 'num_drop': 0.324, 'final_drop': 0.100}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# REDEFINE CLASSES AND FUNCTIONS (restart-safe)\n",
    "# =========================================================\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1): super().__init__(); self.ln = nn.LayerNorm(dim); self.fc1 = nn.Linear(dim, dim); self.fc2 = nn.Linear(dim, dim); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x): h = F.gelu(self.fc1(self.ln(x))); return x + self.fc2(self.dropout(h))\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k): super().__init__(); self.wb = nn.Linear(1, 1); self.ws = nn.Linear(1, k)\n",
    "    def forward(self, t): return torch.cat([self.wb(t), torch.sin(self.ws(t))], dim=-1)\n",
    "\n",
    "class ResFiLMMLP(nn.Module):\n",
    "    def __init__(self, cat_dims, emb_dims, num_numerical, time_dim=8, emb_dropout=0.1, num_dropout=0.1, film_dropout=0.1, final_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(d, e) for d, e in zip(cat_dims, emb_dims)]); self.emb_dropout = nn.Dropout(emb_dropout); emb_total = sum(emb_dims)\n",
    "        self.fc_num = nn.Linear(num_numerical, 256); self.res_blocks = nn.ModuleList([ResBlock(256, num_dropout) for _ in range(4)])\n",
    "        self.film = nn.Linear(256, 2 * emb_total); self.film_dropout = nn.Dropout(film_dropout); self.t2v = Time2Vec(time_dim); fused_dim = 256 + emb_total + (time_dim + 1) + 1\n",
    "        self.reg_head = nn.Sequential(nn.Linear(fused_dim, 256), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(256, 128), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(128, 1))\n",
    "        self.clf_head = nn.Sequential(nn.Linear(fused_dim, 256), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(256, 128), nn.GELU(), nn.Dropout(final_dropout), nn.Linear(128, 1))\n",
    "    def forward(self, x_cat, x_num, x_time):\n",
    "        emb = self.emb_dropout(torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)], dim=-1)); h = F.gelu(self.fc_num(x_num))\n",
    "        for block in self.res_blocks: h = block(h); gamma, beta = torch.chunk(self.film(h), 2, dim=-1); emb_mod = self.film_dropout(gamma) * emb + self.film_dropout(beta)\n",
    "        z = torch.cat([emb_mod, h, self.t2v(x_time), x_time], dim=-1); return self.reg_head(z), self.clf_head(z)\n",
    "\n",
    "class ProductionFlightDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, time_col, target_col, id_col=\"flight_uid\"):\n",
    "        self.cat = torch.tensor(df[cat_cols].values, dtype=torch.long); self.num = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.time = torch.tensor(df[time_col].values, dtype=torch.float32).unsqueeze(1); self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.ids = df[id_col].values \n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.cat[idx], self.num[idx], self.time[idx], self.y[idx], self.ids[idx]\n",
    "\n",
    "def spark_to_pandas_safe(df, indexers, categorical_cols_names, numerical_cols_names, time_col_name, target_col_name):\n",
    "    for ind in indexers: df = ind.transform(df)\n",
    "    select_expr = [sf.col(f\"{c}_idx\").cast(IntegerType()).alias(c) for c in categorical_cols_names] + \\\n",
    "                  [sf.col(c).cast(FloatType()) for c in numerical_cols_names] + \\\n",
    "                  [sf.col(time_col_name).cast(FloatType()), sf.col(target_col_name).cast(FloatType()), sf.col(\"flight_uid\")]\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    return df.select(*select_expr).toPandas()\n",
    "\n",
    "def get_metrics_from_loader(model, loader, threshold):\n",
    "    model.eval(); all_y, all_reg, all_clf = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for cat, num, time_t, y, _ in loader:\n",
    "            reg, clf = model(cat, num, time_t); all_y.append(y); all_reg.append(reg); all_clf.append(clf)\n",
    "    y_true = torch.cat(all_y).cpu().numpy().flatten(); y_reg = torch.cat(all_reg).cpu().numpy().flatten(); y_prob = torch.sigmoid(torch.cat(all_clf)).cpu().numpy().flatten()\n",
    "    mae = mean_absolute_error(y_true, y_reg); f2 = fbeta_score((y_true >= 15.0).astype(int), (y_prob >= threshold).astype(int), beta=2, zero_division=0)\n",
    "    return f2, mae\n",
    "\n",
    "def save_predictions_optimized(model, loader, fold_name, save_path_base, threshold):\n",
    "    \"\"\"Generates predictions, calculates metrics, saves to DBFS/MLflow, and generates download link.\"\"\"\n",
    "    print(f\"  > Generating predictions for {fold_name}...\")\n",
    "    \n",
    "    # 1. Start Timer \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.eval(); res = {\"flight_uid\": [], \"target_delay\": [], \"pred_delay\": [], \"target_class\": [], \"pred_prob\": [], \"pred_class_optimized\": []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cat, num, time_t, y, ids in loader:\n",
    "            reg, clf = model(cat, num, time_t); y_prob = torch.sigmoid(clf).cpu().numpy().flatten()\n",
    "            res[\"flight_uid\"].append(ids); res[\"target_delay\"].append(y.numpy().flatten()); res[\"pred_delay\"].append(reg.numpy().flatten())\n",
    "            res[\"target_class\"].append((y >= 15.0).float().numpy().flatten()); res[\"pred_prob\"].append(y_prob)\n",
    "            res[\"pred_class_optimized\"].append((y_prob >= threshold).astype(int))\n",
    "    \n",
    "    # 2. Stop Timer\n",
    "    inference_time_seconds = time.time() - start_time\n",
    "    \n",
    "    flat_ids = np.concatenate(res[\"flight_uid\"]) if len(res[\"flight_uid\"]) > 0 else []; pdf = pd.DataFrame({\n",
    "        \"flight_uid\": flat_ids, \"target_delay\": np.concatenate(res[\"target_delay\"]), \"pred_delay\": np.concatenate(res[\"pred_delay\"]), \n",
    "        \"target_class\": np.concatenate(res[\"target_class\"]), \"pred_prob\": np.concatenate(res[\"pred_prob\"]), \"pred_class_optimized\": np.concatenate(res[\"pred_class_optimized\"]),\n",
    "        \"split_name\": fold_name\n",
    "    }); \n",
    "    \n",
    "    # Calculate F2 and MAE from the collected predictions\n",
    "    final_f2 = fbeta_score(pdf[\"target_class\"], pdf[\"pred_class_optimized\"], beta=2, zero_division=0)\n",
    "    final_mae = mean_absolute_error(pdf[\"target_delay\"], pdf[\"pred_delay\"])\n",
    "    \n",
    "    # MLflow Logging\n",
    "    mlflow.set_experiment(\"/Shared/team_2_2/mlflow-nn-tower-final\"); run_name=f\"{fold_name}_Result_T={threshold:.3f}\"\n",
    "    with mlflow.start_run(run_name=run_name, nested=True):\n",
    "        mlflow.log_param(\"optimal_threshold_used\", threshold); \n",
    "        mlflow.log_metric(f\"{fold_name}_f2_optimized\", final_f2);\n",
    "        mlflow.log_metric(f\"{fold_name}_mae_optimized\", final_mae); # <-- MAE LOG ADDED\n",
    "        mlflow.log_metric(f\"{fold_name}_inference_time_seconds\", inference_time_seconds); # <-- INFERENCE TIME LOG ADDED\n",
    "    \n",
    "    # DBFS Saving\n",
    "    unique_id = str(uuid.uuid4())[:8]; save_path_final = f\"{save_path_base}/{fold_name}_T{int(threshold*100)}_{unique_id}\"\n",
    "    spark.createDataFrame(pdf).write.mode(\"overwrite\").parquet(save_path_final)\n",
    "    \n",
    "    # Download Link\n",
    "    csv_name = f\"{fold_name}_T{int(threshold*100)}_{unique_id}.csv\"; local_csv = f\"/tmp/{csv_name}\"\n",
    "    pdf.to_csv(local_csv, index=False); dbutils.fs.cp(f\"file:{local_csv}\", f\"dbfs:/FileStore/shared_uploads/predictions/{csv_name}\")\n",
    "    print(f\"  > Saved to DBFS: {save_path_final}\"); print(f\"  > Final F2 Score: {final_f2:.5f} | Final MAE: {final_mae:.4f}\") # <-- MAE PRINT ADDED\n",
    "    print(f\"  > Inference Time: {inference_time_seconds:.2f} seconds\") \n",
    "    displayHTML(f\"\"\"<div style=\"background-color:#ccffee;padding:10px;border:1px solid #91d5ff;\"><b>{fold_name} PREDICTIONS (T={threshold:.3f}):</b> <a href=\"/files/shared_uploads/predictions/{csv_name}\" target=\"_blank\">Download CSV</a></div>\"\"\")\n",
    "    if os.path.exists(local_csv): os.remove(local_csv)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# EXECUTION\n",
    "# =========================================================\n",
    "\n",
    "print(\"--- 1. Recovering Environment and Data Scalers ---\")\n",
    "train_spark_full = spark.read.parquet(TRAIN_PATH)\n",
    "print(\"  > Re-fitting String Indexers on Train Set...\")\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\").fit(train_spark_full) for c in categorical_cols]\n",
    "train_pd_full = spark_to_pandas_safe(train_spark_full, indexers, categorical_cols, numerical_cols, time_col, target_col)\n",
    "del train_spark_full; gc.collect()\n",
    "\n",
    "print(\"  > Re-fitting StandardScaler on Train Set...\")\n",
    "scaler = StandardScaler()\n",
    "train_pd_full[numerical_cols] = scaler.fit_transform(train_pd_full[numerical_cols])\n",
    "\n",
    "# Load Model\n",
    "print(\"  > Loading Model & Determining embedding dimensions...\")\n",
    "# val_spark_temp = spark.read.parquet(VAL_PATH).limit(10000)\n",
    "# val_pd_temp = spark_to_pandas_safe(val_spark_temp, indexers, categorical_cols, numerical_cols, time_col, target_col)\n",
    "# cat_dims = [int(max(val_pd_temp[c].max(), val_pd_temp[c].max()) + 2) for c in categorical_cols]\n",
    "# emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
    "# del val_spark_temp, val_pd_temp; gc.collect()\n",
    "model = mlflow.pytorch.load_model(f\"runs:/{MLFLOW_RUN_ID}/model_final\")\n",
    "\n",
    "\n",
    "# --- B. Train & Validation Set Metric Calculation and Logging ---\n",
    "print(\"\\n--- 2. Calculating and Logging TRAIN & VAL Set Metrics ---\")\n",
    "\n",
    "# 1. Train Data Prep & Loader\n",
    "train_ds = ProductionFlightDataset(train_pd_full, categorical_cols, numerical_cols, time_col, target_col)\n",
    "train_dl = DataLoader(train_ds, batch_size=params[\"batch_size\"], num_workers=0)\n",
    "del train_pd_full; gc.collect()\n",
    "\n",
    "# 2. Validation Data Prep & Loader (Full Load)\n",
    "val_spark_full = spark.read.parquet(VAL_PATH)\n",
    "val_pd_full = spark_to_pandas_safe(val_spark_full, indexers, categorical_cols, numerical_cols, time_col, target_col)\n",
    "val_pd_full[numerical_cols] = scaler.transform(val_pd_full[numerical_cols])\n",
    "val_ds = ProductionFlightDataset(val_pd_full, categorical_cols, numerical_cols, time_col, target_col)\n",
    "del val_spark_full; \n",
    "val_dl = DataLoader(val_ds, batch_size=params[\"batch_size\"], num_workers=0)\n",
    "\n",
    "\n",
    "# 3. Calculate Metrics\n",
    "train_f2, train_mae = get_metrics_from_loader(model, train_dl, OPTIMAL_THRESHOLD)\n",
    "val_f2, val_mae = get_metrics_from_loader(model, val_dl, OPTIMAL_THRESHOLD)\n",
    "\n",
    "# 4. Log Metrics (Nested under a new run for documentation)\n",
    "mlflow.set_experiment(\"/Shared/team_2_2/mlflow-nn-tower-final\")\n",
    "with mlflow.start_run(run_name=f\"Final_Reported_Metrics_T={OPTIMAL_THRESHOLD:.3f}\", nested=True):\n",
    "    mlflow.log_param(\"reporting_threshold\", OPTIMAL_THRESHOLD)\n",
    "    mlflow.log_metric(\"final_train_f2\", train_f2)\n",
    "    mlflow.log_metric(\"final_train_mae\", train_mae)\n",
    "    mlflow.log_metric(\"final_val_f2\", val_f2)\n",
    "    mlflow.log_metric(\"final_val_mae\", val_mae)\n",
    "\n",
    "# 5. Print Final Report\n",
    "print(\"\\n=======================================================\")\n",
    "print(f\"| FINAL REPORTED METRICS (Threshold: {OPTIMAL_THRESHOLD:.3f}) |\")\n",
    "print(\"=======================================================\")\n",
    "print(f\"| TRAIN SET (Internal Fit) | F2: {train_f2:.4f} | MAE: {train_mae:.4f} |\")\n",
    "print(f\"| VALIDATION SET (Tuning)  | F2: {val_f2:.4f} | MAE: {val_mae:.4f} |\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "\n",
    "# --- C. Save Validation Predictions ---\n",
    "print(\"\\n--- 3. Saving VALIDATION Predictions ---\")\n",
    "# This run will calculate F2 and MAE and log them to MLflow\n",
    "save_predictions_optimized(\n",
    "    model, \n",
    "    val_dl, \n",
    "    \"FINAL_VAL_OPTIMIZED\",\n",
    "    PREDS_SAVE_PATH, \n",
    "    OPTIMAL_THRESHOLD\n",
    ")\n",
    "\n",
    "# --- D. Run Final Optimized Inference (TEST Set) ---\n",
    "print(\"\\n--- 4. Running Final Optimized Inference on TEST Set ---\")\n",
    "test_spark = spark.read.parquet(TEST_PATH)\n",
    "if \"xgb_predicted_delay\" in test_spark.columns: test_spark = test_spark.drop(\"xgb_predicted_delay\")\n",
    "    \n",
    "test_pd = spark_to_pandas_safe(test_spark, indexers, categorical_cols, numerical_cols, time_col, target_col)\n",
    "test_pd[numerical_cols] = scaler.transform(test_pd[numerical_cols])\n",
    "\n",
    "test_ds = ProductionFlightDataset(test_pd, categorical_cols, numerical_cols, time_col, target_col)\n",
    "test_dl = DataLoader(test_ds, batch_size=params[\"batch_size\"], num_workers=0)\n",
    "del test_pd, test_spark; gc.collect()\n",
    "\n",
    "# This function calculates metrics and saves predictions, including the inference time\n",
    "# This run will calculate F2, MAE, and Inference Time, logging them to MLflow\n",
    "save_predictions_optimized(\n",
    "    model, \n",
    "    test_dl, \n",
    "    \"FINAL_TEST_OPTIMIZED\", \n",
    "    PREDS_SAVE_PATH, \n",
    "    OPTIMAL_THRESHOLD\n",
    ")\n",
    "\n",
    "print(\"\\nSUCCESS: All predictions and metrics generated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NN_Tower_MLP_arch_hpTune_finalTrain",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
