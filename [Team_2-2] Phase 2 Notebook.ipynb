{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60f6794-b509-44c6-b3bc-0a78d894b6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Moderate to Severe Flight Delay Classification\n",
    "### Team 2-2\n",
    "Daniel Costa (daniel_costa@berkeley.edu)<br>\n",
    "Ryan Farhat-Sabet (ryan_farhat-sabet@berkeley.edu)<br>\n",
    "Ankush Garg (ankush-garg@berkeley.edu) <br>\n",
    "Maia Kennedy (maia_kennedy@berkeley.edu)<br>\n",
    "Stephanie Owyang (seowyang@berkeley.edu)<br><br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/team_picture.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41df99-17d7-407e-b8b2-9e42c5320ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase Leader Plan\n",
    "\n",
    "| Week | Leader | \n",
    "| :--- | :--- | \n",
    "| 10/27 (Phase 1) | Stephanie Owyang | \n",
    "| 11/3 | Daniel Costa|\n",
    "| 11/10 (Phase 2) | Ankush Garg| \n",
    "| 11/24 | Maia Kennedy|\n",
    "| 12/1 | Ryan Farhat-Sabet|\n",
    "| 12/8 (Phase 3) | Daniel Costa | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39769ee8-a510-41be-96f8-0250b7bfa882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase 1: Credit Assignment Plan\n",
    "| **Description** | **Owner(s)** | Est. Hours | Goals\n",
    "| :--- | :--- | :--- |:--- |\n",
    "| Abstract | Stephanie Owyang | 1 | Define the objective of the project, the data and the source of the data.\n",
    "| Data Description  | Ryan Farhat-Sabet <br>Daniel Costa| 5| (Ryan) Basic analysis and understanding of data, data size and source. Analysis and plan for missing and duplicate value. Initial EDA and summary statistics. (Daniel) Develop custom join strategy\n",
    "| Data <br>(Checkpointing Strategy) | Ankush Garg <br> Maia Kennedy| 2-3| (Ankush) Plan for data ingestion and checkpointing strategy (Maia) Check compatilibity with modeling pipeline\n",
    "| Machine Algorithms | Stephanie Owyang <br> Ryan Farhat-Sabet| 2-3 | Description of potential algorithms used including their implementation and loss functions.\n",
    "| Model Training and Evaluation <br>(Cross-validation Strategy) | Ankush Garg <br> Maia Kennedy|2-3 | (Ankush) Develop cross-validation strategy for potential algorithms (Maia) Check compatilibity with modeling pipeline\n",
    "| Evaluation Metrics | Daniel Costa | 2| Description of metrics and analysis to be used including equations for metrics.\n",
    "| Modeling Pipeline | Maia Kennedy| 2| Plan modeling pipeline from data ingestion to model including a block diagram of the architecture\n",
    "| Project Planning <br> (gantt chart) | Maia Kennedy | 1| Map project milestones and delivery dates for each phase\n",
    "| Custom Join | Daniel Costa | 8 | Analyze data for interpolation approach and implement in PySpark\n",
    "\n",
    "### Phase 2: Credit Assignment Plan\n",
    "\n",
    "| **Description** | **Owner(s)** | Est. Hours | Goals\n",
    "| :--- | :--- | :--- |:--- |\n",
    "| Presentation | Ankush Garg| 6 - 8| Ankush will lead development and management of the presentation, and each team member will be responsible for the section’s slides\n",
    "| Abstract | Ankush Garg|2 - 3| Ankush will evolve the phase 1 abstract, covering the business problem, baseline choice & results, key features, and next steps. \n",
    "| Project Description| Daniel Costa |3 - 4 | Daniel will detail data and task descriptions with required sections/subsections and workflow diagrams. \n",
    "| EDA | Ryan Farhat-Sabet<br>Daniel Costa |4 - 6 | Ryan will summarize the detailed EDA from the notebook. By creating, formatting, and annotating the best visuals. <br> Daniel will implement the custom join strategy\n",
    "| Modeling Pipelines| Maia Kennedy <br> Stephanie Owyang| 6 - 8| Maia will visualize the data pipelines, feature family counts, loss function, cluster/time details. <br> Stephanie will make a comprehensive experiment table.\n",
    "| Training Results and Discussion| Stephanie Owyang <br>Ankush Garg | 4 - 6 | Stephanie will report training and blind testing results, and working with Ankush link key pipelines to experimental results and justify findings\n",
    "| Conclusion | Ankush Garg| 2 - 3| Anksuh will summarize our findings, restate focus, hypothesis, summarize points, significance, note and future work.\n",
    "\n",
    "### Phase 3: Credit Assignment Plan\n",
    "| **Description** | **Owner(s)** | Est. Hours | Goals\n",
    "| :--- | :--- | :--- |:--- |\n",
    "| Presentation | Daniel Costa | 6 - 8| Daniel will lead development and management of the presentation, and each team member will be responsible for the section’s slides\n",
    "| Abstract | Daniel Costa | 3 - 4| Daniel will incorporate all phases in the abstract, the new experiments, the final selected model, and the final numerical results.\n",
    "| Data and Feature Engineering | Ryan Farhat-Sabet <br> Stephanie Owyang | 7 - 10| Ryan will detail Data lineage, key transformations, list of all feature families. <br> Stephanie will focus on experiments demonstrating the value of each feature/family.\n",
    "| Neural Network | Stephanie Owyang <br> Daniel Costa | 4 - 6| Stephanie and Daniel will implement the MLP architectures (2-layer) and present the results/training curves.\n",
    "| Modeling Pipeline| Maia Kennedy | 4 - 6| Maia will visualize the final pipeline(s). Detail experiment table covering the baseline, additional experiments, final tuned model, key hyperparameters, loss functions, computational configuration, and wall time for each\n",
    "| Leakage| Ankush Garg | 4 - 6| Ankush will thoroughly inspect the pipeline for violations\n",
    "| Results and Discussion| Daniel Costa <br> Maia Kennedy |7 - 10| Daniel will present, interpret, analyze, and compare results from all phases. Maia will include a gap analysis of the final best pipeline.\n",
    "| Conclusion | Daniel Costa | 3 - 5 | Daniel will restate focus/importance, restate hypothesis, summarize key points (best model, top 10 features, hyperparameters), significance, and future work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993f01f0-0ce9-4098-a47e-5d61c2f550ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "The objective of this project is to develop a predictive model to forecast and communicate critical operational delays to airline operators and air traffic controllers. The core analysis will use the OTPW joined dataset, combining comprehensive flight data from the U.S. Department of Transportation with corresponding weather data from the National Oceanic and Atmospheric Administration. Our initial Exploratory Data Analysis (EDA) of this dataset revealed a significant class imbalance, with only 18.29% of flights experiencing delays of fifteen minutes or more. Given that both False Positives like unnecessarily re-routing flights, and False Negatives such as missing a critical delay opportunity, carry substantial operational costs, the primary validation metric for model selection will be the Precision-Recall Curve Area Under the Curve (PRC-AUC), which provides a robust measure of performance that inherently accounts for both class imbalance and the critical balance between precision and recall.\n",
    "<br><br>\n",
    "Our approach will involve a sequential, two-model prediction strategy. We will begin by training an XGBoost regressor to generate a highly reliable, denoised continuous delay signal, leveraging its intrinsic robustness against sparse matrices. This continuous signal will then be fed into a Random Forest Classifier as a high-fidelity feature to generate the final, discrete classification designed to directly inform and trigger operational decisions. Furthermore, we plan to explore advanced feature engineering, specifically Time-based and Graph-based features, and will implement a client-requested Multilayer Perceptron (MLP) network architecture to establish the optimal, production-ready pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23505007-7126-4736-b259-0accfd5bc6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Description\n",
    "We began by examining all of the data at our disposal. The data is split into three larger datasets with a few smaller, explanatory datasets: a flight dataset with all US passenger flight information, a smaller, separate airport dataset with airport codes and locations, a weather dataset with detailed weather information at specific weather stations and times, a smaller weather station dataset with weather station location information, and a combined dataset that joins the flight and weather datasets along with their supplemental airport and weather station data.\n",
    "<br>\n",
    "<br>\n",
    "- Flight Dataset\n",
    "    - source: Department of Transportation (DOT)\n",
    "    - size: 31,746,841 x 109\n",
    "\n",
    "- Weather Dataset\n",
    "    - source: National Oceanic and Atmospheric Administration repository (NOAA)\n",
    "    - size: 630,904,436 x 177\n",
    "\n",
    "- Airport Dataset\n",
    "    - source: Department of Transportation (DOT)\n",
    "    - size: 18,097 x 10\n",
    "\n",
    "- OTPW Dataset\n",
    "    - source: joined Arrival Time Performance (DOT) with local Weather data (NOAA)\n",
    "    - size: 1401363 x 216\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a420cb38-179b-4ac0-80ed-3018390c477f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e9c310-5e6c-4002-bd9f-bc2e15ab1e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Looking first at the flight data, the dataset itself comes from the Department of Transportation. It has a lot of flight metadata, including date and time information, departure/arrival airport and location data, airline information, and flight route status and timing, all of which we will likely use. To call out a few specific ones that look particularly interesting:\n",
    "- FL_DATE\n",
    "- OP_CARRIER\n",
    "- ORIGIN/DEST_AIRPORT\n",
    "- DEP_TIME\n",
    "\n",
    "As far as completeness of this dataset, all of the above-mentioned features have very few nulls. Many of the fields having to do with when a flight was diverted are haphazardly populated at best, so we will likely ignore those fields. Cause of delay fields also have many nulls, but this is because not all flights are delayed, so we can impute these values with zeros. Most airlines seem to run flights along similar schedules, with one day a week consistently seeing fewer flights.\n",
    "\n",
    "<!-- !-- ![flight_nulls.png](/Workspace/Users/seowyang@berkeley.edu/flight_nulls.png) --\n",
    "\n",
    "!-- ![flights_time_series.png](/Workspace/Users/seowyang@berkeley.edu/flights_time_series.png) -- -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/flight_nulls.png\">\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/flights_time_series.png\">\n",
    "\n",
    "Since the airport code dataset itself is mainly supplemental, we really only need enough data to join it to the flight data, so identity code and name should be enough. There are 57,421 airports in the dataset itself. Some other identity-related fields have lots of nulls, but of those features with no or few nulls, the important ones we're likely to include are:\n",
    "- Coordinates\n",
    "- Type (ex: small_airport, medium_airport, large_airport)\n",
    "- Elevation\n",
    "\n",
    "All airports have coded coordinates and type, and most open passenger airports have elevation, so no data deletion will be necessary here.\n",
    "<br>\n",
    "<!-- !-- ![airport_nulls.png](/Workspace/Users/seowyang@berkeley.edu/airport_nulls.png) -- -->\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/airport_nulls.png\">\n",
    "\n",
    "Moving on to the weather and weather station data, the data is taken from the National Centers for Environmental Information, and it is the messiest of the bunch. We first realized this data was not limited to just the US, so we had to narrow down the data ourselves. We then identified a whole host of features we liked to potentially include:\n",
    "- Hourly data (wind speed, wind direction, dry bulb temp, relative humidity, visibility, sky conditions, precipitation)\n",
    "- Daily and Monthly data (temperature max/min/avg, sunrise/sunset, weather, snow, precipitation, wind)\n",
    "\n",
    "As far as completeness of this dataset, there is a lot of null data. A lot of this stems from the features themselves and how each datapoint is recorded. Each line is a point in time, so it could be the weather report at a very specific minute, whereas a lot of the data, especially a lot of the null data, is at the daily/monthly level. We should be able to impute and copy some of the daily/monthly metrics into the hourly data to fill in some of the nulls there. The hourly data that is null mainly stems from specific weather patterns that aren't always present, like precipitation, so we should be able to impute zero values for some of those variables.\n",
    "\n",
    "<!-- !-- ![weather_nulls.png](/Workspace/Users/seowyang@berkeley.edu/weather_nulls.png) -- -->\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/weather_nulls.png\">\n",
    "\n",
    "Not all stations report in equal time increments as well, so we probably won't want to use all the data anyway. We are thinking of narrowing down the data to just reports at the top of every hour; we see many more stations reporting at the top of the hour, so this, along with the imputation we described above, will likely help us resolve a good portion of the nulls and allow us to use the variables we identified. If we are unable to successfully impute this or we discover that there are still too many nulls, we might narrow the data down further to just daily forecasts for each station, either from the daily forecast already in the data or by averaging/summing the features we want to capture.\n",
    "<br>\n",
    "\n",
    "<!-- !-- ![daily_weather.png](/Workspace/Users/seowyang@berkeley.edu/daily_weather.png) -- -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/daily_weather.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0aad8d9-9c42-4d88-9fe2-b4bb20e0e3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Custom Join\n",
    "\n",
    "Our custom join will use spatiotemporal Kriging interpolation to estimate the weather conditions at both source and destination airports for each flight at departure and arrival times respectively. There are a few assumptions that this type of Kriging interpolation makes:\n",
    "\n",
    "- Stationarity: The mean of weather readings is constant across local space and time\n",
    "- Isotropy: Covariance of interpolation points is only dependent on the distance between points, not their location or orientation\n",
    "\n",
    "In order to make this interpolation computationally tractable, we will be constraining the complexity of our algorithm in the following ways:\n",
    "\n",
    "- We will only take into account 10 hours of preceding weather readings (corresponding to 12 hours before prediction time). If during further EDA we discover the data is too sparse, we may expand this time horizon. We will not include weather readings after the desired reading time to avoid data leakage.\n",
    "- We will only interpolate readings from stations within a fixed radius of each airport. Although the continuity and decay of weather phenomena vary greatly, for simplicity/parallelization we will use the same radius to define the neighborhood to interpolate all weather readings.\n",
    "\n",
    "There is a natural tradeoff between expanding the radius of interpolation and the computational complexity of our algorithm. While we would ideally use global interpolation to maximize the accuracy of our model while minimizing null observations, this comes at the cost of ballooning matrix calculations and lack of meterological continuity between distant stations. To explore an optimal choice in interpolation radius, we conducted EDA on the distribution of stations relative to US airports. The top chart below includes histograms of the number of stations within several radii of each US airport. To visualize the tradeoff in airport coverage vs. interpolation radius, the bottom chart shows that the maximum curvature of this plot occurs at around 50km, corresponding to an expected number of interpolation stations of about 4.22 per airport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f26e0a1-0828-4a68-a17f-86a569ec5240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<!-- ![histograms_stations.png](/Workspace/Users/seowyang@berkeley.edu/histograms_stations.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/histograms_stations.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d218f4-b70b-4098-8a28-3f66fcfb20b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<!-- <p style=\"text-align: center;\">\n",
    "  <img src=\"/Workspace/Users/seowyang@berkeley.edu/updated_coverage_airports_2.png\" width=\"800\"/>\n",
    "</p> -->\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/updated_coverage_airports_2.png\">  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba717e9d-90ed-4e63-afda-4a4c7aa28fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This radius would provide reliable weather readings for 93.37% of airports while greatly reducing runtime relative to global interpolation. Significantly beyond 50km, assumptions of stationarity begin to break down, and weather patterns can become discontinuous, so providing non-null imputations for remote airports would introduce significant noise to our model. To address this, we favor machine learning algorithms that can handle null values, such as XGBoost.\n",
    "\n",
    "To preserve information about the degree of certainty/proximity of our interpolations, we will record the variance associated with each reading for potential use as a model feature.\n",
    "\n",
    "We also need to assess the feasibility of our algorithm on the larger dataset. In the final model setting of 5 years worth of flight data, we can interpolate readings at each hour for each airport with a flight arrival or departure 2 hours after a given time. Here are some relevant statistics to assess the feasibility of our algorithm:\n",
    "\n",
    "- In the 3-month dataset, an average of 1277 airports are \"active\" (at least 1 departing or arriving flight) during a given hour.\n",
    "- N = 22764 or 39.64% of airports in the dataset are within the United States\n",
    "- N = 2787 or 22.26% of weather stations in the dataset are within 50km of a U.S. airport\n",
    "- We will need to interpolate 5 * 365 * 24 = 43,800 timesteps in the 5-year dataset\n",
    "- There are only 18 weather metric columns with at least 5% non-null values in the 3-month dataset.\n",
    "- Station weights can be pre-computed per-airport and only take up around 22764 airports * 4 stations * 4 bytes/coefficient = 364KB of space.\n",
    "\n",
    "Taking all this together, we see that we will have to perform on the order of 1,277*43,800 = 55.93M interpolations, each consisting of at most 18 measurements and around 4 stations * 10 hours = 40 weighted summands (with 40 multiplications and 39 additions), totaling around 79.53B FLOPs. From a pure computation standpoint, this is feasible on our hardware, but the bottleneck will be I/O. Luckily, the 364KB static weather station weight matrix associated with each airport can be pre-computed and broadcast, and since all time steps can be calculated independently, this task is embarassingly parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36d8b09-d76f-4169-84ef-001418398b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Checkpoint Strategy\n",
    "\n",
    "<!-- ![checkpointing.png](/Workspace/Users/seowyang@berkeley.edu/checkpointing.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/checkpointing.png\">\n",
    "\n",
    "We're converting all CSVs into Parquet format for our data ingestion pipeline, which offers significant advantage in terms of storage efficiency and faster read/writes. Since we're planning to join the flights dataset with weather information, maintaining everything in Parquet format ensures optimal performance when handling the large files.\n",
    "\n",
    "For checkpoint strategy: we will first split the combined dataset into train, test, and validation sets to prevent any data leakage, establishing this as our initial checkpoint. This step is critical because it ensures that all subsequent transformations are fitted exclusively on training data, preventing any leakage. We will then preprocess the training data by applying techniques such as imputation for missing values, and standardization for numerical features, checkpointing the cleaned dataset at the stage. Next, we'll develop our feature engineering pipeline, carefully fitting on training data, and transforing the three splits accordingly, with another checkpoint capturing these fully transformed datasets. Throughout the preprocessing and feature engineering stages, we'll build and save the complete training pipeline, which encapsulates all transformation steps for easy reproducibility.\n",
    "\n",
    "Finally, we'll train and validate our models, saving both the training models and their performance results. The pipeline will then be tested on the test data (unseen) to evaluate real-world performance. This systematic checkpointing approach creates clear recovery points throughout our workflow, allowing us to iterate efficiently on different stages without reprocessing from scratch, preventing data leakage, and ensuring reliable model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8ef224-db0d-4277-af33-462802f129a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Machine Algorithms and Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efd80fd-ab1a-452b-939b-d602f1513296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cross Validation Strategy\n",
    "\n",
    "<!-- ![cross_validaiton.png](/Workspace/Users/seowyang@berkeley.edu/cross_validation.jpeg) -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/cross_validation.jpeg\">\n",
    "\n",
    "We plan to split the dataset into a 70/10/20 train/validation/test by time. To evaluate model performance while preventing leakage, we plan to use a time series cross validation approach with a two-hour gap in between training and validation. Given that predictions must be made 2 hours prior to scheduled departure, we're implementing a rolling window validation scheme where each fold consists of a training period, a 2-hour gap period, and a validation period. The 2-hour gap between the training and validation sets ensures that no information from after the prediction time can inadvertently influence the model training. This gap is critial for flight delay prediction because delays cascade through airline networks. For example: a flight delayed at 3:00 PM may affect subsequent departures, and without a temporal gap, the model could learn from the information that is not available at the prediction time (2 hours ahead of the flight schedule). When predicting a 4:00 PM departure at 2:00 PM, we can only use delay information from flights that departed before 2:00 PM, not the 3:00 PM flight's outcome.\n",
    "\n",
    "Our validation strategy uses 5 folds with rolling windows across our dataset spanning 2015-2021. Each training window contains approximately 4-5 years of data, with validation periods of several months per fold. The rolling window approach maintains consistent training set sizes across folds, ensuring stable model performance and comparable computational requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7065984d-a8f4-4d8b-bc35-8204d303e79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Machine Learning Algorithms\n",
    "\n",
    "Our EDA revealed the data is quite noisy and has a lot of missing values. While we have a strong preprocessing and filtering strategy to mitigate this, we still anticipate some amount of data imperfections to reach our model pipeline. To address this, we will use a cascade model where the first model is a regression model predicting the travel delay in minutes, and the second model is a classification model classifying if the delay is slightly delayed, moderately delayed, significantly delayed or severely delayed. The wording and definition may change for each class, but this is where we will start.\n",
    "\n",
    "**Regression Model (Tier 1)**\n",
    "- Implementation: `XGBoost`\n",
    "- Loss Function: Mean Absolute Error (MAE)\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**Classificaiton Model (Tier 2)**\n",
    "- Implementation: `Random Forest`\n",
    "-  Loss Function: Multiclass Cross Entropy\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss } (L) = -\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(p_{i,c})\n",
    "$$\n",
    "\n",
    "\n",
    "The regression model is designed to leverage the inherent noise-tolerance of ensemble tree models and architected to perform non-linear feature engineering. With XGBoost, the model learns the optimal direction for a split when an observation’s feature value is missing, strengthening the model's resilience against features with high dimensionality of null entries and preserving predictive integrity. XGBoost’s built in regularization feature and second-order Taylor approximation optimization makes it a better option over Random Forest or linear regression. With this regression model we are creating a highly informed prediction for the target. The loss function will be the mean absolute error since the data is noisy and contains outliers. This prediction is more powerful and reliable than any single, raw input feature, making the input to the classifier cleaner.\n",
    "\n",
    "This clean, predictive output goes straight into the classification model as a denoised feature. The Random Forest classifier now benefits from this highly reliable signal, its job is much simpler, and we get dramatically improved accuracy and stability. Random Forest offers intrinsic robustness to residual variance from the regression model, and its bagging ensemble mechanism mitigates high variance that resists overfitting. The parallelizable training methodology also accelerates the fitting process and inference latency, ideal for our large datasets. For our baseline, we will use multiclass cross-entropy log loss, but suspect we may need to use a more advanced loss function like focal loss to address the highly imbalanced data. \n",
    "\n",
    "$$\n",
    "\\text{Focal Loss} (p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)\n",
    "$$\n",
    "\n",
    "This tiered structure gives us valuable diagnostic capability, so if the final prediction is wrong, we can look at the regression model's output to immediately pinpoint whether the problem was a poor magnitude estimate or a failed classification boundary.\n",
    "\n",
    "A final classification output was selected for its interpretability and risk assessment for front-line decision-makers. While the regression model output provides a high-fidelity magnitude estimate, the classification model explicitly maps this magnitude to a probability distribution over predefined operational states. The discrete output immediately determines resource allocation, enabling controllers to instantly decide if a flight needs a gate change, if a crew will time out, or if air traffic sequencing must be altered, thus optimizing real-time decision-making where precise actions, not continuous predictions, are required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32438866-3969-45e4-baa9-f2faa1b6301c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Metrics\n",
    "\n",
    "In order to deliver a model that best suits the needs of air traffic controllers, we need to optimize for metrics that account for both the costs and benefits of our predictions, as well as the class balance of our dataset.\n",
    "\n",
    "For example, consider the following outcomes of our final model:\n",
    "\n",
    "- True Positive: We correctly predict that a flight will be delayed. This could allow air traffic controllers to re-allocate gates and runways, potentially saving several minutes for dozens of flights - a huge net gain especially during congestion.\n",
    "- True Negative: We correctly predict that a flight will be on time. This will help air traffic controllers to focus on other flights more likely to need their attention.\n",
    "- False Positive: We incorrectly predict that a flight will be delayed. This could cause several planes to be re-routed unnecessarily and cause the flight in question to circle the skies waiting to land and waiting to park at their previously allocated gate. This mistake could snowball to downstream flights and occupy stand-by space that other genuinely delayed flights may need.\n",
    "- False Negative: We incorreclty predict that a flight will be on time. There will be idle space on the runways, and air traffic controllers will miss the opportunity to make up for other true delays.\n",
    "\n",
    "Additionally, it is important that our metrics account for the fact only around 18.29% of flights are delayed by 15 minutes or more, leading to significant class imbalance. We don't want our validation metrics to lead us to chose a model that fails to learn the important minority class.\n",
    "\n",
    "<!-- ![response_dist.png](/Workspace/Users/seowyang@berkeley.edu/response_dist.png) -->\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/response_dist.png\">\n",
    "</div>\n",
    "\n",
    "Between class imbalance and prediction consequences, the former will inform our choice of validation metric for model selection, while the latter will help us choose a classification threshold that leverages our model's predictive power while also optimizing for airport safety KPIs. To this end, a few of validation metrics are disqualified:\n",
    "\n",
    "- Accuracy: This metric is notoriously sensitive to class imbalance, and could lead us to optimize for a model that fails to learn the minority class at all by simply always predicting the majority class.\n",
    "- ROC-AUC: Similarly, this metric does not handle class imbalance well. Integrating under a curve plotting true-positive rate (TP / (TP + FN)) against false-positive rate (FP / (FP + TN)). If the negative class is large and a model largely favors classifying records as negative, the FPR will have a much larger denominator and comparable numerator, causing X to tend smaller than Y - inflating the ROC-AUC integral of this faulty model and hurting downstream KPIs.\n",
    "\n",
    "While precision and recall are both valid options for a key metric, focusing on one and not the other would ignore important consequences of our model. Precision ignores false negatives, and recall ignores false positives. If there are known costs associated with these errors, both are necessary in choosing our model, and both our validation metric and thresholding metric should account for this.\n",
    "\n",
    "Thus, for our validation metric we will use PRC-AUC, helping us to select a model that will give us the best balance between precision and recall regardless of threshold. This will set us up for success to use F_1 as a metric when deciding our threshold. If information about the cost associated with any of the four classification outcomes, we could empirically determine the costs of false positives and false negatives to choose a Beta to optimize F_Beta and choose a threshold. \n",
    "\n",
    "Alternatively, if we have additional information about costs associated with true positives and true negatives, we could incorporate all 4 of these coefficients into a utility function to optimize our threshold. During our EDA, we will be exploring heuristics to approximate such coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f02bdb5-2129-4235-ae13-fbb49adf63ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Machine Learning Pipelines\n",
    "\n",
    "The figure below illustrates the end-to-end machine learning pipeline we will implement for our flight delay prediction system. The design follows Databricks’ modular workflows emphasizing reproducibility, efficiency, and operational realism. Each stage of the pipeline introduces formal checkpoints to ensure data integrity, model traceability, and temporal consistency throughout the experimentation and deployment lifecycle.\n",
    "\n",
    "The data ingestion stage consolidates multiple raw data sources into unified, high-performance Parquet tables. Converting CSVs into Parquet format improves I/O performance, reduces storage overhead, and optimizes large-scale joins required for feature enrichment. The ingestion stage produces a single, time-indexed dataset representing all relevant operational and environmental factors.\n",
    "\n",
    "The Data Engineering & Checkpointing stage prepares the dataset for modeling. The pipeline begins by performing a temporal split into training, validation, and test sets following a 70/10/20 ratio by time. To prevent data leakage and ensure causal validity, a two-hour exclusion gap is introduced between the training and validation windows. This ensures that no information about future flights leaks into the training process. Each major transformation step (splitting, cleaning, feature engineering) is checkpointed, enabling efficient re-runs and consistent intermediate outputs. Engineered features, such as lagged delay metrics, temporal indicators, and weather-derived variables, are stored in a Feature Store, allowing for reusability across models and experiments.\n",
    "\n",
    "The Modeling & Time-Series Cross-Validation stage formalizes model training and evaluation through a rolling window cross-validation scheme. The dataset is divided into five temporal folds, separated by a 2-hour gap. This design closely mirrors real-world airline operations, where predictions must be made two hours prior to departure, and future data cannot be accessed at prediction time. Each fold trains a complete preprocessing and modeling pipeline, applies hyperparameter tuning, and evaluates performance metrics. The final model is retrained on the full training period and stored alongside the fitted preprocessing pipeline to ensure complete reproducibility.\n",
    "\n",
    "The Evaluation, Deployment, and Monitoring stage validates the final model on a held-out test set that represents future, unseen data. Experiment tracking through MLflow records metrics, hyperparameters, and artifacts, enabling transparent model comparison. The best-performing models are registered in the Model Registry and deployed to production where predictions are generated at least two hours before scheduled departure. \n",
    "\n",
    "Continuous monitoring is implemented to detect performance drift and trigger scheduled retraining, ensuring long-term model reliability and alignment with operational conditions.\n",
    "Overall, this pipeline design emphasizes reproducibility, leakage prevention, and temporal validity. The inclusion of structured checkpoints and feature versioning ensures that each experiment remains traceable and recoverable, while the rolling validation design provides an empirically sound foundation for model selection and performance reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be35f86-4859-4e68-bb3c-7086a827faef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<!-- ![261_ML_Pipeline5.png](/Workspace/Users/seowyang@berkeley.edu/261_ML_Pipeline5.png) -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/261_ML_Pipeline5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7000c8e9-47dc-407d-a53a-9f6211799596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Team Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ebe017c-6249-4cea-8ac7-313a9769a73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gantt Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70f5d4f-7566-4968-b888-af7e1cc184f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To streamline progress monitoring and accountability, we have developed a dynamic project tracker that provides real-time updates on task ownership and internal deadlines. The project is structured into phased milestones, each aligned with client deliverables, submission requirements, and upcoming requests.\n",
    "Leadership rotates weekly, designating a different team member as lead to encourage shared ownership and diverse decision-making perspectives. Regular check-ins are scheduled to maintain alignment and foster collaboration across workstreams.\n",
    "\n",
    "Phases are interdependent by design. For example, the initial exploratory data analysis (EDA) conducted this week establishes the foundation for deeper analytical work and advanced EDA during subsequent feature engineering stages. This progressive approach ensures that insights evolve alongside the modeling pipeline.\n",
    "\n",
    "Finally, we have collectively discussed team priorities, individual development goals, and preferred working styles to optimize task allocation, leverage existing expertise, and provide opportunities for growth in new technical areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9b9041-978e-4dbf-912f-c853a6a68278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<!-- ![261_Gantt_Chart_Project_Plan.png](/Workspace/Users/seowyang@berkeley.edu/261_Gantt_Chart_Project_Plan.png) -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/261_Gantt_Chart_Project_Plan.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37024f2-3500-4bf4-bee7-252869aae530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[Team_2-2] Phase 2 Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
