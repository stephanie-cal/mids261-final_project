{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PIN_THREAD'] = 'false'\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efed1655-84c8-4947-934c-0b7956a52912",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763839566213}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in feature engineered custom joined data\n",
    "month_or_year = \"1_year_custom_joined\"\n",
    "train_df = spark.read.parquet(f\"dbfs:/student-groups/Group_2_2/{month_or_year}/feature_eng/training_splits/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"dbfs:/student-groups/Group_2_2/{month_or_year}/feature_eng/training_splits/validation.parquet\")\n",
    "\n",
    "df = train_df.unionByName(validation_df)\n",
    "\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "print(df.count())\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48661458-d75f-4d4f-9256-e1d6f99c6e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7bbd64-8024-444a-9b1c-60e5a6abd229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine date and scheduled departure time\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c11636-0f97-4444-ae07-2f093109b2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Splits for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bfe8bf3-76f2-44da-8527-e63e29e68ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Truncate timestamp to hour level\n",
    "df_indexed = df.withColumn(\n",
    "    \"hour\", \n",
    "    F.date_trunc(\"hour\", F.col(\"utc_timestamp\"))\n",
    ")\n",
    "\n",
    "# Create time index based on unique hours\n",
    "window_spec = Window.orderBy(\"hour\")\n",
    "df_indexed = df_indexed.withColumn(\n",
    "    \"time_idx\", \n",
    "    F.dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "df_indexed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e6ac15a-407b-41b7-a670-a6c09ede6b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 M splits config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de439e60-8e63-4173-990e-96c348187267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_time_idx = df_indexed.agg(F.max(\"time_idx\")).collect()[0][0]\n",
    "print(f\"  Max time index: {max_time_idx}\")\n",
    "\n",
    "train_size = 720      # 30 days (720 hours)\n",
    "gap_size = 2          # 2 hours\n",
    "val_size = 168        # 7 days (168 hours)\n",
    "step_size = 85       # Calculated to get exactly 10 folds\n",
    "\n",
    "fold_window_size = train_size + gap_size + val_size\n",
    "n_folds = (max_time_idx - fold_window_size) // step_size + 1\n",
    "print(f\"Step 2: Calculated {n_folds} folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7245718-dde9-4742-8143-4bffc3b59a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 Year splits config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a70bff7-db39-4892-9d38-e0647de70ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_time_idx = df_indexed.agg(F.max(\"time_idx\")).collect()[0][0]\n",
    "print(f\"  Max time index: {max_time_idx}\")\n",
    "\n",
    "train_size = 720*4      # 30 days (720 hours)\n",
    "gap_size = 2          # 2 hours\n",
    "val_size = 168*4        # 7 days (168 hours)\n",
    "step_size = 90*4       # Calculated to get exactly 10 folds\n",
    "\n",
    "fold_window_size = train_size + gap_size + val_size\n",
    "n_folds = (max_time_idx - fold_window_size) // step_size + 1\n",
    "print(f\"Step 2: Calculated {n_folds} folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf9f18a-7e97-4901-812e-6397b27b7ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_mapping = []\n",
    "\n",
    "for fold_id in range(1, n_folds + 1):\n",
    "    fold_start = 1 + (fold_id - 1) * step_size\n",
    "    # print(fold_id, fold_start)\n",
    "    for t in range(fold_start, fold_start + train_size):\n",
    "        fold_mapping.append((t, fold_id, \"train\"))\n",
    "    \n",
    "    for t in range(fold_start + train_size, fold_start + train_size + gap_size):\n",
    "        fold_mapping.append((t, fold_id, \"gap\"))\n",
    "\n",
    "    for t in range(fold_start + train_size + gap_size, fold_start + train_size + gap_size + val_size):\n",
    "        fold_mapping.append((t, fold_id, \"validation\"))\n",
    "\n",
    "fold_df = spark.createDataFrame(fold_mapping, [\"time_idx\", \"fold_id\", \"split_type\"])\n",
    "\n",
    "result = df_indexed.join(\n",
    "    F.broadcast(fold_df),\n",
    "    on='time_idx',\n",
    "    how='inner'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9d0f39-847c-4e38-acfe-0a5266787463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if input(\"Careful! About to overwrite splits. If you want to continue, type y\") == \"y\":\n",
    "    result.write \\\n",
    "    .partitionBy(\"fold_id\", \"split_type\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7119ff85-a394-41b5-9644-caedb89f27fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to read the CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488c9531-9a7a-45b7-9837-ef369d4b230d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    return spark.read.parquet(f\"{path}/fold_id={fold_id}/split_type={split_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f5528e-6b64-4016-84b0-5228a19e0bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n",
    "\n",
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"tail_num_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e732e7e-3b42-4296-85e1-5c3d78506174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    \"\"\"\n",
    "    Read a specific fold from partitioned parquet data.\n",
    "    Falls back to filtering if direct partition read fails.\n",
    "    \"\"\"\n",
    "    fold_path = f\"{path}/fold_id={fold_id}/split_type={split_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Try direct partition read\n",
    "        return spark.read.parquet(fold_path)\n",
    "    except:\n",
    "        # Fallback: read all data and filter\n",
    "        print(f\"Direct read failed for fold {fold_id}, using filter method...\")\n",
    "        all_data = spark.read.parquet(path)\n",
    "        return all_data.filter(\n",
    "            (all_data.fold_id == fold_id) & \n",
    "            (all_data.split_type == split_type)\n",
    "        )\n",
    "\n",
    "\n",
    "# Your original train function works as-is now\n",
    "def train_cv_models(n_folds=10, month_or_year=\"3_month_custom_joined\"):\n",
    "    cv_results = []\n",
    "    cv_models = []\n",
    "\n",
    "    linear_reg = LinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"DEP_DELAY_NEW\",\n",
    "        maxIter=10, \n",
    "        regParam=0.3\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[\n",
    "        carrier_indexer, origin_indexer, dest_indexer, tail_num_indexer,\n",
    "        carrier_encoder, origin_encoder, dest_encoder, tail_num_encoder,\n",
    "        assembler,\n",
    "        linear_reg\n",
    "    ])\n",
    "\n",
    "    mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    mlflow.spark.autolog(disable=True)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LR_CV_3_MONTH\") as parent_run:\n",
    "        MODEL_NAME = \"LR_CV_3_MONTH\"\n",
    "        \n",
    "        mlflow.log_param(\"n_folds\", n_folds)\n",
    "        mlflow.log_param(\"maxIter\", 10)\n",
    "        mlflow.log_param(\"regParam\", 0.3)\n",
    "        mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "        mlflow.log_param(\"dataset\", month_or_year)\n",
    "        \n",
    "        fold_metrics = {\n",
    "            'train_mae': [], 'val_mae': [],\n",
    "            'train_rmse': [], 'val_rmse': []\n",
    "        }\n",
    "\n",
    "        for fold_id in range(1, n_folds + 1):\n",
    "            # Child run for this fold\n",
    "            with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True) as fold_run:\n",
    "                \n",
    "                fold_train = read_specific_fold(\n",
    "                    path=f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\", \n",
    "                    fold_id=fold_id, \n",
    "                    split_type=\"train\"\n",
    "                )\n",
    "                fold_val = read_specific_fold(\n",
    "                    path=f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\", \n",
    "                    fold_id=fold_id, \n",
    "                    split_type=\"validation\"\n",
    "                )\n",
    "\n",
    "                print(f\"Fold {fold_id} - Train size: {fold_train.count()}, Val size: {fold_val.count()}\")\n",
    "\n",
    "                print(f\"Training fold {fold_id}...\")\n",
    "                model = pipeline.fit(fold_train)\n",
    "                \n",
    "                print(f\"Making predictions...\")\n",
    "                training_predictions = model.transform(fold_train)\n",
    "                validation_predictions = model.transform(fold_val)\n",
    "                \n",
    "                mae_t = mae_evaluator.evaluate(training_predictions)\n",
    "                mae_v = mae_evaluator.evaluate(validation_predictions)\n",
    "                rmse_t = rmse_evaluator.evaluate(training_predictions)\n",
    "                rmse_v = rmse_evaluator.evaluate(validation_predictions)\n",
    "                \n",
    "                fold_metrics['train_mae'].append(mae_t)\n",
    "                fold_metrics['val_mae'].append(mae_v)\n",
    "                fold_metrics['train_rmse'].append(rmse_t)\n",
    "                fold_metrics['val_rmse'].append(rmse_v)\n",
    "                \n",
    "                # Log to child run\n",
    "                mlflow.log_metrics({\n",
    "                    \"train_mae\": mae_t,\n",
    "                    \"val_mae\": mae_v,\n",
    "                    \"train_rmse\": rmse_t,\n",
    "                    \"val_rmse\": rmse_v,\n",
    "                })\n",
    "                \n",
    "                cv_results.append({\n",
    "                    'fold': fold_id,\n",
    "                    'train_mae': mae_t,\n",
    "                    'val_mae': mae_v,\n",
    "                    'train_rmse': rmse_t,\n",
    "                    'val_rmse': rmse_v\n",
    "                })\n",
    "                cv_models.append(model)\n",
    "                \n",
    "                print(f\"Fold {fold_id} - Train MAE: {mae_t:.4f}, Val MAE: {mae_v:.4f}\")\n",
    "            \n",
    "            # After child run closes, log to parent run\n",
    "            # Now we're back in the parent run context automatically\n",
    "            mlflow.log_metrics({\n",
    "                f\"fold_{fold_id}_train_mae\": mae_t,\n",
    "                f\"fold_{fold_id}_val_mae\": mae_v,\n",
    "                f\"fold_{fold_id}_train_rmse\": rmse_t,\n",
    "                f\"fold_{fold_id}_val_rmse\": rmse_v,\n",
    "            })\n",
    "            print(\"=\"*120)\n",
    "        \n",
    "        import numpy as np\n",
    "        mlflow.log_metrics({\n",
    "            \"avg_train_mae\": np.mean(fold_metrics['train_mae']),\n",
    "            \"avg_val_mae\": np.mean(fold_metrics['val_mae']),\n",
    "            \"std_val_mae\": np.std(fold_metrics['val_mae']),\n",
    "            \"avg_train_rmse\": np.mean(fold_metrics['train_rmse']),\n",
    "            \"avg_val_rmse\": np.mean(fold_metrics['val_rmse']),\n",
    "            \"std_val_rmse\": np.std(fold_metrics['val_rmse'])\n",
    "        })\n",
    "        \n",
    "        # Create DataFrame from cv_results and log as table\n",
    "        results_df = pd.DataFrame(cv_results)\n",
    "        mlflow.log_table(data=results_df, artifact_file=\"cv_fold_results.json\")\n",
    "        \n",
    "        print(f\"\\nCV Complete - Avg Val MAE: {np.mean(fold_metrics['val_mae']):.4f} Â± {np.std(fold_metrics['val_mae']):.4f}\")\n",
    "        print(f\"\\nFold Results:\\n{results_df.to_string()}\")\n",
    "    \n",
    "    return cv_results, cv_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113edef5-63d2-49a7-979f-027cf684f374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_results, cv_models = train_cv_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23373f57-2d55-49d3-9a5d-28c25599ae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6978085643709185,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CV-3M",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
