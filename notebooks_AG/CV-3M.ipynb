{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PIN_THREAD'] = 'false'\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efed1655-84c8-4947-934c-0b7956a52912",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763839566213}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in feature engineered custom joined data\n",
    "train_df = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/feature_eng/train.parquet\")\n",
    "validation_df = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/feature_eng/validation.parquet\")\n",
    "\n",
    "df = train_df.unionByName(validation_df)\n",
    "\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "print(df.count())\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7bbd64-8024-444a-9b1c-60e5a6abd229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine date and scheduled departure time\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c11636-0f97-4444-ae07-2f093109b2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Splits for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bfe8bf3-76f2-44da-8527-e63e29e68ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Truncate timestamp to hour level\n",
    "df_indexed = df.withColumn(\n",
    "    \"hour\", \n",
    "    F.date_trunc(\"hour\", F.col(\"utc_timestamp\"))\n",
    ")\n",
    "\n",
    "# Create time index based on unique hours\n",
    "window_spec = Window.orderBy(\"hour\")\n",
    "df_indexed = df_indexed.withColumn(\n",
    "    \"time_idx\", \n",
    "    F.dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "df_indexed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e6ac15a-407b-41b7-a670-a6c09ede6b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 M splits config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de439e60-8e63-4173-990e-96c348187267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_time_idx = df_indexed.agg(F.max(\"time_idx\")).collect()[0][0]\n",
    "print(f\"  Max time index: {max_time_idx}\")\n",
    "\n",
    "train_size = 720      # 30 days (720 hours)\n",
    "gap_size = 2          # 2 hours\n",
    "val_size = 168        # 7 days (168 hours)\n",
    "step_size = 90       # Calculated to get exactly 10 folds\n",
    "\n",
    "fold_window_size = train_size + gap_size + val_size\n",
    "n_folds = (max_time_idx - fold_window_size) // step_size + 1\n",
    "print(f\"Step 2: Calculated {n_folds} folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7245718-dde9-4742-8143-4bffc3b59a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 Year splits config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a70bff7-db39-4892-9d38-e0647de70ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# max_time_idx = df_indexed.agg(F.max(\"time_idx\")).collect()[0][0]\n",
    "# print(f\"  Max time index: {max_time_idx}\")\n",
    "\n",
    "# train_size = 720*4      # 30 days (720 hours)\n",
    "# gap_size = 2          # 2 hours\n",
    "# val_size = 168*4        # 7 days (168 hours)\n",
    "# step_size = 130*4       # Calculated to get exactly 10 folds\n",
    "\n",
    "# fold_window_size = train_size + gap_size + val_size\n",
    "# n_folds = (max_time_idx - fold_window_size) // step_size + 1\n",
    "# print(f\"Step 2: Calculated {n_folds} folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf9f18a-7e97-4901-812e-6397b27b7ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_mapping = []\n",
    "\n",
    "for fold_id in range(1, n_folds + 1):\n",
    "    fold_start = 1 + (fold_id - 1) * step_size\n",
    "    # print(fold_id, fold_start)\n",
    "    for t in range(fold_start, fold_start + train_size):\n",
    "        fold_mapping.append((t, fold_id, \"train\"))\n",
    "    \n",
    "    for t in range(fold_start + train_size, fold_start + train_size + gap_size):\n",
    "        fold_mapping.append((t, fold_id, \"gap\"))\n",
    "\n",
    "    for t in range(fold_start + train_size + gap_size, fold_start + train_size + gap_size + val_size):\n",
    "        fold_mapping.append((t, fold_id, \"validation\"))\n",
    "\n",
    "fold_df = spark.createDataFrame(fold_mapping, [\"time_idx\", \"fold_id\", \"split_type\"])\n",
    "\n",
    "result = df_indexed.join(\n",
    "    F.broadcast(fold_df),\n",
    "    on='time_idx',\n",
    "    how='inner'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9d0f39-847c-4e38-acfe-0a5266787463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if input(\"Careful! About to overwrite splits. If you want to continue, type y\") == \"y\":\n",
    "    result.write \\\n",
    "    .partitionBy(\"fold_id\", \"split_type\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/cv_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7119ff85-a394-41b5-9644-caedb89f27fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to read the CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488c9531-9a7a-45b7-9837-ef369d4b230d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    return spark.read.parquet(f\"{path}/fold_id={fold_id}/split_type={split_type}\")\n",
    "\n",
    "fold_1_val = read_specific_fold(path=f\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/cv_splits\", fold_id=1, split_type=\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05f5528e-6b64-4016-84b0-5228a19e0bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n",
    "\n",
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"tail_num_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94313128-a68e-4d6a-b721-7fba875bd6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# linear regression baseline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.spark.autolog()\n",
    "with mlflow.start_run(run_name=\"lr - weather baseline 3m\"):\n",
    "    MODEL_NAME = \"LR_WEATHER_BASELINE_3M\"\n",
    "\n",
    "    linear_reg = LinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"DEP_DELAY_NEW\",\n",
    "        # Linear Regression has different parameters than Random Forest\n",
    "        maxIter=10, \n",
    "        regParam=0.3\n",
    "    )\n",
    "\n",
    "    # rf = RandomForestRegressor(\n",
    "    #     featuresCol=\"features\",  \n",
    "    #     labelCol=\"DEP_DELAY_NEW\",   \n",
    "    #     numTrees=20,\n",
    "    #     maxDepth=10\n",
    "    # )\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(stages=[\n",
    "        carrier_indexer, origin_indexer, dest_indexer, tail_num_indexer,\n",
    "        carrier_encoder, origin_encoder, dest_encoder, tail_num_encoder,\n",
    "        assembler,\n",
    "        linear_reg\n",
    "        # rf\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(train_3m_df)\n",
    "    training_predictions = model.transform(train_3m_df)\n",
    "    validation_predictions = model.transform(validation_3m_df)\n",
    "\n",
    "    mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_t = mae_evaluator.evaluate(training_predictions)\n",
    "    mae_v = mae_evaluator.evaluate(validation_predictions)\n",
    "    # Calculate RMSE\n",
    "    rmse_t = rmse_evaluator.evaluate(training_predictions)\n",
    "    rmse_v = rmse_evaluator.evaluate(validation_predictions)\n",
    "\n",
    "    signature = infer_signature(train_df, training_predictions)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        model, \n",
    "        MODEL_NAME,\n",
    "        input_example=train_df.limit(1).toPandas(),\n",
    "        signature=signature,\n",
    "        registered_model_name=\"flight_delay_prediction_baseline\"\n",
    "        )\n",
    "\n",
    "    mlflow.log_metric(\"train_mae\", mae_t)\n",
    "    mlflow.log_metric(\"validation_mae\", mae_v)\n",
    "    mlflow.log_metric(\"train_rmse\", rmse_t)\n",
    "    mlflow.log_metric(\"validation_rmse\", rmse_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e732e7e-3b42-4296-85e1-5c3d78506174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_cv_models(train_pipeline, evaluator, n_folds=10):\n",
    "  cv_results = []\n",
    "  cv_models = []\n",
    "  \n",
    "  for fold_id in range(1, n_folds + 1):\n",
    "    fold_train = read_specific_fold(path=f\"dbfs:/student-groups/Group_2_2/1_year_custom_joined/cv_splits\", fold_id=fold_id, split_type=\"train\")\n",
    "    fold_1_val = read_specific_fold(path=f\"dbfs:/student-groups/Group_2_2/1_year_custom_joined/cv_splits\", fold_id=1, split_type=\"validation\")\n",
    "    \n",
    "    print(f\"Training fold {fold_id}...\")\n",
    "    train_pipeline.fit(fold_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6978085643709185,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CV-3M",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
