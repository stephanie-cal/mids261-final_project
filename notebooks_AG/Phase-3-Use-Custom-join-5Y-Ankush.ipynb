{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PIN_THREAD'] = 'false'\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9d2cb9-bb9d-4e67-bd6f-b3c101c73df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/student-groups/Group_2_2/5_year_custom_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efed1655-84c8-4947-934c-0b7956a52912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in custom joined data\n",
    "custom_joined_path = 'dbfs:/student-groups/Group_2_2/5_year_custom_joined'\n",
    "\n",
    "df = spark.read.parquet(custom_joined_path)\n",
    "\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "print(df.count())\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94ebd20-41bb-4147-808d-278f49b2c9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for column_name in df.columns:\n",
    "    print(f\"{column_name} ------> {df.filter(F.col(column_name).isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38767e31-d3f0-4741-a025-b53d950ddd4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Column analysis - Can remove later\n",
    "- OP_UNIQUE_CARRIER == OP_CARRIER_AIRLINE_ID == OP_CARRIER == OP_CARRIER_FL_NUM\n",
    "- TAIL_NUMBER --> License Plate Number\n",
    "- ORIGIN_AIRPORT_SEQ_ID == ORIGIN_CITY_MARKET_ID == ORIGIN == ORIGIN_CITY_NAME == ORIGIN_STATE_ABR == ORIGIN_STATE_FIPS (code for state) == ORIGIN_STATE_NM == ORIGIN_WAC (origin airport, world area code)\n",
    "- DEST_AIRPORT_ID == DEST_AIRPORT_SEQ_ID == DEST_CITY_MARKET_ID == DEST == DEST_CITY_NAME == DEST_STATE_ABR == DEST_STATE_FIPS == DEST_STATE_NAME == DEST_WAC\n",
    "- CRS_DEP_TIME -> Scheduled departure time in the computer reservation system\n",
    "---\n",
    "- DEP_TIME -> Actual Dept time\n",
    "- DEPT_DELAY -> difference in minutes between scheduled and actual departure time\n",
    "- DEPT_DELAY_NEW -> early flights are 0\n",
    "- DEP_DEL15\n",
    "- DEP_DELAY_GROUP\n",
    "- DEP_TIME_BLK\n",
    "- TAXI_OUT -> Taxi out time in minutes\n",
    "- WHEELS OFF\n",
    "- WHEELS ON - Time at landing (local time)\n",
    "- TAXI_IN\n",
    "- CRS ARR TIME - scheduled arrival time\n",
    "- ARR_TIME - actual arrival time\n",
    "- ARR_DELAY\n",
    "- ARR_DELAY_NEW\n",
    "- ARR_DEL15\n",
    "- ARR_DELAY_GROUP\n",
    "- ARR_TIME_BLK\n",
    "- CANCELLED, CANELLATION_CODE, DIVERTED\n",
    "- CRS_ELAPSED_TIME - scheduled flight time\n",
    "- ACTUAL_ELAPSED_TIME\n",
    "- AIR_TIME = flight time in minutes\n",
    "- FLIGHTS = Number of flights (Idk what this means)\n",
    "- DISTANCE\n",
    "- DISTANCE_GROUP (every 250 miles)\n",
    "- CARRIER_DELAY\n",
    "- WEATHER_DELAY\n",
    "- NAS_DELAY (National air system delay)\n",
    "- SECURITY_DELAY\n",
    "- LATE_AIRCRAFT_DELAY\n",
    "- FIRST_DEP_TIME - first gate departure time at origin airport (nulls see what to do with them)\n",
    "- TOTAL_ADD_GTIME - total ground time away from gate for gate return or cancelled flight\n",
    "- LONGEST_ADD_GTME - longest time away from gate for gate return or cancelled flight\n",
    "- A BUNCH OF DIVERTED AIRPORT COLUMNS (do some eda, they seem empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "924d73f8-797f-419e-a4c6-a2973ba7b1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Things to keep in mind\n",
    "- Predict two hours before\n",
    "- Remove all the delay columns\n",
    "- Are we only predicting departure delays or arrival delays also? For example, the pilot misses the landing, and has to circle back for 20 minutes. Should we solve for that? I don't think we should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d49ea99-57cc-4ba9-b438-7386aa04e939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessing / Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c79c86e-1796-406c-a96d-43c0b244fbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of the dataset\n",
    "df = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efb6463-6f5c-4337-bb60-7a3d614940b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine date and scheduled departure time\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b283c45-ce7c-4a18-8401-18c7d26ea587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Split 5 Year joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992d2acd-319d-442d-b6e4-d877b88c74ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_by_year(df, time_col=\"utc_timestamp\"):\n",
    "    # Extract year from timestamp\n",
    "\n",
    "    # REMOVE ALL CANCELLED FLIGHTS\n",
    "    df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "\n",
    "    df = df.sort('utc_timestamp')\n",
    "    \n",
    "    df = df.withColumn(\"year\", F.year(F.col(time_col)))\n",
    "    \n",
    "    # Test set: all rows from 2019\n",
    "    test_df = df.filter(F.col(\"year\") == 2019)\n",
    "    \n",
    "    # Train/validation: all rows before 2019\n",
    "    train_val_df = df.filter(F.col(\"year\") < 2019)\n",
    "    \n",
    "    # Optionally, split train/validation chronologically (e.g., 80/20)\n",
    "    total_rows = train_val_df.count()\n",
    "    train_end = int(total_rows * 0.8)\n",
    "    \n",
    "    window = Window.orderBy(time_col)\n",
    "    train_val_df = train_val_df.withColumn(\"row_num\", F.row_number().over(window))\n",
    "    train_df = train_val_df.filter(F.col(\"row_num\") <= train_end).drop(\"row_num\")\n",
    "    validation_df = train_val_df.filter(F.col(\"row_num\") > train_end).drop(\"row_num\")\n",
    "    \n",
    "    # Drop helper column from test set\n",
    "    # test_df = test_df.drop(\"year\")\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = split_by_year(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f4b638-d0f1-46d3-a231-ffa22cfb76c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e5bd64-cf4c-4575-9f6f-6357989e9e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the last utc_timestamp from train_df\n",
    "last_flight_ts = train_df.agg(F.max(\"utc_timestamp\").alias(\"last_ts\")).collect()[0][\"last_ts\"]\n",
    "\n",
    "# Add a 2 hour gap\n",
    "gap_ts = F.timestamp_add(\"HOUR\", F.lit(2), F.lit(last_flight_ts))\n",
    "\n",
    "# Filter validation_df to keep everything after the gap timestamp\n",
    "validation_after_gap_df = validation_df.filter(F.col(\"utc_timestamp\") > gap_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70fff051-4e93-4c84-af75-a736bd8e8275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write joined splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cabbbb6-918f-4a3e-bc7f-3c008b6adade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check validation dataset size\n",
    "print(f\"Validation row count: {validation_after_gap_df.count()}\")\n",
    "print(f\"Validation partitions: {validation_after_gap_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bf8cbe-d368-42c4-8eca-ff74aa910952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validation_after_gap_df = validation_after_gap_df.repartition(20)\n",
    "test_df = test_df.repartition(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b72a714-f0d5-4f19-9214-7e8fc838c387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if input(\"CAREFUL: You're about to write to DBFS. Type 'y' to continue.\") == \"y\":\n",
    "    checkpoint_dataset(train_df, \"5_year_custom_joined/raw_data/training_splits/train\")\n",
    "    checkpoint_dataset(validation_after_gap_df, \"5_year_custom_joined/raw_data/training_splits/validation\")\n",
    "    checkpoint_dataset(test_df, \"5_year_custom_joined/raw_data/training_splits/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "772c5ce8-150f-4366-acb8-823a80ae2293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d6f9c3-47ea-4ff0-8ed4-67f891e00385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "dataset_path = f\"{checkpoint_path}/1_year_custom_joined/raw_data/training_splits\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44a1f367-4098-4cc0-a7d5-b52634530fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ignoring the weather columns, will have to figure out better imputation strategies later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09e1d73-f355-4f44-adcf-b54923741e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])\n",
    "\n",
    "validation_df = validation_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9ad4ff-0453-41cc-8c32-577b10bcdcdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## CRS_DEP_TIME is local time so we can use this feature \n",
    "## But in order to use it, we have to convert it to minutes since midnight\n",
    "## Otherwise the timing will be off b/c it's not true UTC\n",
    "\n",
    "train_df = train_df.\\\n",
    "        withColumn(\"CRS_DEP_MINUTES\", (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))).\\\n",
    "        drop(\"CRS_DEP_TIME\").\\\n",
    "        drop(\"CRS_ARR_TIME\")\n",
    "\n",
    "validation_df = validation_df.\\\n",
    "        withColumn(\"CRS_DEP_MINUTES\", (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))).\\\n",
    "        drop(\"CRS_DEP_TIME\").\\\n",
    "        drop(\"CRS_ARR_TIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b48fba-396a-4a0f-9280-78250938628d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Eng.\n",
    "\n",
    "#### Was the previous flight delayed? And by how much was the previous flight delayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96047694-161f-431a-9fdf-f9a11b3618eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88bc6f0-ba4d-4358-84c2-bf54aa9dfcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n",
    "    \n",
    "validation_df = validation_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f88cb621-66e4-424c-917a-85011399dd63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Number of delays before in the last 4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c950b161-bad1-412a-b602-6d9f43a93574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "useful_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_DEP_MINUTES\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"utc_timestamp\",\n",
    "    \"prev_flight_delay_in_minutes\",\n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    'HourlyDryBulbTemperature',\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed'  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2254a6a6-a88e-4a4c-a407-4ea08be698a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_4h = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-14400, -7200) # 4 hours to 2 hours before\n",
    "\n",
    "train_df = train_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )\n",
    "validation_df = validation_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d9c263-ad93-4094-9f1d-45388954d17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_DEP_MINUTES\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"utc_timestamp\",\n",
    "    \"prev_flight_delay_in_minutes\",\n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    'HourlyDryBulbTemperature',\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed'  \n",
    "]\n",
    "\n",
    "\n",
    "train_df = train_df.filter(F.col(\"DEP_DELAY_NEW\").isNotNull()).select(baselines_columns)\n",
    "validation_df = validation_df.filter(F.col(\"DEP_DELAY_NEW\").isNotNull()).select(baselines_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff4688-854e-4a7f-9429-b74fdc925f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "# tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "# tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843a6331-dbc1-4547-a14f-b08445f3e626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, check the cardinality of your categorical features\n",
    "print(\"Carrier count:\", train_df.select(\"OP_CARRIER\").distinct().count())\n",
    "print(\"Origin count:\", train_df.select(\"ORIGIN_AIRPORT_SEQ_ID\").distinct().count())\n",
    "print(\"Dest count:\", train_df.select(\"DEST_AIRPORT_SEQ_ID\").distinct().count())\n",
    "print(\"Tail num count:\", train_df.select(\"TAIL_NUM\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde39db-852d-4ce5-882d-a3f7f7cf3f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2261bf73-37b9-4449-8bd0-445fd78d1659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# linear regression baseline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "train_df = train_df.repartition(50)\n",
    "\n",
    "mlflow.spark.autolog()\n",
    "with mlflow.start_run(run_name=\"RF - 1 year dataset\"):\n",
    "    MODEL_NAME = \"RF_ORIGIN_DELAYS\"\n",
    "\n",
    "    # linear_reg = LinearRegression(\n",
    "    #     featuresCol=\"features\",\n",
    "    #     labelCol=\"DEP_DELAY_NEW\",\n",
    "    #     # Linear Regression has different parameters than Random Forest\n",
    "    #     maxIter=10, \n",
    "    #     regParam=0.3\n",
    "    # )\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",  \n",
    "        labelCol=\"DEP_DELAY_NEW\",   \n",
    "            numTrees=10,\n",
    "            maxDepth=5,\n",
    "            maxBins=32,\n",
    "            subsamplingRate=0.8  # Use 80% of data per tree\n",
    ")\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(stages=[\n",
    "        carrier_indexer, origin_indexer, dest_indexer,\n",
    "        carrier_encoder, origin_encoder, dest_encoder,\n",
    "        assembler,\n",
    "        # linear_reg\n",
    "        rf\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(train_df)\n",
    "    training_predictions = model.transform(train_df)\n",
    "    validation_predictions = model.transform(validation_df)\n",
    "\n",
    "    mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_t = mae_evaluator.evaluate(training_predictions)\n",
    "    mae_v = mae_evaluator.evaluate(validation_predictions)\n",
    "    # Calculate RMSE\n",
    "    rmse_t = rmse_evaluator.evaluate(training_predictions)\n",
    "    rmse_v = rmse_evaluator.evaluate(validation_predictions)\n",
    "\n",
    "    signature = infer_signature(train_df, training_predictions)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        model, \n",
    "        MODEL_NAME,\n",
    "        input_example=train_df.limit(1).toPandas(),\n",
    "        signature=signature,\n",
    "        registered_model_name=\"flight_delay_prediction_baseline\"\n",
    "        )\n",
    "\n",
    "    mlflow.log_metric(\"train_mae\", mae_t)\n",
    "    mlflow.log_metric(\"validation_mae\", mae_v)\n",
    "    mlflow.log_metric(\"train_rmse\", rmse_t)\n",
    "    mlflow.log_metric(\"validation_rmse\", rmse_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca6e576c-33d9-412d-a2ea-f0bc9035128c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Develop and Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce98ccb9-f492-48d3-940c-4c237eaf83e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c39b08-da9a-4430-85af-61cb665b1965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save metrics, pipeline, any other steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc20216a-d892-45d1-8042-3f13d3f2496b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d8aae1-c078-4cad-9e1c-162d66cf7434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering Ideas\n",
    "- Previous flight delay in minutes for the aircraft [DONE] - this added value to the linear regression model!\n",
    "- Number of delayed flights from 4 hours (DONE)\n",
    "- Number of delays in the route in the last 30 days\n",
    "- Time between landing and scheduled current flight\n",
    "- Airport + utc time type of delay - Ohare at 6PM is always late\n",
    "- Number of delayed flights in departure and arrival location (total or 4 hours before, 6 hours before, etc.)\n",
    "- Average delay time by airport\n",
    "- Average taxi out time by airport/flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e732e7e-3b42-4296-85e1-5c3d78506174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8173636742503592,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase-3-Use-Custom-join-5Y-Ankush",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
