{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PIN_THREAD'] = 'false'\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848d9d47-06c1-4b42-8e94-3541106feaf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71296232-b78e-410e-9742-438588e14d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in feature engineered custom joined data\n",
    "month_or_year = \"1_year_custom_joined\"\n",
    "train_df = spark.read.parquet(f\"dbfs:/student-groups/Group_2_2/{month_or_year}/feature_eng/training_splits/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"dbfs:/student-groups/Group_2_2/{month_or_year}/feature_eng/training_splits/validation.parquet\")\n",
    "test_df = spark.read.parquet(f\"dbfs:/student-groups/Group_2_2/{month_or_year}/feature_eng/training_splits/test.parquet\")\n",
    "\n",
    "df = train_df.unionByName(validation_df)\n",
    "\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "print(df.count())\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23373f57-2d55-49d3-9a5d-28c25599ae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"utc_timestamp\",\n",
    "    \"CRS_DEP_MINUTES\",            # feature eng start\n",
    "    \"prev_flight_delay_in_minutes\", \n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",        # feature eng end\n",
    "    'HourlyDryBulbTemperature',     # weather start\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed'               # weather end\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7de9c3-346c-422e-85da-bc20a8264d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d43976c-df29-4229-95cd-64774314978d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'                   # weather end\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7690a544-2dfe-4815-8ccc-fd416bea2f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from mlflow.models import infer_signature\n",
    "MODEL_NAME = \"XGB_TUNED_CVD_FINAL\"\n",
    "\n",
    "# --- Model Estimators ---\n",
    "preprocessing_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    assembler \n",
    "]\n",
    "\n",
    "# A. XGBoost Regressor\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=2, \n",
    "    max_depth=6,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    objective=\"reg:squarederror\"\n",
    "\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer, tail_num_indexer,\n",
    "    carrier_encoder, origin_encoder, dest_encoder, tail_num_encoder,\n",
    "    assembler,\n",
    "    xgb\n",
    "    # linear_reg\n",
    "    # rf\n",
    "])\n",
    "with mlflow.start_run(run_name=\"Baseline - random forest\"):\n",
    "    model = pipeline.fit(train_df)\n",
    "    training_predictions = model.transform(train_df)\n",
    "    test_predictions = model.transform(test_df)\n",
    "\n",
    "    # Define DBFS paths\n",
    "    train_pred_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/train_predictions\"\n",
    "    test_pred_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/test_predictions\"\n",
    " \n",
    "\n",
    "    # Save as Parquet to DBFS\n",
    "    training_predictions.select(\"DEP_DELAY_NEW\", \"prediction\").write.mode(\"overwrite\").parquet(train_pred_path)\n",
    "    test_predictions.select(\"DEP_DELAY_NEW\", \"prediction\").write.mode(\"overwrite\").parquet(test_pred_path)\n",
    "\n",
    "    # Log artifacts\n",
    "    mlflow.log_artifacts(train_pred_path, \"train_predictions\")\n",
    "    mlflow.log_artifacts(test_pred_path, \"test_predictions\")\n",
    "\n",
    "    mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_train = mae_evaluator.evaluate(training_predictions)\n",
    "    mae_test = mae_evaluator.evaluate(test_predictions)\n",
    "    # Calculate RMSE\n",
    "    rmse_train = rmse_evaluator.evaluate(training_predictions)\n",
    "    rmse_test = rmse_evaluator.evaluate(test_predictions)\n",
    "\n",
    "    signature = infer_signature(df, training_predictions)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        model, \n",
    "        MODEL_NAME,\n",
    "        input_example=df.limit(1).toPandas(),\n",
    "        signature=signature,\n",
    "        registered_model_name=\"flight_delay_prediction_baseline\"\n",
    "        )\n",
    "\n",
    "    mlflow.log_metric(\"train_mae\", mae_train)\n",
    "    mlflow.log_metric(\"validation_mae\", mae_test)\n",
    "    mlflow.log_metric(\"train_rmse\", rmse_train)\n",
    "    mlflow.log_metric(\"validation_rmse\", rmse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae753be8-8abb-4990-aefa-c09e6a954969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create table with performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d42c23a-7b47-41c4-901e-876563eb67da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model_uri = 'runs:/1f03617a78bb42efb5177d7927b819f5/XGB_TUNED_CVD_FINAL'\n",
    "\n",
    "# Load as Spark model directly\n",
    "loaded_model = mlflow.spark.load_model(model_uri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314073b5-2c06-4da7-b701-db105b370b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday/training_splits\n",
    "\n",
    "train_df = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday/training_splits/train.parquet\")\n",
    "validation_df = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday/training_splits/validation.parquet\")\n",
    "test_df = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdd7dcd-ff1e-46fc-98ea-3163e56fb6c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",      \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = loaded_model.transform(train_df)\n",
    "test_predictions_2015 = loaded_model.transform(test_df)\n",
    "validation_predictions = loaded_model.transform(validation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71037f8b-1cae-4349-8b9b-72ccd6a8e305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_mae = mae_evaluator.evaluate(train_predictions)\n",
    "train_rmse = rmse_evaluator.evaluate(train_predictions)\n",
    "test_mae = mae_evaluator.evaluate(test_predictions)\n",
    "test_rmse = rmse_evaluator.evaluate(test_predictions)\n",
    "validation_mae = mae_evaluator.evaluate(validation_predictions)\n",
    "validation_rmse = rmse_evaluator.evaluate(validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e18ded7-6f33-48b0-be98-abed8e4a5024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New features\n",
    "import pandas as pd\n",
    "\n",
    "# Create dictionary with metrics\n",
    "metrics_dict = {\n",
    "    'Dataset': ['Train', 'Test', 'Validation'],\n",
    "    'MAE': [train_mae, test_mae, validation_mae],\n",
    "    'RMSE': [train_rmse, test_rmse, validation_rmse]\n",
    "}\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "# Display\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd3ea534-10a7-440a-b4e3-6eb2bdd2bb50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Scores on 4 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ab381d-72ba-441e-97ff-6365f5f8f3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "month_or_year = \"5_year_custom_joined/\"\n",
    "# \\dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits\n",
    "dataset_path = f\"{checkpoint_path}/{month_or_year}/fe_graph_and_holiday/training_splits\"\n",
    "\n",
    "\n",
    "# # Read datasets from checkpoint\n",
    "train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")\n",
    "test_df = spark.read.parquet(f\"{dataset_path}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac577db7-8027-48e3-894d-4325f8df2f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 1: Get the schema for each DataFrame\n",
    "# train_schema = train_df.dtypes\n",
    "# validation_schema = validation_df.dtypes\n",
    "# test_schema = test_df.dtypes\n",
    "\n",
    "# # Step 2: Find columns with mismatched types\n",
    "# for idx, (t, v, te) in enumerate(zip(train_schema, validation_schema, test_schema)):\n",
    "#     if not (t[1] == v[1] == te[1]):\n",
    "#         print(f\"Column {idx+1}: {t[0]} - train: {t[1]}, validation: {v[1]}, test: {te[1]}\")\n",
    "\n",
    "# # Step 3: Cast all mismatched columns to a common type\n",
    "# mismatched_cols = []\n",
    "# for idx, (t, v, te) in enumerate(zip(train_schema, validation_schema, test_schema)):\n",
    "#     if not (t[1] == v[1] == te[1]):\n",
    "#         mismatched_cols.append(t[0])\n",
    "\n",
    "# for col in mismatched_cols:\n",
    "#     train_df = train_df.withColumn(col, train_df[col].cast(\"string\"))\n",
    "#     validation_df = validation_df.withColumn(col, validation_df[col].cast(\"string\"))\n",
    "#     test_df = test_df.withColumn(col, test_df[col].cast(\"string\"))\n",
    "\n",
    "# Step 4: Union by NAME, not position\n",
    "full_dataset = train_df.unionByName(validation_df).unionByName(test_df)\n",
    "display(full_dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe33a95-5b7c-43f8-b92f-77e0ebb484ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "year_2016 = full_dataset.filter(full_dataset['year'] == '2016')\n",
    "year_2017 = full_dataset.filter(full_dataset['year'] == '2017')\n",
    "year_2018 = full_dataset.filter(full_dataset['year'] == '2018')\n",
    "year_2019 = full_dataset.filter(full_dataset['year'] == '2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadc82db-968f-4db7-9999-c3cbe797e092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def split_top_bottom(df, sort_col=\"utc_timestamp\", train_ratio=0.8):\n",
    "    # Create window spec for ordering\n",
    "    window_spec = Window.orderBy(sort_col)\n",
    "    \n",
    "    df_with_percentile = df.withColumn(\n",
    "        \"percentile_bucket\", \n",
    "        F.ntile(100).over(window_spec)\n",
    "    )\n",
    "    \n",
    "    split_point = int(train_ratio * 100)\n",
    "    train_val_df = df_with_percentile.filter(F.col(\"percentile_bucket\") <= split_point).drop(\"percentile_bucket\")\n",
    "    test_df = df_with_percentile.filter(F.col(\"percentile_bucket\") > split_point).drop(\"percentile_bucket\")\n",
    "    \n",
    "    return train_val_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150ad340-c986-4611-9f73-373931fe1f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_val_2016, test_2016 = split_top_bottom(year_2016)\n",
    "train_val_2017, test_2017 = split_top_bottom(year_2017)\n",
    "train_val_2018, test_2018 = split_top_bottom(year_2018)\n",
    "train_val_2019, test_2019 = split_top_bottom(year_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3988a6fd-2b9d-45b7-b775-373b48ac39d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def get_predictions(df):\n",
    "    cast_columns = {\n",
    "        \"YEAR\": \"integer\",\n",
    "        \"CRS_DEP_MINUTES\": \"double\",\n",
    "        \"prev_flight_delay_in_minutes\": \"double\",\n",
    "        \"prev_flight_delay\": \"double\",\n",
    "        \"origin_delays_4h\": \"double\",\n",
    "        \"delay_origin_7d\": \"double\",\n",
    "        \"delay_origin_carrier_7d\": \"double\",\n",
    "        \"delay_route_7d\": \"double\",\n",
    "        \"flight_count_24h\": \"double\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\": \"double\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\": \"double\",\n",
    "        \"HourlyCloudElevation\": \"double\",\n",
    "        \"HourlyWindSpeed\": \"double\"\n",
    "    }\n",
    "\n",
    "    for col_name, target_type in cast_columns.items():\n",
    "        df = df.withColumn(\n",
    "            col_name, \n",
    "            col(col_name).cast(target_type)\n",
    "        )\n",
    "\n",
    "    return loaded_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101d7f4e-a3fe-4f0b-b7a6-f428be382ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_predictions_2016 = get_predictions(test_2016)\n",
    "test_predictions_2017 = get_predictions(test_2017)\n",
    "test_predictions_2018 = get_predictions(test_2018)\n",
    "test_predictions_2019 = get_predictions(test_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6700c1b-7fdd-470b-8698-e34c6ce18f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_mae_2015 = mae_evaluator.evaluate(test_predictions_2015)\n",
    "test_mae_2016 = mae_evaluator.evaluate(test_predictions_2016)\n",
    "test_mae_2017 = mae_evaluator.evaluate(test_predictions_2017)\n",
    "test_mae_2018 = mae_evaluator.evaluate(test_predictions_2018)\n",
    "test_mae_2019 = mae_evaluator.evaluate(test_predictions_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37808bdd-308c-4093-b495-da03b9bf49e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Year\": [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\"],\n",
    "    \"Test_MAE\": [test_mae_2015, test_mae_2016, test_mae_2017, test_mae_2018, test_mae_2019]\n",
    "})\n",
    "\n",
    "print(metrics_df)\n",
    "print(f\"\\nMean MAE: {metrics_df['Test_MAE'].mean():.2f}\")\n",
    "print(f\"Std Dev: {metrics_df['Test_MAE'].std():.2f}\")\n",
    "print(f\"Range: {metrics_df['Test_MAE'].max() - metrics_df['Test_MAE'].min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33abbca0-823f-4bb7-acb7-45f9f8fcdcf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f76f2aa-8b7b-461d-a024-2f2a42dcca3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_test_predictions = loaded_model.transform(test_df)\n",
    "all_validation_predictions = loaded_model.transform(validation_df)\n",
    "all_train_predctions = loaded_model.transform(train_df)\n",
    "\n",
    "all_test_mae = mae_evaluator.evaluate(all_test_predictions)\n",
    "all_train_mae = mae_evaluator.evaluate(all_train_predctions)\n",
    "all_validation_mae = mae_evaluator.evaluate(all_validation_predictions)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Dataset\": [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"MAE\": [all_train_mae, all_validation_mae, all_test_mae]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b260a1-265c-4af7-ac6d-02fdfc8b572b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7133245340534945,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Test_predictions_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
