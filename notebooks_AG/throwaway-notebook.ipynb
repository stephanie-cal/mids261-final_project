{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d6a5ac-8bbf-47d4-822e-27b3f1b35460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# XGBOOST model baseline - 1 year - hyperparameter tuning testing\n",
    "- run model from 1_year_combined data with feature engineering\n",
    "  - TAIL_NUM causes OOM error, comment out for now\n",
    "- featuring engineering handled in https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1792055957780055?o=4021782157704243\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets - custom join\n",
    "- get checkpoint data\n",
    "  - 1 year combined join, with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e82e60a-8a87-407b-a471-dc0f631286a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stephanie's latest features = 1_year_custom_joined/feature_eng_ph3/training_splits/\n",
    "# Daniel's Graph features = 1_year_custom_joined/graph_feature_splits\n",
    "# Raw splits = 3_month_custom_joined/raw_data/training_splits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7b274b-c8d2-4949-9a8a-2b58e6a17564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/student-groups/Group_2_2/5_year_custom_joined/raw_data/training_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc3628a-3798-477c-8afc-c627758eaed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_splits = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/raw_data/training_splits/train.parquet/\")\n",
    "val_splits = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/raw_data/training_splits/validation.parquet/\")\n",
    "test_splits = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/raw_data/training_splits/test.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253d3deb-2254-497a-a393-912e97420f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_splits.count(), val_splits.count(), test_splits.count()\n",
    "# (951978, 133991, 271994)\n",
    "# (5100978, 726381, 1457423)\n",
    "# (19175825, 4791467, 7287112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4825eb9-9441-4e5d-a2c3-6adc6cea10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71052af-525f-4775-a2cc-79142b51f875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"utc_timestamp\",\n",
    "    \"CRS_DEP_MINUTES\",            # feature eng start\n",
    "    \"prev_flight_delay_in_minutes\", \n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",        # feature eng end\n",
    "    'HourlyDryBulbTemperature',     # weather start\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed'               # weather end\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff4688-854e-4a7f-9429-b74fdc925f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde39db-852d-4ce5-882d-a3f7f7cf3f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'                   # weather end\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70368be9-d8ed-4557-8841-52d5e1751acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be653215-8965-4f23-bc26-6db14cc0b802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# --- Model Estimators ---\n",
    "preprocessing_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    assembler \n",
    "]\n",
    "\n",
    "# A. XGBoost Regressor\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=2, \n",
    ")\n",
    "\n",
    "# # B. Random Forest Regressor\n",
    "# rf = RandomForestRegressor(\n",
    "#     featuresCol=\"features\",\n",
    "#     labelCol=\"DEP_DELAY_NEW\"\n",
    "# )\n",
    "\n",
    "# # C. Linear Regression\n",
    "# lr = LinearRegression(\n",
    "#     featuresCol=\"features\",\n",
    "#     labelCol=\"DEP_DELAY_NEW\"\n",
    "# )\n",
    "\n",
    "# --- Parameter Grids ---\n",
    "# A. XGBoost Grid\n",
    "# max_depth [2, 4, 6]\n",
    "# n_estimators [10, 20, 100]\n",
    "# learning rate [0.05, 0.1, 0.3]\n",
    "#   * Had to run in chunks, running into:\n",
    "#       * Executor/Worker Instability -  OOM, heartbeat failures\n",
    "#       * Resource starvation\n",
    "#       * Syncronous job failure\n",
    "xgb_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb.max_depth, [4, 6]) \\\n",
    "    .addGrid(xgb.n_estimators, [20, 50, 100]) \\\n",
    "    .addGrid(xgb.learning_rate, [0.05, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# xgb_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(xgb.max_depth, [2, 4, 6]) \\\n",
    "#     .addGrid(xgb.n_estimators, [10, 20, 100]) \\\n",
    "#     .addGrid(xgb.learning_rate, [0.05, 0.1, 0.3]) \\\n",
    "#     .build()\n",
    "\n",
    "\n",
    "# # B. Random Forest Grid\n",
    "# rf_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(rf.numTrees, [10, 20]) \\\n",
    "#     .addGrid(rf.maxDepth, [3, 5]) \\\n",
    "#     .addGrid(rf.maxBins, [20, 32, 40]) \\\n",
    "#     .build()\n",
    "\n",
    "# # C. Linear Regression Grid (Regularization/ElasticNet)\n",
    "# # use this to test if everything links properly, but not for final model\n",
    "# lr_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "#     .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "#     .build()\n",
    "\n",
    "# --- Evaluator (Use one metric for optimization) ---\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" \n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183b0901-3ce1-479f-b620-1da5a5a5eff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# param_names = [p.name for p in xgb_grid[0].keys()]\n",
    "\n",
    "# hyperparam_xgb_df = pd.DataFrame(\n",
    "#     columns=param_names + ['train_mae', 'validation_mae', 'train_rmse', 'validation_rmse']\n",
    "# )\n",
    "\n",
    "# hyperparam_xgb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36887f53-3436-46c9-bc0e-aaca9fe12246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    \"\"\"\n",
    "    Read a specific fold from partitioned parquet data.\n",
    "    Falls back to filtering if direct partition read fails.\n",
    "    \"\"\"\n",
    "    fold_path = f\"{path}/fold_id={fold_id}/split_type={split_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Try direct partition read\n",
    "        return spark.read.parquet(fold_path)\n",
    "    except:\n",
    "        # Fallback: read all data and filter\n",
    "        print(f\"Direct read failed for fold {fold_id}, using filter method...\")\n",
    "        all_data = spark.read.parquet(path)\n",
    "        return all_data.filter(\n",
    "            (all_data.fold_id == fold_id) & \n",
    "            (all_data.split_type == split_type)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2e3116-3f08-4b5b-ac61-f64ea4f5e339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with CV for XGBoost - OPTIMIZED\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "alg = 'XGB'\n",
    "n_folds = 5  # CHANGE 1: Reduced from 10 to 5 for 2x speedup\n",
    "month_or_year = \"1_year_custom_joined\"\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",      \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"           \n",
    ")\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",      \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Store results for all hyperparameter combinations\n",
    "hyperparam_results = []\n",
    "\n",
    "# Parent run for entire hyperparameter tuning experiment\n",
    "with mlflow.start_run(run_name=\"XGB_HPTUNE_WITH_CV_1_YEAR\") as hptune_parent_run:\n",
    "    mlflow.log_param(\"algorithm\", \"XGBoost\")\n",
    "    mlflow.log_param(\"n_folds\", n_folds)\n",
    "    mlflow.log_param(\"dataset\", month_or_year)\n",
    "    mlflow.log_param(\"n_param_combinations\", len(xgb_grid))\n",
    "    \n",
    "    # Iterate through each hyperparameter combination\n",
    "    for param_idx, params_ in enumerate(xgb_grid):\n",
    "        estimator_with_params = xgb.copy(params_)\n",
    "        pipeline = Pipeline(stages=preprocessing_stages + [estimator_with_params])\n",
    "        \n",
    "        param_str = \"_\".join([f\"{p.name}_{params_[p]}\" for p in params_])\n",
    "        \n",
    "        # Child run for each hyperparameter combination\n",
    "        with mlflow.start_run(run_name=f\"params_{param_idx+1}_{param_str}\", nested=True) as param_run:\n",
    "            \n",
    "            # Log hyperparameters for this combination\n",
    "            mlflow.log_param(\"max_depth\", params_[xgb.max_depth])\n",
    "            mlflow.log_param(\"n_estimators\", params_[xgb.n_estimators])\n",
    "            mlflow.log_param(\"learning_rate\", params_[xgb.learning_rate])\n",
    "            \n",
    "            cv_results = []\n",
    "            fold_metrics = {\n",
    "                'train_mae': [], 'val_mae': [],\n",
    "                'train_rmse': [], 'val_rmse': []\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{'='*120}\")\n",
    "            print(f\"Hyperparameter Combination {param_idx+1}/{len(xgb_grid)}: {param_str}\")\n",
    "            print(f\"{'='*120}\\n\")\n",
    "            \n",
    "            # CV loop for this hyperparameter combination\n",
    "            for fold_id in range(1, n_folds + 1):\n",
    "                # Nested run for each fold (nested within the param run)\n",
    "                with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True) as fold_run:\n",
    "                    \n",
    "                    # CHANGE 2: Cache data for speed\n",
    "                    fold_train = read_specific_fold(\n",
    "                        path=f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\", \n",
    "                        fold_id=fold_id, \n",
    "                        split_type=\"train\"\n",
    "                    ).cache()\n",
    "                    \n",
    "                    fold_val = read_specific_fold(\n",
    "                        path=f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\", \n",
    "                        fold_id=fold_id, \n",
    "                        split_type=\"validation\"\n",
    "                    ).cache()\n",
    "                    \n",
    "                    # Materialize cache with single count\n",
    "                    fold_train.count()\n",
    "                    fold_val.count()\n",
    "                    \n",
    "                    print(f\"Training fold {fold_id}/{n_folds}...\")\n",
    "                    \n",
    "                    # Train model\n",
    "                    model = pipeline.fit(fold_train)\n",
    "                    \n",
    "                    # Make predictions (keeping both train and val)\n",
    "                    training_predictions = model.transform(fold_train)\n",
    "                    validation_predictions = model.transform(fold_val)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    mae_t = mae_evaluator.evaluate(training_predictions)\n",
    "                    mae_v = mae_evaluator.evaluate(validation_predictions)\n",
    "                    rmse_t = rmse_evaluator.evaluate(training_predictions)\n",
    "                    rmse_v = rmse_evaluator.evaluate(validation_predictions)\n",
    "                    \n",
    "                    fold_metrics['train_mae'].append(mae_t)\n",
    "                    fold_metrics['val_mae'].append(mae_v)\n",
    "                    fold_metrics['train_rmse'].append(rmse_t)\n",
    "                    fold_metrics['val_rmse'].append(rmse_v)\n",
    "                    \n",
    "                    # Log to fold run\n",
    "                    mlflow.log_metrics({\n",
    "                        \"train_mae\": mae_t,\n",
    "                        \"val_mae\": mae_v,\n",
    "                        \"train_rmse\": rmse_t,\n",
    "                        \"val_rmse\": rmse_v,\n",
    "                    })\n",
    "                    \n",
    "                    cv_results.append({\n",
    "                        'fold': fold_id,\n",
    "                        'train_mae': mae_t,\n",
    "                        'val_mae': mae_v,\n",
    "                        'train_rmse': rmse_t,\n",
    "                        'val_rmse': rmse_v\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Fold {fold_id} - Train MAE: {mae_t:.4f}, Val MAE: {mae_v:.4f}\")\n",
    "                    \n",
    "                    # CHANGE 3: Unpersist cache after fold completes\n",
    "                    fold_train.unpersist()\n",
    "                    fold_val.unpersist()\n",
    "                \n",
    "                # Log fold metrics to param run (after fold run closes)\n",
    "                mlflow.log_metrics({\n",
    "                    f\"fold_{fold_id}_train_mae\": mae_t,\n",
    "                    f\"fold_{fold_id}_val_mae\": mae_v,\n",
    "                    f\"fold_{fold_id}_train_rmse\": rmse_t,\n",
    "                    f\"fold_{fold_id}_val_rmse\": rmse_v,\n",
    "                })\n",
    "            \n",
    "            # Calculate and log aggregated CV metrics for this param combination\n",
    "            avg_metrics = {\n",
    "                \"avg_train_mae\": np.mean(fold_metrics['train_mae']),\n",
    "                \"avg_val_mae\": np.mean(fold_metrics['val_mae']),\n",
    "                \"std_val_mae\": np.std(fold_metrics['val_mae']),\n",
    "                \"avg_train_rmse\": np.mean(fold_metrics['train_rmse']),\n",
    "                \"avg_val_rmse\": np.mean(fold_metrics['val_rmse']),\n",
    "                \"std_val_rmse\": np.std(fold_metrics['val_rmse'])\n",
    "            }\n",
    "            mlflow.log_metrics(avg_metrics)\n",
    "            \n",
    "            # Log CV results table\n",
    "            results_df = pd.DataFrame(cv_results)\n",
    "            mlflow.log_table(data=results_df, artifact_file=\"cv_fold_results.json\")\n",
    "            \n",
    "            # Store results for comparison across all param combinations\n",
    "            hyperparam_results.append({\n",
    "                'param_idx': param_idx + 1,\n",
    "                'max_depth': params_[xgb.max_depth],\n",
    "                'n_estimators': params_[xgb.n_estimators],\n",
    "                'learning_rate': params_[xgb.learning_rate],\n",
    "                'avg_train_mae': avg_metrics['avg_train_mae'],\n",
    "                'avg_val_mae': avg_metrics['avg_val_mae'],\n",
    "                'std_val_mae': avg_metrics['std_val_mae'],\n",
    "                'avg_train_rmse': avg_metrics['avg_train_rmse'],\n",
    "                'avg_val_rmse': avg_metrics['avg_val_rmse'],\n",
    "                'std_val_rmse': avg_metrics['std_val_rmse']\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nParam Combo {param_idx+1} Complete - Avg Val MAE: {avg_metrics['avg_val_mae']:.4f} ± {avg_metrics['std_val_mae']:.4f}\")\n",
    "            print(f\"{'='*120}\\n\")\n",
    "    \n",
    "    # Log summary of all hyperparameter combinations\n",
    "    hyperparam_df = pd.DataFrame(hyperparam_results)\n",
    "    mlflow.log_table(data=hyperparam_df, artifact_file=\"hyperparam_comparison.json\")\n",
    "    \n",
    "    # Find and log best parameters\n",
    "    best_idx = hyperparam_df['avg_val_mae'].idxmin()\n",
    "    best_params = hyperparam_df.iloc[best_idx]\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        \"best_avg_val_mae\": best_params['avg_val_mae'],\n",
    "        \"best_std_val_mae\": best_params['std_val_mae'],\n",
    "    })\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"best_max_depth\": best_params['max_depth'],\n",
    "        \"best_n_estimators\": best_params['n_estimators'],\n",
    "        \"best_learning_rate\": best_params['learning_rate'],\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "    print(\"=\"*120)\n",
    "    print(\"\\nAll Parameter Combinations:\")\n",
    "    print(hyperparam_df.to_string(index=False))\n",
    "    print(f\"\\nBest Parameters (by Val MAE):\")\n",
    "    print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "    print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "    print(f\"  learning_rate: {best_params['learning_rate']}\")\n",
    "    print(f\"  Avg Val MAE: {best_params['avg_val_mae']:.4f} ± {best_params['std_val_mae']:.4f}\")\n",
    "    print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59bc036-2de1-46fd-9d01-bcd2cd04dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hyperparam_xgb_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4093368127199383,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "throwaway-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
