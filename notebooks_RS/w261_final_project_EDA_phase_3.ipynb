{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569ad7dd-f594-41bb-b4ab-d832f29fcfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860d1e18-8843-40ec-9b06-baca76a0b902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "import pyspark.sql.functions as F #(func)\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "print(\"Welcome to the W261 final project!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af9a84b-45f0-4b2a-a66e-2b33e2512a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Know your mount\n",
    "Here is the mounting for this class, your source for the original data! Remember, you only have Read access, not Write! Also, become familiar with `dbutils` the equivalent of `gcp` in DataProc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea5cf50-f7c0-415d-9894-734ac509088c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0093204d-cd35-44ef-a4e2-e3d2c1ee9e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7628f4ac-4467-4fbd-a8f1-ba6d1c30e6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e31cc1e-bd39-4cfa-9a39-353dbd47c8c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data for the Project\n",
    "\n",
    "For the project you will have 4 sources of data:\n",
    "\n",
    "1. Airlines Data: This is the raw data of flights information. You have 3 months, 6 months, 1 year, and full data from 2015 to 2019. Remember the maxima: \"Test, Test, Test\", so a lot of testing in smaller samples before scaling up! Location of the data? `dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/`, `dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_1y/`, etc. (Below the dbutils to get the folders)\n",
    "2. Weather Data: Raw data for weather information. Same as before, we are sharing 3 months, 6 months, 1 year\n",
    "3. Stations data: Extra information of the location of the different weather stations. Location `dbfs:/mnt/mids-w261/datasets_final_project_2022/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/`\n",
    "4. OTPW Data: This is our joined data (We joined Airlines and Weather). This is the main dataset for your project, the previous 3 are given for reference. You can attempt your own join for Extra Credit. Location `dbfs:/mnt/mids-w261/OTPW_60M/OTPW_60M/` and more, several samples are given!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617ca3ec-3007-449c-a6fc-ec0c1a007d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/student-groups/\"))\n",
    "\n",
    "# display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/\"))      # individual datasets by time increments\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/OTPW_1D_CSV/OTPW_1D_CSV/\"))      # combined datasets\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/\"))      # broken out by each year\n",
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2015/\"))      \n",
    "# display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/\"))      # broken out by each year\n",
    "# display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_3m/\"))\n",
    "\n",
    "# display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/\"))      # one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d273895-e22a-44b7-8554-752f913214c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_dir_size_gb(path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    total_bytes = sum([f.size for f in files])\n",
    "    return total_bytes / (1024 ** 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dec0ceb-20e0-4427-bf5c-bb4e7673a8b2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765145363009}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airline Data    \n",
    "df_flights_5y = spark.read.parquet(\n",
    "    f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2015/\",\n",
    "    f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2016/\",\n",
    "    f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2017/\",\n",
    "    f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2018/\",\n",
    "    f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2019/\"\n",
    ")\n",
    "print(f\"Approximate size in GB: {get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2015/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2016/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2017/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2018/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/YEAR=2019/')}\")\n",
    "display(df_flights_5y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32264b0-79ba-422e-b20c-7149ab664d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather data\n",
    "df_weather_5y = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2015/\",\n",
    "                                   f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2016/\",\n",
    "                                   f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2017/\",\n",
    "                                   f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2018\",\n",
    "                                   f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2019/\")\n",
    "print(f\"Approximate size in GB: {get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2015/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2016/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2017/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2018/') + get_dir_size_gb('dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/YEAR=2019/')}\")\n",
    "print(df_weather_5y.count(), len(df_weather_5y.columns))\n",
    "display(df_weather_5y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ffeee8-8755-4fe3-b63e-7dcf755cb347",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761858989859}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stations data      \n",
    "df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")\n",
    "print(f\"Approximate size in GB: {get_dir_size_gb(\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")}\")\n",
    "display(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acb9811-0b6d-4413-bcd6-26b002473c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OTPW\n",
    "df_otpw = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/OTPW_3M_2015.csv\")\n",
    "print(f\"Approximate size in GB: {get_dir_size_gb(\"dbfs:/mnt/mids-w261/OTPW_3M_2015.csv\")}\")\n",
    "display(df_otpw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51075e4a-4e1d-4470-a990-188917f55b47",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761858970342}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1761859268710}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airport Codes\n",
    "df_airports = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/airport-codes_csv.csv\")\n",
    "print(f\"Approximate size in GB: {get_dir_size_gb(\"dbfs:/mnt/mids-w261/airport-codes_csv.csv\")}\")\n",
    "display(df_airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c5840c7-8997-452c-b25a-88d120fa703c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8fcda6-13ce-4f2d-a0a8-6a3ee80ba41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read as csv, write as parquet, read back as parquet to do analysis\n",
    "\n",
    "# %pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d71df13f-eeca-4723-b121-5e935e6c5302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Airports EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4bc96a-6590-4352-8655-1c21b2f45488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airports = df_airports.cache()\n",
    "\n",
    "print(f\"Number of rows: {df_airports.count()}\")\n",
    "print(f\"Number of columns: {len(df_airports.columns)}\")\n",
    "\n",
    "display(df_airports.limit(10))\n",
    "\n",
    "display(df_airports.describe())\n",
    "\n",
    "# convert elevation_ft to float\n",
    "df_airports = df_airports.withColumn(\"elevation_ft\", df_airports[\"elevation_ft\"].cast(\"float\")) \n",
    "\n",
    "sum_stats = [\n",
    "    (c, \n",
    "     df_airports.select(c).count(),\n",
    "     df_airports.select(c).distinct().count(),\n",
    "     df_airports.filter(F.col(c).isNull()).count(),\n",
    "     df_airports.agg(F.mean(c)).collect()[0][0],\n",
    "     df_airports.agg(F.stddev(c)).collect()[0][0],\n",
    "     df_airports.agg(F.min(c)).collect()[0][0],\n",
    "     df_airports.agg(F.max(c)).collect()[0][0]\n",
    "     ) for c in df_airports.columns\n",
    "]\n",
    "display(\n",
    "    spark.createDataFrame(\n",
    "        sum_stats,\n",
    "        [\"column\", \"count\", \"unique_count\", \"null_count\", \"mean\", \"stddev\", \"min\", \"max\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05f763a-e6a7-46b1-ae6f-964504a0734d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# count of nulls, visualized\n",
    "columns = [stat[0] for stat in sum_stats]\n",
    "null_counts = [stat[3] for stat in sum_stats]\n",
    "sorted_data = sorted(zip(columns, null_counts), key=lambda x: x[1], reverse=True)\n",
    "sorted_columns, sorted_null_counts = zip(*sorted_data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_columns, sorted_null_counts)\n",
    "plt.title(\"Number of Nulls per Feature in df_airports\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Null Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# different types of airports\n",
    "type_counts = df_airports.groupBy(\"type\").count().collect()\n",
    "display(type_counts)\n",
    "types = [row['type'] for row in type_counts]\n",
    "counts = [row['count'] for row in type_counts]\n",
    "sorted_data = sorted(zip(types, counts), key=lambda x: x[1], reverse=True)\n",
    "sorted_columns, sorted_counts = zip(*sorted_data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_columns, sorted_counts)\n",
    "plt.title(\"Counts per Airport Type in df_airports\")\n",
    "plt.xlabel(\"Airport Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a06d8b00-b68f-4dce-8fd2-5d334ce2a421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Flights EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d23bb1-9538-42f9-9e6a-7af9f458cde0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_5y = df_flights_5y.dropDuplicates().cache()\n",
    "\n",
    "print(f\"Number of rows: {df_flights_5y.count()}\")\n",
    "print(f\"Number of columns: {len(df_flights_5y.columns)}\")\n",
    "\n",
    "# display(df_flights_1y.limit(10))\n",
    "\n",
    "df_flights_5y = df_flights_5y.withColumn(\n",
    "    \"FL_ID\",\n",
    "    F.concat_ws(\"_\",\n",
    "        F.col(\"FL_DATE\").cast(\"string\"),\n",
    "        F.col(\"OP_CARRIER\").cast(\"string\"),\n",
    "        F.col(\"OP_CARRIER_FL_NUM\").cast(\"string\"),\n",
    "        F.col(\"ORIGIN_AIRPORT_ID\").cast(\"string\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_flights_5y = df_flights_5y.withColumn(\"FL_ID_HASH\", F.hash(F.col(\"FL_ID\")))\n",
    "\n",
    "display(df_flights_5y.limit(10))\n",
    "display(df_flights_5y.describe())\n",
    "\n",
    "agg_exprs = []\n",
    "for c in df_flights_5y.columns:\n",
    "    agg_exprs.extend([\n",
    "        F.count(c).alias(f\"{c}_count\"),\n",
    "        F.countDistinct(c).alias(f\"{c}_distinct\"),\n",
    "        F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(f\"{c}_nulls\")\n",
    "    ])\n",
    "stats_row = df_flights_5y.agg(*agg_exprs).collect()[0]\n",
    "sum_stats_1y = [\n",
    "    (c, stats_row[f\"{c}_count\"], stats_row[f\"{c}_distinct\"], stats_row[f\"{c}_nulls\"])\n",
    "    for c in df_flights_5y.columns\n",
    "]\n",
    "display(\n",
    "    spark.createDataFrame(\n",
    "        sum_stats_1y,\n",
    "        [\"column\", \"non_null_count\", \"unique_count\", \"null_count\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "columns = [stat[0] for stat in sum_stats_1y]\n",
    "# null_counts = [stat[3] for stat in sum_stats]\n",
    "null_percentages = [(stat[3] / (stat[1]+stat[3])) * 100 for stat in sum_stats_1y]\n",
    "sorted_data = sorted(zip(columns, null_percentages), key=lambda x: x[1], reverse=True)\n",
    "sorted_columns, sorted_null_percentages = zip(*sorted_data)\n",
    "plt.figure(figsize=(12, 20))  # Taller figure for horizontal bars\n",
    "plt.barh(sorted_columns, sorted_null_percentages)\n",
    "plt.title(\"% Nulls per Feature in df_flights\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Null %\", fontsize=12)\n",
    "plt.ylabel(\"Features\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(df_flights_5y.groupBy(\"OP_CARRIER\").count().orderBy(\"count\", ascending=False))\n",
    "# time series of flights per day, maybe split out by carrier\n",
    "carrier_flights = df_flights_5y.groupBy([\"FL_DATE\", \"OP_CARRIER\"]).count().orderBy(\"FL_DATE\").toPandas()\n",
    "display(carrier_flights)\n",
    "carrier_flights['FL_DATE'] = pd.to_datetime(carrier_flights['FL_DATE'])\n",
    "carrier_pivot = carrier_flights.pivot(index='FL_DATE', columns='OP_CARRIER', values='count').fillna(0)\n",
    "carrier_totals = carrier_pivot.sum().sort_values(ascending=False)\n",
    "carrier_pivot = carrier_pivot[carrier_totals.index]\n",
    "n_carriers = len(carrier_pivot.columns)\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, n_carriers))\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.stackplot(carrier_pivot.index, \n",
    "              *[carrier_pivot[col] for col in carrier_pivot.columns],\n",
    "              labels=carrier_pivot.columns,\n",
    "              colors=colors,\n",
    "              alpha=0.8)\n",
    "plt.title(\"Daily Flight Counts by Carrier (Stacked by Volume)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Flights\", fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "display(df_flights_5y.groupBy(\"DEP_DEL15\").count())\n",
    "display(df_flights_5y.groupBy(\"ARR_DEL15\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fceb1ad-8e6d-4e48-b9ac-cd6de083fbd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_airports.filter((F.col(\"type\") == \"large_airport\") & (F.col(\"iso_country\") == \"US\")))\n",
    "df_us_airports = df_airports.filter((F.col(\"iso_country\") == \"US\"))\n",
    "\n",
    "# join df_flights and df_airports on ORIGIN and local_code\n",
    "df_flights_5y_joined = df_flights_5y.join(df_us_airports, df_flights_5y.ORIGIN == df_us_airports.local_code, \"inner\")\n",
    "display(df_flights_5y_joined.limit(10))\n",
    "\n",
    "# visualize number of flights by TYPE (airport type) and DELAYED (DEP_DEL15)\n",
    "display(\n",
    "    df_flights_5y_joined.groupBy(\"type\", \"DEP_DEL15\").count().orderBy(\"type\", \"DEP_DEL15\")\n",
    ")\n",
    "airport_types_delay = df_flights_5y_joined.groupBy(\"type\", \"DEP_DEL15\").count().orderBy(\"type\", \"DEP_DEL15\").toPandas()\n",
    "plt.figure(figsize=(12, 6))\n",
    "for delayed, color in zip([0, 1], ['skyblue', 'orange']):\n",
    "    subset = airport_types_delay[airport_types_delay['DEP_DEL15'] == delayed]\n",
    "    plt.bar(subset['type'], subset['count'], label=f'Delayed={delayed}', color=color, alpha=0.7)\n",
    "plt.title(\"Number of Flights by Airport Type and Delay Status (1Y)\")\n",
    "plt.xlabel(\"Airport Type\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# visualize number of flights by airport\n",
    "display(df_flights_5y_joined.groupBy(\"ORIGIN\").count().orderBy(\"count\", ascending=False))\n",
    "airport_counts = df_flights_5y_joined.groupBy(\"ORIGIN\").count().orderBy(\"count\", ascending=False).toPandas()\n",
    "# side by side of all airports and top 10 airports\n",
    "plt.figure(figsize=(18, 6))\n",
    "# All airports\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(airport_counts['ORIGIN'], airport_counts['count'], color='skyblue')\n",
    "plt.title(\"Number of Flights by Airport (1Y) - All Airports\")\n",
    "plt.xlabel(\"Airport\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "# Top 10 airports\n",
    "plt.subplot(1, 2, 2)\n",
    "top10 = airport_counts.head(10)\n",
    "plt.bar(top10['ORIGIN'], top10['count'], color='orange')\n",
    "plt.title(\"Number of Flights by Airport (1Y) - Top 10 Airports\")\n",
    "plt.xlabel(\"Airport\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c0be4ec-0e91-40af-9e12-db14b0c8167f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Weather EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25f4f0a-6577-454d-ba7c-8c1c76f7815c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765172131116}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "us_stations = df_stations \\\n",
    "    .filter(\"distance_to_neighbor == 0\") \\\n",
    "    .select([\"station_id\", \"neighbor_name\", \"neighbor_state\"])\n",
    "\n",
    "joined_test = df_weather_5y.join(us_stations, df_weather_5y.STATION==us_stations.station_id, how='inner').drop(\"station_id\", \"neighbor_name\", \"neighbor_state\")\n",
    "df_weather_5y = joined_test.cache()\n",
    "\n",
    "print(f\"Number of rows: {df_weather_5y.count()}\")\n",
    "print(f\"Number of columns: {len(df_weather_5y.columns)}\")\n",
    "\n",
    "display(df_weather_5y.limit(10))\n",
    "\n",
    "display(df_weather_5y.describe())\n",
    "\n",
    "agg_exprs = []\n",
    "for c in df_weather_5y.columns:\n",
    "    agg_exprs.extend([\n",
    "        F.count(c).alias(f\"{c}_count\"),\n",
    "        F.countDistinct(c).alias(f\"{c}_distinct\"),\n",
    "        F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(f\"{c}_nulls\")\n",
    "    ])\n",
    "stats_row = df_weather_5y.agg(*agg_exprs).collect()[0]\n",
    "sum_stats = [\n",
    "    (c, stats_row[f\"{c}_count\"], stats_row[f\"{c}_distinct\"], stats_row[f\"{c}_nulls\"])\n",
    "    for c in df_weather_5y.columns\n",
    "]\n",
    "display(\n",
    "    spark.createDataFrame(\n",
    "        sum_stats,\n",
    "        [\"column\", \"non_null_count\", \"unique_count\", \"null_count\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65b9a89-f937-4dea-b47b-bdd54c167ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = [stat[0] for stat in sum_stats]\n",
    "# null_counts = [stat[3] for stat in sum_stats]\n",
    "null_percentages = [(stat[3] / (stat[1]+stat[3])) * 100 for stat in sum_stats]\n",
    "sorted_data = sorted(zip(columns, null_percentages), key=lambda x: x[1], reverse=True)\n",
    "sorted_columns, sorted_null_percentages = zip(*sorted_data)\n",
    "plt.figure(figsize=(12, 20))  # Taller figure for horizontal bars\n",
    "plt.barh(sorted_columns, sorted_null_percentages)\n",
    "plt.title(\"% Nulls per Feature in df_weather\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Null %\", fontsize=12)\n",
    "plt.ylabel(\"Features\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "null_pcts = [(stat[3] / (stat[1]+stat[3])) * 100 for stat in sum_stats]\n",
    "plt.hist(null_pcts, bins=30, edgecolor='black', color='skyblue')\n",
    "plt.title(\"Distribution of Null Percentages Across All Features\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Null Percentage (%)\")\n",
    "plt.ylabel(\"Number of Features\")\n",
    "plt.axvline(x=np.median(null_pcts), color='red', linestyle='--', label=f'Median: {np.median(null_pcts):.1f}%')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Features with >50% nulls: {sum(1 for p in null_pcts if p > 50)}\")\n",
    "print(f\"Features with 0% nulls: {sum(1 for p in null_pcts if p == 0)}\")\n",
    "\n",
    "\n",
    "# time series of weather reports per day ----- we should just look at the weather report for each hour, that's where the spikes are and when we have the most data\n",
    "weather_reports = df_weather_5y.groupBy(\"DATE\").count().orderBy(\"DATE\").toPandas()\n",
    "display(weather_reports)\n",
    "weather_reports['DATE'] = pd.to_datetime(weather_reports['DATE'])\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(weather_reports['DATE'], weather_reports['count'], linewidth=2, color='steelblue')\n",
    "plt.title(\"Daily Weather Reports Over Time\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Weather Reports\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Optional: Print some summary stats\n",
    "print(f\"Total days: {len(weather_reports)}\")\n",
    "print(f\"Average reports per day: {weather_reports['count'].mean():.2f}\")\n",
    "print(f\"Max reports in a day: {weather_reports['count'].max()}\")\n",
    "print(f\"Min reports in a day: {weather_reports['count'].min()}\")\n",
    "\n",
    "\n",
    "# station, count number of distinct weather reports (dates)\n",
    "num_reports = df_weather_5y.groupBy(\"STATION\").agg(F.countDistinct(\"DATE\").alias(\"DATE\")).toPandas()\n",
    "display(num_reports)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(num_reports['DATE'], bins=50, edgecolor='black', color='skyblue')\n",
    "plt.title(\"Distribution of Number of Weather Reports per Station\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Number of Weather Reports\", fontsize=12)\n",
    "plt.ylabel(\"Number of Stations\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f44d7e-42a9-47ce-885f-a0d5d70aaa1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stations EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831b874f-f520-43e7-9565-e2dee8b68519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stations = df_stations.cache()\n",
    "\n",
    "print(f\"Number of rows: {df_stations.count()}\")\n",
    "print(f\"Number of columns: {len(df_stations.columns)}\")\n",
    "\n",
    "display(df_stations.limit(10))\n",
    "\n",
    "display(df_stations.describe())\n",
    "\n",
    "sum_stats = [\n",
    "    (c, \n",
    "     df_stations.select(c).count(),\n",
    "     df_stations.select(c).distinct().count(),\n",
    "     df_stations.filter(F.col(c).isNull()).count(),\n",
    "     df_stations.agg(F.mean(c)).collect()[0][0],\n",
    "     df_stations.agg(F.stddev(c)).collect()[0][0],\n",
    "     df_stations.agg(F.min(c)).collect()[0][0],\n",
    "     df_stations.agg(F.max(c)).collect()[0][0]\n",
    "     ) for c in df_stations.columns\n",
    "]\n",
    "display(\n",
    "    spark.createDataFrame(\n",
    "        sum_stats,\n",
    "        [\"column\", \"count\", \"unique_count\", \"null_count\", \"mean\", \"stddev\", \"min\", \"max\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "columns = [stat[0] for stat in sum_stats]\n",
    "null_counts = [stat[3] for stat in sum_stats]\n",
    "# null_percentages = [(stat[3] / stat[1]) * 100 for stat in sum_stats]\n",
    "sorted_data = sorted(zip(columns, null_counts), key=lambda x: x[1], reverse=True)\n",
    "sorted_columns, sorted_null_counts = zip(*sorted_data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_columns, sorted_null_counts)\n",
    "plt.title(\"Number of Nulls per Feature in df_stations\")\n",
    "plt.ylabel(\"Null Count\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# station_id, count number of distinct neighbors\n",
    "num_neighbors = df_stations.groupBy(\"station_id\").agg(F.countDistinct(\"neighbor_id\").alias(\"num_neighbors\")).toPandas()\n",
    "display(num_neighbors)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(num_neighbors['num_neighbors'], bins=50, edgecolor='black', color='skyblue')\n",
    "plt.title(\"Distribution of Number of Neighbors per Station\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Number of Neighbors\", fontsize=12)\n",
    "plt.ylabel(\"Number of Stations\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "188a5fa7-c98a-4c70-8053-13465ee10346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom Joined Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6e9d59-d905-437d-a3e2-e2c44dcd52c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "custom_join_5y_path ='dbfs:/student-groups/Group_2_2/5_year_custom_joined'\n",
    "\n",
    "\n",
    "join_data_5y = spark.read.parquet(custom_join_5y_path)\n",
    "\n",
    "# drop null flight_uid\n",
    "join_data_5y_df = join_data_5y.dropna(subset=['flight_uid'])\n",
    "display(join_data_5y_df)\n",
    "\n",
    "# check 1y dataset\n",
    "print(f'rows: {join_data_5y_df.count()}')\n",
    "print(f'cols: {len(join_data_5y_df.columns)}\\n')\n",
    "\n",
    "join_data_5y_df.printSchema()\n",
    "\n",
    "# df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "# print(df.count())\n",
    "# display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f27d510-bfde-4cb8-9301-2acc29088539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_5y_df = join_data_5y_df.dropDuplicates().cache()\n",
    "\n",
    "join_data_5y_df = join_data_5y_df.withColumn(\n",
    "    \"FL_ID\",\n",
    "    F.concat_ws(\"_\",\n",
    "        F.col(\"FL_DATE\").cast(\"string\"),\n",
    "        F.col(\"OP_CARRIER\").cast(\"string\"),\n",
    "        F.col(\"OP_CARRIER_FL_NUM\").cast(\"string\"),\n",
    "        F.col(\"ORIGIN_AIRPORT_ID\").cast(\"string\")\n",
    "    )\n",
    ")\n",
    "\n",
    "join_data_5y_df = join_data_5y_df.withColumn(\"FL_ID_HASH\", F.hash(F.col(\"FL_ID\")))\n",
    "\n",
    "display(join_data_5y_df.limit(10))\n",
    "display(join_data_5y_df.describe())\n",
    "\n",
    "agg_exprs = []\n",
    "for c in join_data_5y_df.columns:\n",
    "    agg_exprs.extend([\n",
    "        F.count(c).alias(f\"{c}_count\"),\n",
    "        F.countDistinct(c).alias(f\"{c}_distinct\"),\n",
    "        F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(f\"{c}_nulls\")\n",
    "    ])\n",
    "stats_row = join_data_5y_df.agg(*agg_exprs).collect()[0]\n",
    "sum_stats_1y = [\n",
    "    (c, stats_row[f\"{c}_count\"], stats_row[f\"{c}_distinct\"], stats_row[f\"{c}_nulls\"])\n",
    "    for c in join_data_5y_df.columns\n",
    "]\n",
    "display(\n",
    "    spark.createDataFrame(\n",
    "        sum_stats_1y,\n",
    "        [\"column\", \"non_null_count\", \"unique_count\", \"null_count\"]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d8526e5-223e-4af4-9288-383d924e8ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "columns = [stat[0] for stat in sum_stats_1y]\n",
    "# null_counts = [stat[3] for stat in sum_stats]\n",
    "null_percentages = [(stat[3] / (stat[1]+stat[3])) * 100 for stat in sum_stats_1y]\n",
    "sorted_data = sorted(zip(columns, null_percentages), key=lambda x: x[1], reverse=True)\n",
    "sorted_columns, sorted_null_percentages = zip(*sorted_data)\n",
    "plt.figure(figsize=(12, 20))  # Taller figure for horizontal bars\n",
    "plt.barh(sorted_columns, sorted_null_percentages)\n",
    "plt.title(\"% Nulls per Feature in df_flights\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Null %\", fontsize=12)\n",
    "plt.ylabel(\"Features\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(join_data_5y_df.groupBy(\"OP_CARRIER\").count().orderBy(\"count\", ascending=False))\n",
    "# time series of flights per day, maybe split out by carrier\n",
    "carrier_flights = join_data_5y_df.groupBy([\"FL_DATE\", \"OP_CARRIER\"]).count().orderBy(\"FL_DATE\").toPandas()\n",
    "display(carrier_flights)\n",
    "carrier_flights['FL_DATE'] = pd.to_datetime(carrier_flights['FL_DATE'])\n",
    "carrier_pivot = carrier_flights.pivot(index='FL_DATE', columns='OP_CARRIER', values='count').fillna(0)\n",
    "carrier_totals = carrier_pivot.sum().sort_values(ascending=False)\n",
    "carrier_pivot = carrier_pivot[carrier_totals.index]\n",
    "n_carriers = len(carrier_pivot.columns)\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, n_carriers))\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.stackplot(carrier_pivot.index, \n",
    "              *[carrier_pivot[col] for col in carrier_pivot.columns],\n",
    "              labels=carrier_pivot.columns,\n",
    "              colors=colors,\n",
    "              alpha=0.8)\n",
    "plt.title(\"Daily Flight Counts by Carrier (Stacked by Volume)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Flights\", fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "display(join_data_5y_df.groupBy(\"DEP_DEL15\").count())\n",
    "display(join_data_5y_df.groupBy(\"ARR_DEL15\").count())\n",
    "\n",
    "# number of flights by airport\n",
    "display(join_data_5y_df.groupBy(\"ORIGIN\").count().orderBy(\"count\", ascending=False))\n",
    "airport_counts = join_data_5y_df.groupBy(\"ORIGIN\").count().orderBy(\"count\", ascending=False).toPandas()\n",
    "# side by side of all airports and top 10 airports\n",
    "plt.figure(figsize=(18, 6))\n",
    "# All airports\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(airport_counts['ORIGIN'], airport_counts['count'], color='skyblue')\n",
    "plt.title(\"Number of Flights by Airport - All Airports\")\n",
    "plt.xlabel(\"Airport\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "# Top 10 airports\n",
    "plt.subplot(1, 2, 2)\n",
    "top10 = airport_counts.head(10)\n",
    "plt.bar(top10['ORIGIN'], top10['count'], color='orange')\n",
    "plt.title(\"Number of Flights by Airport - Top 10 Airports\")\n",
    "plt.xlabel(\"Airport\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# number of flight delays by airport\n",
    "delayed_by_airport = join_data_5y_df.filter(F.col(\"DEP_DEL15\") == 1).groupBy(\"ORIGIN\").count().orderBy(\"count\", ascending=False)\n",
    "display(delayed_by_airport)\n",
    "delayed_airport_counts = delayed_by_airport.toPandas()\n",
    "plt.figure(figsize=(18, 6))\n",
    "# All airports\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(delayed_airport_counts['ORIGIN'], delayed_airport_counts['count'], color='salmon')\n",
    "plt.title(\"Number of Delayed Flights by Airport - All Airports\")\n",
    "plt.xlabel(\"Airport\")\n",
    "plt.ylabel(\"Number of Delayed Flights\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "# Top 10 airports\n",
    "plt.subplot(1, 2, 2)\n",
    "top10_delayed = delayed_airport_counts.head(10)\n",
    "plt.bar(top10_delayed['ORIGIN'], top10_delayed['count'], color='orange')\n",
    "plt.title(\"Number of Delayed Flights by Airport - Top 10 Airports\")\n",
    "plt.xlabel(\"Airport\")\n",
    "plt.ylabel(\"Number of Delayed Flights\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # time series of weather reports per day ----- we should just look at the weather report for each hour, that's where the spikes are and when we have the most data\n",
    "# weather_reports = join_data_1y_df.groupBy(\"DATE\").count().orderBy(\"DATE\").toPandas()\n",
    "# display(weather_reports)\n",
    "# weather_reports['DATE'] = pd.to_datetime(weather_reports['DATE'])\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.plot(weather_reports['DATE'], weather_reports['count'], linewidth=2, color='steelblue')\n",
    "# plt.title(\"Daily Weather Reports Over Time\", fontsize=14, fontweight='bold')\n",
    "# plt.xlabel(\"Date\", fontsize=12)\n",
    "# plt.ylabel(\"Number of Weather Reports\", fontsize=12)\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# # Optional: Print some summary stats\n",
    "# print(f\"Total days: {len(weather_reports)}\")\n",
    "# print(f\"Average reports per day: {weather_reports['count'].mean():.2f}\")\n",
    "# print(f\"Max reports in a day: {weather_reports['count'].max()}\")\n",
    "# print(f\"Min reports in a day: {weather_reports['count'].min()}\")\n",
    "\n",
    "\n",
    "# # station, count number of distinct weather reports (dates)\n",
    "# num_reports = df_weather_1y.groupBy(\"STATION\").agg(F.countDistinct(\"DATE\").alias(\"DATE\")).toPandas()\n",
    "# display(num_reports)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.hist(num_reports['DATE'], bins=50, edgecolor='black', color='skyblue')\n",
    "# plt.title(\"Distribution of Number of Weather Reports per Station\", fontsize=14, fontweight='bold')\n",
    "# plt.xlabel(\"Number of Weather Reports\", fontsize=12)\n",
    "# plt.ylabel(\"Number of Stations\", fontsize=12)\n",
    "# plt.grid(True, alpha=0.3, axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c60db69-8664-498d-9d4e-e2a6a4d1d04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for delayed flights, plotting a histogram of the number of flights at each time of day\n",
    "delayed_flights = join_data_5y_df.filter(F.col(\"DEP_DEL15\") == 1)\n",
    "delayed_flights_pd = delayed_flights.select(\"CRS_DEP_TIME\").toPandas()\n",
    "delayed_flights_pd['hour'] = delayed_flights_pd['CRS_DEP_TIME'] // 100\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(delayed_flights_pd['hour'], bins=range(0, 25), edgecolor='black', color='salmon', align='left')\n",
    "plt.title(\"Histogram of Delayed Flights by Scheduled Departure Hour\")\n",
    "plt.xlabel(\"Scheduled Departure Hour\")\n",
    "plt.ylabel(\"Number of Delayed Flights\")\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# for delayed flights, plotting a stacked bar chart of the number of flights at each time of day, broken down by airline\n",
    "delayed_flights = join_data_5y_df.filter(F.col(\"DEP_DEL15\") == 1)\n",
    "delayed_flights_pd = delayed_flights.select(\"CRS_DEP_TIME\", \"OP_CARRIER\").toPandas()\n",
    "delayed_flights_pd['hour'] = delayed_flights_pd['CRS_DEP_TIME'] // 100\n",
    "pivot_df = delayed_flights_pd.pivot_table(index='hour', columns='OP_CARRIER', aggfunc='size', fill_value=0)\n",
    "pivot_df = pivot_df.sort_index()\n",
    "plt.figure(figsize=(14, 8))\n",
    "bottom = None\n",
    "for carrier in pivot_df.columns:\n",
    "    plt.bar(pivot_df.index, pivot_df[carrier], bottom=bottom, label=carrier)\n",
    "    if bottom is None:\n",
    "        bottom = pivot_df[carrier].copy()\n",
    "    else:\n",
    "        bottom += pivot_df[carrier]\n",
    "plt.title(\"Stacked Bar Chart of Delayed Flights by Scheduled Departure Hour (by Airline)\")\n",
    "plt.xlabel(\"Scheduled Departure Hour\")\n",
    "plt.ylabel(\"Number of Delayed Flights\")\n",
    "plt.xticks(range(0, 24))\n",
    "plt.legend(title=\"Airline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# calculate delay rate for each airline as delayed flights for a given day divided by flight volume for a given day, take the average daily delay rate, and then plot it as a bar chart over date\n",
    "daily_flights = join_data_5y_df.groupBy(\"FL_DATE\", \"OP_CARRIER\").count().withColumnRenamed(\"count\", \"total_flights\")\n",
    "daily_delays = join_data_5y_df.filter(F.col(\"DEP_DEL15\") == 1).groupBy(\"FL_DATE\", \"OP_CARRIER\").count().withColumnRenamed(\"count\", \"delayed_flights\")\n",
    "delay_rate_df = daily_flights.join(daily_delays, [\"FL_DATE\", \"OP_CARRIER\"], \"left\").fillna(0, subset=[\"delayed_flights\"])\n",
    "delay_rate_df = delay_rate_df.withColumn(\"delay_rate\", F.col(\"delayed_flights\") / F.col(\"total_flights\"))\n",
    "delay_rate_pd = delay_rate_df.groupBy(\"FL_DATE\").agg(F.mean(\"delay_rate\").alias(\"avg_delay_rate\")).orderBy(\"FL_DATE\").toPandas()\n",
    "delay_rate_pd['FL_DATE'] = pd.to_datetime(delay_rate_pd['FL_DATE'])\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(delay_rate_pd['FL_DATE'], delay_rate_pd['avg_delay_rate'], color='darkred', width=1)\n",
    "plt.title(\"Bar Chart of Average Daily Delay Rate Across Airlines Over Time\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Average Delay Rate\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# now separated out by carrier, plot as a stacked bar chart\n",
    "delay_rate_pd = delay_rate_df.orderBy(\"FL_DATE\").toPandas()\n",
    "delay_rate_pd['FL_DATE'] = pd.to_datetime(delay_rate_pd['FL_DATE'])\n",
    "pivot_df = delay_rate_pd.pivot(index='FL_DATE', columns='OP_CARRIER', values='delay_rate').fillna(0)\n",
    "pivot_df = pivot_df.sort_index()\n",
    "plt.figure(figsize=(16, 8))\n",
    "bottom = None\n",
    "for carrier in pivot_df.columns:\n",
    "    plt.bar(pivot_df.index, pivot_df[carrier], bottom=bottom, label=carrier, width=1)\n",
    "    if bottom is None:\n",
    "        bottom = pivot_df[carrier].copy()\n",
    "    else:\n",
    "        bottom += pivot_df[carrier]\n",
    "plt.title(\"Stacked Bar Chart of Daily Delay Rate by Airline Carrier Over Time\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Delay Rate\", fontsize=12)\n",
    "plt.legend(title=\"Airline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284b1692-4a8a-4c69-9e0e-ae9afb1ecb96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# just do correlation heatmap for custom joined data\n",
    "# correlation matrix and heat map of numeric variables\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# ['DEP_DELAY', 'TAXI_OUT', 'TAXI_IN', 'ARR_DELAY', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'CARRIER_DELAY' (fillna), 'WEATHER_DELAY' (fillna), 'NAS_DELAY' (fillna), 'SECURITY_DELAY' (fillna), 'LATE_AIRCRAFT_DELAY' (fillna), 'HourlyDryBulbTemperature', 'HourlyDewPointTemperature', 'HourlyRelativeHumidity', 'HourlyAltimeterSetting', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWetBulbTemperature', 'HourlyPrecipitation', 'HourlyCloudCoverage', 'HourlyCloudElevation', 'HourlyWindSpeed']\n",
    "cols_to_keep = ['DEP_DELAY', 'TAXI_OUT', 'TAXI_IN', 'ARR_DELAY', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'HourlyDryBulbTemperature', 'HourlyDewPointTemperature', 'HourlyRelativeHumidity', 'HourlyAltimeterSetting', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWetBulbTemperature', 'HourlyPrecipitation', 'HourlyCloudCoverage', 'HourlyCloudElevation', 'HourlyWindSpeed']\n",
    "\n",
    "# fill na for certain columns\n",
    "join_data_5y_df = join_data_5y_df.fillna(0, subset=['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols_to_keep, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "df_numeric = assembler.transform(join_data_5y_df.select(cols_to_keep)).select(\"features\")\n",
    "corr_matrix_row = Correlation.corr(df_numeric, \"features\", \"pearson\").collect()[0][0]\n",
    "corr_matrix = corr_matrix_row.toArray().reshape(len(cols_to_keep), len(cols_to_keep))\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", xticklabels=cols_to_keep, yticklabels=cols_to_keep)\n",
    "plt.title(\"Correlation Matrix (Numeric Features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74d4834-d506-4181-a590-ad13f07768dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Markdown\n",
    "extra EDA:\n",
    "- correlation of different features (heatmap)\n",
    "- delay propagation throughout the day (delay rate by hour of day) - broken down by airport or by carrier\n",
    "- delay rate vs flight volume (by airport and by carrier)\n",
    "\n",
    "notes from Vini:\n",
    "- 19 min, cut to 14 min max\n",
    "- we didn't do classification just yet, so be careful when including\n",
    "    - we need to discuss the metrics here as well (eval each model?, eval on the whole?); how do we eval our final model that we use is a good one?\n",
    "- EDA\n",
    "    - need to explicitly specify where we are getting our data from (add link for data sources) - call it Data Sources\n",
    "    - make number of nulls a table rather than a chart\n",
    "    - no overlapping graphs, can split into multiple slides and say the same amount\n",
    "    - be clear on the time frame (3 months, 6 months) - just specify\n",
    "    - for custom join, explain that we are not dropping features until we do the custom join since then we can do the imputation\n",
    "- custom join\n",
    "    - make clear how many columns survive (have few enough nulls) after custom join \n",
    "- baseline model\n",
    "    - how do we handle imbalance on the second model? maybe we'll want to just have 15-min delay and 30+\n",
    "    - why are we using gap? so val can't learn from train\n",
    "    - change our outcome to be log(1+departure delay)\n",
    "- discuss feature engineering before we talk about model\n",
    "- training results\n",
    "    - can we put arrows in percentage instead of absolute numbers?\n",
    "    - make sure we are making our decisions on at least 1 year of data\n",
    "    - maybe holidays can be another feature?\n",
    "- do error analysis once we're sure we have the best model (should be our ending)\n",
    "- put pipeline much earlier in the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cacdbb3-6638-4e83-a594-a90de878297a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "a32264b0-79ba-422e-b20c-7149ab664d86",
       "elementType": "command",
       "guid": "63922e80-69bb-448f-998e-2b96018aa772",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "848a23e1-4a38-4b97-b84c-de53113ea481",
     "origId": 4093368127198739,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "w261_final_project_EDA_phase_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
