{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d470617-592c-474d-986a-b7b9557e9aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca31ebf3-241b-4b34-b298-84e3f1c46a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom Join Dataset - 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ab9809-79af-40a1-9adb-ce2273026d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b669106-ecd2-494e-8477-d8503cf43d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in custom joined data\n",
    "custom_joined_path = 'dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V3/custom_join_v3_1y.parquet'\n",
    "\n",
    "df = spark.read.parquet(custom_joined_path)\n",
    "\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "print(df.count())\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "924d73f8-797f-419e-a4c6-a2973ba7b1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Things to keep in mind\n",
    "- Predict two hours before\n",
    "- Remove all the delay columns\n",
    "- Are we only predicting departure delays or arrival delays also? For example, the pilot misses the landing, and has to circle back for 20 minutes. Should we solve for that? I don't think we should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d49ea99-57cc-4ba9-b438-7386aa04e939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessing / Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5999fa0d-91b2-4e26-a0ff-5133dd88042c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.cache() # cache joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8d68c2-a1d0-487b-8d7f-8fb65a48a2cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine date and scheduled departure time\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe271d1e-f7a8-4575-840c-5873bd34aaa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Split 3 month joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fd7fc9-335e-4473-9f1f-a0ffdbbab4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "TRAIN_SIZE = 0.70\n",
    "VALIDATION_SIZE = 0.10\n",
    "\n",
    "# REMOVE ALL CANCELLED FLIGHTS\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "\n",
    "df = df.sort('utc_timestamp')\n",
    "\n",
    "# Add row number based on timestamp order\n",
    "window = Window.orderBy('utc_timestamp')\n",
    "df = df.withColumn(\"row_num\", F.row_number().over(window))\n",
    "\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate split points\n",
    "train_end = int(total_rows * TRAIN_SIZE)\n",
    "validation_end = int(total_rows * (TRAIN_SIZE + VALIDATION_SIZE))  # 70% + 10%\n",
    "\n",
    "# Split based on row number\n",
    "train_df = df.filter(F.col(\"row_num\") <= train_end)\n",
    "validation_df = df.filter((F.col(\"row_num\") > train_end) & (F.col(\"row_num\") <= validation_end))\n",
    "test_df = df.filter(F.col(\"row_num\") > validation_end)\n",
    "\n",
    "# Drop the helper column\n",
    "train_df = train_df.drop(\"row_num\")\n",
    "validation_df = validation_df.drop(\"row_num\")\n",
    "test_df = test_df.drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f6c1ba-2796-4811-a921-6afd8ad45615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the last utc_timestamp from train_df\n",
    "last_flight_ts = train_df.agg(F.max(\"utc_timestamp\").alias(\"last_ts\")).collect()[0][\"last_ts\"]\n",
    "\n",
    "# Add a 2 hour gap\n",
    "gap_ts = F.timestamp_add(\"HOUR\", F.lit(2), F.lit(last_flight_ts))\n",
    "\n",
    "# Filter validation_df to keep everything after the gap timestamp\n",
    "# validation_after_gap_df = validation_df.filter(F.col(\"utc_timestamp\") > gap_ts)\n",
    "validation_df = validation_df.filter(F.col(\"utc_timestamp\") > gap_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b028b59-602f-406d-9192-de9224ef8674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4721fde3-e6e7-49b5-b0a0-e04b625d2010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if input(\"CAREFUL: You're about to write to DBFS. Type 'y' to continue.\") == \"y\":\n",
    "    checkpoint_dataset(train_df, \"1_year_custom_joined/raw_data/training_splits/train\")\n",
    "    checkpoint_dataset(validation_df, \"1_year_custom_joined/raw_data/training_splits/validation\")\n",
    "    checkpoint_dataset(test_df, \"1_year_custom_joined/raw_data/training_splits/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33656b4d-4319-4f22-a283-0326d462b666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### check checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7428a617-73d4-4405-b82c-7187c50bb863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/raw_data/training_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "772c5ce8-150f-4366-acb8-823a80ae2293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d6f9c3-47ea-4ff0-8ed4-67f891e00385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "\n",
    "dataset_path = f\"{checkpoint_path}/{month_or_year}/raw_data/training_splits\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")\n",
    "test_df = spark.read.parquet(f\"{dataset_path}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81511ba8-3f44-4128-bc4c-36477eb9b97e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validation_df.columns == test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428f769d-c4ca-4304-bfd0-a96f56642385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in check_train_df.columns:\n",
    "    if col not in check_test_df.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3aea631-5944-45ce-9096-2a1cacc346a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ignore weather rows with nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f39192-106c-4853-8367-a1d0a83404e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])\n",
    "\n",
    "validation_df = validation_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])\n",
    "\n",
    "test_df = test_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9ad4ff-0453-41cc-8c32-577b10bcdcdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## CRS_DEP_TIME is local time so we can use this feature \n",
    "## But in order to use it, we have to convert it to minutes since midnight\n",
    "## Otherwise the timing will be off b/c it's not true UTC\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"CRS_DEP_MINUTES\", \n",
    "    (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    \"CRS_DEP_MINUTES\", \n",
    "    (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"CRS_DEP_MINUTES\", \n",
    "    (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b48fba-396a-4a0f-9280-78250938628d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Eng.\n",
    "\n",
    "#### Was the previous flight delayed? And by how much was the previous flight delayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96047694-161f-431a-9fdf-f9a11b3618eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.cache()\n",
    "validation_df = validation_df.cache()\n",
    "test_df = test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5a7658-38d6-4b8d-bab5-12780012133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_4h = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-14400, -7200) # 4 hours to 2 hours before\n",
    "\n",
    "train_df = train_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )\n",
    "validation_df = validation_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )\n",
    "\n",
    "test_df = test_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88bc6f0-ba4d-4358-84c2-bf54aa9dfcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n",
    "    \n",
    "validation_df = validation_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n",
    "    \n",
    "test_df = test_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af24ebb-0fc2-43f9-83a1-5a64ad2ef18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Delay time for flights at departure locations over the past 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f50f7c-6df1-46b6-bc9e-2425dfde8da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_origin = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b62b4d0-e8b9-4a4d-8c07-b5ce60740deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Number of delayed flights at departure and carrier location over the last 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4028168e-a6bd-4a35-b476-ec7a0809f2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_origin_carrier = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\", \"OP_UNIQUE_CARRIER\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n",
    "\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cc01fa3-a466-4ad2-a2ab-613b1b24faff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] number of delays in route in the last 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f904db4-4a82-4605-babb-0026d37d950d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec504be-ec13-4ba4-8635-6cc8efce7b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_route = Window \\\n",
    "    .partitionBy(\"route\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') \n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e36fafe1-655c-4323-bfda-c1246f631f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] - number of flights per day for one plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc32923f-0f9f-465a-be55-24d18076540a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_flights_24h = Window \\\n",
    "  .partitionBy(\"TAIL_NUM\", \"FL_DATE\") \\\n",
    "  .orderBy(F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")\n",
    "test_df = test_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b536a756-4f44-4c1a-9c2d-ae429367a689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] time between landed and scheduled flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f0dcf-57c5-4c1c-aa58-784daa759ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def hhmm_to_time_str(col):\n",
    "    padded = F.lpad(F.col(col).cast(\"string\"), 4, \"0\")\n",
    "    return F.concat_ws(\":\", padded.substr(1, 2), padded.substr(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f12936-a110-463f-90dd-94f906887b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df = train_df.withColumn(\n",
    "#     \"CRS_ARR_TIME_STR\",\n",
    "#     hhmm_to_time_str(\"ARR_TIME\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_STR\",\n",
    "#     hhmm_to_time_str(\"WHEELS_ON\")\n",
    "# )\n",
    "\n",
    "# train_df = train_df.withColumn(\n",
    "#     \"CRS_ARR_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"CRS_ARR_TIME_STR\", \"HH:mm\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"WHEELS_ON_STR\", \"HH:mm\")\n",
    "# )\n",
    "\n",
    "# train_df = train_df.withColumn(\n",
    "#     \"LANDING_TIME_DIFF_MINUTES\",\n",
    "#     F.coalesce(\n",
    "#         (\n",
    "#             (F.col(\"WHEELS_ON_TIMESTAMP\").cast(\"long\") - \n",
    "#              F.col(\"CRS_ARR_TIMESTAMP\").cast(\"long\")) / 60\n",
    "#         ),\n",
    "#         F.lit(0)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# validation_df = validation_df.withColumn(\n",
    "#     \"CRS_ARR_TIME_STR\",\n",
    "#     hhmm_to_time_str(\"ARR_TIME\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_STR\",\n",
    "#     hhmm_to_time_str(\"WHEELS_ON\")\n",
    "# )\n",
    "\n",
    "# validation_df = validation_df.withColumn(\n",
    "#     \"CRS_ARR_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"CRS_ARR_TIME_STR\", \"HH:mm\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"WHEELS_ON_STR\", \"HH:mm\")\n",
    "# )\n",
    "\n",
    "# validation_df = validation_df.withColumn(\n",
    "#     \"LANDING_TIME_DIFF_MINUTES\",\n",
    "#     F.coalesce(\n",
    "#         (\n",
    "#             (F.col(\"WHEELS_ON_TIMESTAMP\").cast(\"long\") - \n",
    "#              F.col(\"CRS_ARR_TIMESTAMP\").cast(\"long\")) / 60\n",
    "#         ),\n",
    "#         F.lit(0)\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a41b91-86a5-4b39-9247-9f566e63bbd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_turnaround = Window \\\n",
    "    .partitionBy(\"TAIL_NUM\") \\\n",
    "    .orderBy(F.col(\"WHEELS_ON\").cast(\"long\")) \n",
    "\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"next_scheduled_dep_ts\", \n",
    "    F.lead(\"CRS_DEP_TIME\", 1).over(window_turnaround)\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    F.coalesce(\n",
    "        (F.col(\"next_scheduled_dep_ts\").cast(\"long\") - F.col(\"WHEELS_ON\").cast(\"long\")) / 60,\n",
    "        F.lit(-999) \n",
    "    )\n",
    ").drop(\"next_scheduled_dep_ts\")\n",
    "\n",
    "train_df.select(\"TAIL_NUM\", \"WHEELS_ON\", \"CRS_DEP_TIME\", \"LANDING_TIME_DIFF_MINUTES\").orderBy(\"TAIL_NUM\", \"WHEELS_ON\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6274962-9a22-4fa0-a388-eff57540f38d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validation_df = validation_df.withColumn(\n",
    "    \"next_scheduled_dep_ts\", \n",
    "    F.lead(\"CRS_DEP_TIME\", 1).over(window_turnaround)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    F.coalesce(\n",
    "        (F.col(\"next_scheduled_dep_ts\").cast(\"long\") - F.col(\"WHEELS_ON\").cast(\"long\")) / 60,\n",
    "        F.lit(-999) \n",
    "    )\n",
    ").drop(\"next_scheduled_dep_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b90248-0f5e-43ba-ab27-941faf666042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\n",
    "    \"next_scheduled_dep_ts\", \n",
    "    F.lead(\"CRS_DEP_TIME\", 1).over(window_turnaround)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    F.coalesce(\n",
    "        (F.col(\"next_scheduled_dep_ts\").cast(\"long\") - F.col(\"WHEELS_ON\").cast(\"long\")) / 60,\n",
    "        F.lit(-999) \n",
    "    )\n",
    ").drop(\"next_scheduled_dep_ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5b48dd8-cdd6-4c41-946e-ca93b38dd6b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Average Delay time by airport\n",
    "- by origin airport and by destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d81c599-5aa0-4c80-9600-8784f8f490e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_delay_by_airport_train = train_df.groupBy(\"DEST_AIRPORT_SEQ_ID\").agg(\n",
    "    F.avg(\"ARR_DELAY\").alias(\"AVG_ARR_DELAY\")\n",
    ")\n",
    "\n",
    "avg_delay_by_airport_val = validation_df.groupBy(\"DEST_AIRPORT_SEQ_ID\").agg(\n",
    "    F.avg(\"ARR_DELAY\").alias(\"AVG_ARR_DELAY\")\n",
    ")\n",
    "\n",
    "avg_delay_by_airport_test = test_df.groupBy(\"DEST_AIRPORT_SEQ_ID\").agg(\n",
    "    F.avg(\"ARR_DELAY\").alias(\"AVG_ARR_DELAY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22deb43-774c-4526-94b2-304b367f70d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df.select(\"DEST\", \"ARR_DELAY\", \"AVG_ARR_DELAY\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9657d8d4-253d-4884-b28d-770fd61bbc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "window_7d_origin = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days (604800s) to -4 hours (14400s)\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'avg_delay_origin_7d_raw', \n",
    "    F.avg('ARR_DELAY').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'AVG_ARR_DELAY_ORIGIN', \n",
    "    F.coalesce(F.col('avg_delay_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_delay_origin_7d_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'avg_delay_origin_7d_raw', \n",
    "    F.avg('ARR_DELAY').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'AVG_ARR_DELAY_ORIGIN', \n",
    "    F.coalesce(F.col('avg_delay_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_delay_origin_7d_raw')\n",
    "\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'avg_delay_origin_7d_raw', \n",
    "    F.avg('ARR_DELAY').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'AVG_ARR_DELAY_ORIGIN', \n",
    "    F.coalesce(F.col('avg_delay_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_delay_origin_7d_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b81c2209-ed8b-4612-ac98-c598ad244aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Average taxi-out time by airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb15f98-3575-47f6-8ac9-3dbcd278e204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn(\n",
    "    'avg_taxi_out_origin_7d_raw', \n",
    "    F.avg('TAXI_OUT').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'AVG_TAXI_OUT_ORIGIN', \n",
    "    F.coalesce(F.col('avg_taxi_out_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_taxi_out_origin_7d_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'avg_taxi_out_origin_7d_raw', \n",
    "    F.avg('TAXI_OUT').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'AVG_TAXI_OUT_ORIGIN', \n",
    "    F.coalesce(F.col('avg_taxi_out_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_taxi_out_origin_7d_raw')\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'avg_taxi_out_origin_7d_raw', \n",
    "    F.avg('TAXI_OUT').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'AVG_TAXI_OUT_ORIGIN', \n",
    "    F.coalesce(F.col('avg_taxi_out_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_taxi_out_origin_7d_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10343c24-d4e5-45d1-980a-9f15f0a39eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Is holiday?\n",
    "US Holidays only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0fa9ee0-332d-475e-b1b4-c99a13ba932f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ee3ff1-20d2-4231-9311-f6ae352894ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9c5388-7d93-4d7b-9453-a0c95a52c0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, to_date\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "holidays_2015 = {\n",
    "    date(2015, 1, 1),   # New Year's Day\n",
    "    date(2015, 1, 19),  # MLK Jr.'s Birthday\n",
    "    date(2015, 2, 16),  # Washington's Birthday (Presidents' Day)\n",
    "    date(2015, 5, 25),  # Memorial Day\n",
    "    date(2015, 7, 3),   # Independence Day (observed)\n",
    "    date(2015, 9, 7),   # Labor Day\n",
    "    date(2015, 10, 12), # Columbus Day\n",
    "    date(2015, 11, 11), # Veterans Day\n",
    "    date(2015, 11, 26), # Thanksgiving Day\n",
    "    date(2015, 12, 25)  # Christmas Day\n",
    "}\n",
    "\n",
    "holidays_2016 = {\n",
    "    date(2016, 1, 1),   # New Year's Day\n",
    "    date(2016, 1, 18),  # MLK Jr.'s Birthday\n",
    "    date(2016, 2, 15),  # Washington's Birthday (Presidents' Day)\n",
    "    date(2016, 5, 30),  # Memorial Day\n",
    "    date(2016, 7, 4),   # Independence Day\n",
    "    date(2016, 9, 5),   # Labor Day\n",
    "    date(2016, 10, 10), # Columbus Day\n",
    "    date(2016, 11, 11), # Veterans Day\n",
    "    date(2016, 11, 24), # Thanksgiving Day\n",
    "    date(2016, 12, 26)  # Christmas Day (observed)\n",
    "}\n",
    "\n",
    "holidays_2017 = {\n",
    "    date(2017, 1, 2),   # New Year's Day (observed)\n",
    "    date(2017, 1, 16),  # MLK Jr.'s Birthday\n",
    "    date(2017, 2, 20),  # Washington's Birthday (Presidents' Day)\n",
    "    date(2017, 5, 29),  # Memorial Day\n",
    "    date(2017, 7, 4),   # Independence Day\n",
    "    date(2017, 9, 4),   # Labor Day\n",
    "    date(2017, 10, 9),  # Columbus Day\n",
    "    date(2017, 11, 10), # Veterans Day (observed)\n",
    "    date(2017, 11, 23), # Thanksgiving Day\n",
    "    date(2017, 12, 25)  # Christmas Day\n",
    "}\n",
    "\n",
    "holidays_2018 = {\n",
    "    date(2018, 1, 1),   # New Year's Day\n",
    "    date(2018, 1, 15),  # MLK Jr.'s Birthday\n",
    "    date(2018, 2, 19),  # Washington's Birthday (Presidents' Day)\n",
    "    date(2018, 5, 28),  # Memorial Day\n",
    "    date(2018, 7, 4),   # Independence Day\n",
    "    date(2018, 9, 3),   # Labor Day\n",
    "    date(2018, 10, 8),  # Columbus Day\n",
    "    date(2018, 11, 12), # Veterans Day (observed)\n",
    "    date(2018, 11, 22), # Thanksgiving Day\n",
    "    date(2018, 12, 25)  # Christmas Day\n",
    "}\n",
    "\n",
    "holidays_2019 = {\n",
    "    date(2019, 1, 1),   # New Year's Day\n",
    "    date(2019, 1, 21),  # MLK Jr.'s Birthday\n",
    "    date(2019, 2, 18),  # Washington's Birthday (Presidents' Day)\n",
    "    date(2019, 5, 27),  # Memorial Day\n",
    "    date(2019, 7, 4),   # Independence Day\n",
    "    date(2019, 9, 2),   # Labor Day\n",
    "    date(2019, 10, 14), # Columbus Day\n",
    "    date(2019, 11, 11), # Veterans Day\n",
    "    date(2019, 11, 28), # Thanksgiving Day\n",
    "    date(2019, 12, 25)  # Christmas Day\n",
    "}\n",
    "\n",
    "all_holidays = (\n",
    "    holidays_2015 | holidays_2016 | holidays_2017 | holidays_2018 | holidays_2019\n",
    ")\n",
    "\n",
    "# Convert all unique holiday dates into a set of strings for the UDF\n",
    "all_holidays_str = {d.strftime('%Y-%m-%d') for d in all_holidays}\n",
    "\n",
    "holidays_2019_str = [\n",
    "    d.strftime('%Y-%m-%d')\n",
    "    for d in holidays_2019\n",
    "]\n",
    "\n",
    "def check_holiday_window(flight_date_str, holidays_set_str, window_days=3):\n",
    "    if flight_date_str is None:\n",
    "        return 0\n",
    "        \n",
    "    try:\n",
    "        # Convert string back to date object for arithmetic\n",
    "        flight_date = date.fromisoformat(flight_date_str)\n",
    "        \n",
    "        # Check current date and dates +/- window_days\n",
    "        for i in range(-window_days, window_days + 1):\n",
    "            target_date = flight_date + timedelta(days=i)\n",
    "            target_date_str = target_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            if target_date_str in holidays_set_str:\n",
    "                return 1\n",
    "        return 0\n",
    "    except ValueError:\n",
    "        return 0 # Return 0 if date format is invalid\n",
    "\n",
    "# Register the UDF with Spark\n",
    "is_holiday_udf = udf(\n",
    "    lambda d: check_holiday_window(d, all_holidays_str, window_days=3), \n",
    "    IntegerType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6935db-8245-4526-b267-c1a217ea2ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    'IS_HOLIDAY', \n",
    "    (when(col(\"FL_DATE\").isin(all_holidays_str_list), 1).otherwise(0)).cast(\"integer\")\n",
    ")\n",
    "\n",
    "# validation\n",
    "validation_df = validation_df.withColumn(\n",
    "    'IS_HOLIDAY', \n",
    "    (when(col(\"FL_DATE\").isin(all_holidays_str_list), 1).otherwise(0)).cast(\"integer\")\n",
    ")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    'IS_HOLIDAY', \n",
    "    (when(col(\"FL_DATE\").isin(all_holidays_str_list), 1).otherwise(0)).cast(\"integer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c233b6-e91f-4863-882b-fb414ef6ab3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.select('IS_HOLIDAY').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f075443-77ca-416a-8366-102716e16191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Within 3 days of a holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f396a9a4-c303-4531-be1d-180802a0a3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get window\n",
    "is_in_holiday_window_udf = udf(\n",
    "    lambda x: check_holiday_window(x, holidays_2019, 3), \n",
    "    IntegerType()\n",
    ")\n",
    "\n",
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    \"FL_DATE_DT\", \n",
    "    to_date(col(\"FL_DATE\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'IS_HOLIDAY_WINDOW', \n",
    "    is_in_holiday_window_udf(col(\"FL_DATE_DT\"))\n",
    ")\n",
    "\n",
    "train_df = train_df.drop(\"FL_DATE_DT\")\n",
    "\n",
    "# val\n",
    "validation_df = validation_df.withColumn(\n",
    "    \"FL_DATE_DT\", \n",
    "    to_date(col(\"FL_DATE\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'IS_HOLIDAY_WINDOW', \n",
    "    is_in_holiday_window_udf(col(\"FL_DATE_DT\"))\n",
    ")\n",
    "\n",
    "validation_df = validation_df.drop(\"FL_DATE_DT\")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    \"FL_DATE_DT\", \n",
    "    to_date(col(\"FL_DATE\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'IS_HOLIDAY_WINDOW', \n",
    "    is_in_holiday_window_udf(col(\"FL_DATE_DT\"))\n",
    ")\n",
    "\n",
    "test_df = test_df.drop(\"FL_DATE_DT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7872b5-ac92-45b5-b0ad-96e7db825682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df.select('IS_HOLIDAY_WINDOW').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e047651-2087-497d-b897-972fb052c32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73fd1abe-ee77-489c-93ba-a6c2ae8e7f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Airport Size\n",
    "Based on FAA Hub categories\n",
    "<br>\n",
    "hub category = airport's annual passenger boardings/US annual passenger boardings\n",
    "\n",
    "- [0] Large Hub (P-L)\tHandles 1.00% or more of total U.S. annual boardings.\n",
    "- [1] Medium Hub (P-M)\tHandles 0.25% to less than 1.00% of total U.S. annual boardings.\n",
    "- [2] Small Hub (P-S)\tHandles 0.05% to less than 0.25% of total U.S. annual boardings.\n",
    "- [3] Non-Hub Primary (P-N)\tHandles less than 0.05% but has at least 10,000 annual boardings.\n",
    "- [4] Non-Primary Commercial Service (CS)\tHas between 2,500 and 10,000 annual boardings.\n",
    "- [5] Other\tIncludes Reliever and General Aviation (GA) airports, or codes that were not valid airport identifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b8c039-ecd3-49ce-845f-172b6ecf3a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airport_classification_data = {\n",
    "    # Large Hub (0)\n",
    "    \"ATL\": 0, \"BOS\": 0, \"CLT\": 0, \"DCA\": 0, \"DEN\": 0, \"DFW\": 0, \"DTW\": 0, \"EWR\": 0, \n",
    "    \"FLL\": 0, \"IAD\": 0, \"IAH\": 0, \"JFK\": 0, \"LAS\": 0, \"LAX\": 0, \"LGA\": 0, \"MCO\": 0, \n",
    "    \"MIA\": 0, \"MSP\": 0, \"ORD\": 0, \"PHL\": 0, \"PHX\": 0, \"SAN\": 0, \"SEA\": 0, \"SFO\": 0, \n",
    "    \"SLC\": 0,\n",
    "\n",
    "    # Medium Hub (1)\n",
    "    \"ABQ\": 1, \"ANC\": 1, \"BNA\": 1, \"BWI\": 1, \"BUR\": 1, \"CVG\": 1, \"DAL\": 1, \"FAI\": 1, \n",
    "    \"GEG\": 1, \"HNL\": 1, \"HOU\": 1, \"IND\": 1, \"MCI\": 1, \"MDW\": 1, \"MEM\": 1, \"MSY\": 1, \n",
    "    \"OAK\": 1, \"OGG\": 1, \"ONT\": 1, \"PDX\": 1, \"PIT\": 1, \"RDU\": 1, \"RNO\": 1, \"RSW\": 1, \n",
    "    \"SJC\": 1, \"SJU\": 1, \"SNA\": 1, \"TPA\": 1, \"TUS\": 1, \"XNA\": 1,\n",
    "    \n",
    "    # Small Hub (2)\n",
    "    \"ABE\": 2, \"ABY\": 2, \"ACV\": 2, \"AGS\": 2, \"ALB\": 2, \"ALO\": 2, \"AMA\": 2, \"APN\": 2,\n",
    "    \"ART\": 2, \"ASE\": 2, \"ATW\": 2, \"AVP\": 2, \"AZO\": 2, \"BFL\": 2, \"BGM\": 2, \"BIL\": 2, \n",
    "    \"BIS\": 2, \"BJI\": 2, \"BLI\": 2, \"BOI\": 2, \"BQK\": 2, \"BRO\": 2, \"BTR\": 2, \"BTV\": 2, \n",
    "    \"BZN\": 2, \"CAE\": 2, \"CAK\": 2, \"CDC\": 2, \"CID\": 2, \"CIU\": 2, \"CLE\": 2, \"CLL\": 2, \n",
    "    \"CMH\": 2, \"CMX\": 2, \"CNY\": 2, \"COD\": 2, \"COS\": 2, \"COU\": 2, \"CPR\": 2, \"CRW\": 2, \n",
    "    \"CSG\": 2, \"CWA\": 2, \"CYS\": 2, \"DAB\": 2, \"DAY\": 2, \"DHN\": 2, \"DLH\": 2, \"DSM\": 2, \n",
    "    \"DRO\": 2, \"EAU\": 2, \"EGE\": 2, \"EKO\": 2, \"ELM\": 2, \"ELP\": 2, \"ERI\": 2, \"ESC\": 2, \n",
    "    \"EUG\": 2, \"EVV\": 2, \"EWN\": 2, \"EYW\": 2, \"FAR\": 2, \"FAT\": 2, \"FAY\": 2, \"FCA\": 2, \n",
    "    \"FLG\": 2, \"FSD\": 2, \"FSM\": 2, \"FWA\": 2, \"GCK\": 2, \"GFK\": 2, \"GJT\": 2, \"GNV\": 2, \n",
    "    \"GPT\": 2, \"GRB\": 2, \"GRI\": 2, \"GRK\": 2, \"GRR\": 2, \"GSO\": 2, \"GSP\": 2, \"GTF\": 2, \n",
    "    \"GUM\": 2, \"HA\": 2, \"HDN\": 2, \"HIB\": 2, \"HLN\": 2, \"HRL\": 2, \"HSV\": 2, \"HTS\": 2, \n",
    "    \"HVN\": 2, \"HYA\": 2, \"IDA\": 2, \"ITH\": 2, \"JAC\": 2, \"JAN\": 2, \"JMS\": 2, \"JNU\": 2, \n",
    "    \"KTN\": 2, \"LAN\": 2, \"LAR\": 2, \"LAW\": 2, \"LEX\": 2, \"LFT\": 2, \"LGB\": 2, \"LIH\": 2,\n",
    "    \"LIT\": 2, \"LNK\": 2, \"LRD\": 2, \"LSE\": 2, \"LWS\": 2, \"LYH\": 2, \"MAF\": 2, \"MBS\": 2, \n",
    "    \"MGM\": 2, \"MHK\": 2, \"MLI\": 2, \"MLU\": 2, \"MMH\": 2, \"MOB\": 2, \"MOT\": 2, \"MQT\": 2, \n",
    "    \"MSO\": 2, \"MTJ\": 2, \"MVY\": 2, \"OAJ\": 2, \"OGS\": 2, \"OME\": 2, \"OTH\": 2, \"OWB\": 2, \n",
    "    \"PAH\": 2, \"PBG\": 2, \"PGV\": 2, \"PHF\": 2, \"PIB\": 2, \"PIH\": 2, \"PIR\": 2, \"PLN\": 2, \n",
    "    \"PNS\": 2, \"PPG\": 2, \"PRC\": 2, \"PSC\": 2, \"PSG\": 2, \"PSM\": 2, \"PSP\": 2, \"PUB\": 2, \n",
    "    \"PVU\": 2, \"RDM\": 2, \"RHI\": 2, \"RKS\": 2, \"RST\": 2, \"ROW\": 2, \"SAF\": 2, \"SBP\": 2, \n",
    "    \"SCC\": 2, \"SCE\": 2, \"SGU\": 2, \"SHD\": 2, \"SIT\": 2, \"SLN\": 2, \"SMX\": 2, \"SPN\": 2, \n",
    "    \"SPI\": 2, \"STC\": 2, \"STS\": 2, \"SUX\": 2, \"SWF\": 2, \"SWO\": 2, \"TLH\": 2, \"TOL\": 2, \n",
    "    \"TRI\": 2, \"TVC\": 2, \"TXK\": 2, \"TYR\": 2, \"UIN\": 2, \"VLD\": 2, \"WRG\": 2, \"WYS\": 2, \n",
    "    \"YAK\": 2, \"YUM\": 2, \"RFD\": 2, \"LBE\": 2, \"DRT\": 2, \n",
    "    \n",
    "    # Non-Hub Primary (4)\n",
    "    # Note: Many smaller airports fluctuate between categories, placed here for the enumeration request.\n",
    "    \"ABR\": 4, \"ACK\": 4, \"AKN\": 4, \"AZA\": 4, \"BGM\": 4, \"BKG\": 4, \"BRW\": 4, \"CDV\": 4, \n",
    "    \"GTR\": 4, \"LBL\": 4, \"LCK\": 4, \"LWB\": 4, \"MEI\": 4, \"OGD\": 4, \"OME\": 4, \"OTH\": 4, \n",
    "    \"PIE\": 4, \"PVU\": 4, \"RFD\": 4, \"RHI\": 4, \"SLN\": 4, \"STT\": 4, \"SUN\": 4, \"SWO\": 4, \n",
    "    \"TTN\": 4, \"VEL\": 4, \"WRG\": 4, \"YAK\": 4, \"YUM\": 4,\n",
    "    \n",
    "    # Non-Primary Commercial Service (5)\n",
    "    \"ATY\": 5, \"BFF\": 5, \"BTM\": 5, \"CIU\": 5, \"DBQ\": 5, \"DLG\": 5, \"GST\": 5, \"IMT\": 5, \n",
    "    \"INL\": 5, \"LWF\": 5, \"MQT\": 5, \"PIR\": 5, \"PSM\": 5, \"TOL\": 5, \n",
    "\n",
    "    # Other/General Aviation/Reliever (6)\n",
    "    \"BRD\": 6, \"CMI\": 6, \"HHH\": 6, \"HYS\": 6, \"LWL\": 6, \"MMH\": 6, \"PAE\": 6, \"PSE\": 6, \n",
    "    \"PSP\": 6, \"RKS\": 6, \"USA\": 6, \"VEL\": 6,\n",
    "    \n",
    "    # Missing/Uncertain Codes (Set to 6 for consistency)\n",
    "    \"9E\": 6, \"EV\": 6, \"MQ\": 6, \"NK\": 6, \"OO\": 6, \"WN\": 6, \"YV\": 6, \"YX\": 6, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378f6b7f-a216-43bc-8efd-2500b53ca0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def map_airport_class(airport_code):\n",
    "    \"\"\"Looks up the hub class for a given airport code, defaulting to 6.\"\"\"\n",
    "    global airport_classification_data  # Good practice if running in Spark\n",
    "    return airport_classification_data.get(airport_code, 6)\n",
    "\n",
    "hub_class_udf = udf(map_airport_class, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1abc762-fa81-4c90-bfb5-f8e9d79a62e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    'AIRPORT_HUB_CLASS', \n",
    "    hub_class_udf(col(\"ORIGIN\"))\n",
    ")\n",
    "\n",
    "# validation\n",
    "validation_df = validation_df.withColumn(\n",
    "    'AIRPORT_HUB_CLASS', \n",
    "    hub_class_udf(col(\"ORIGIN\"))\n",
    ")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    'AIRPORT_HUB_CLASS', \n",
    "    hub_class_udf(col(\"ORIGIN\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab7c9c8c-3764-49ac-a1a5-bb3ba299c6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Airline Sentiment\n",
    "- get list of unique airlines\n",
    "- rate airline sentiment/perception on liechert scale 1-5 in some llm\n",
    "  - based on:\n",
    "    - on-time performance\n",
    "    - cancellation rate\n",
    "    - involuntary denied boarding rate\n",
    "    - value\n",
    "    - level of trust\n",
    "    - mishandled baggage rate\n",
    "    - pre/post flight experience\n",
    "    - boarding process\n",
    "    - flight crew service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb2dc13b-bb85-486c-922b-78459b87e4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airline_sentiment_data = {\n",
    "    \"UA\": {\n",
    "        \"rating\": 3.8,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"NK\": {\n",
    "        \"rating\": 1.8,\n",
    "        \"category\": 3\n",
    "    },\n",
    "    \"AA\": {\n",
    "        \"rating\": 3.5,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"EV\": {\n",
    "        \"rating\": 3.0,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"B6\": {\n",
    "        \"rating\": 4.2,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"DL\": {\n",
    "        \"rating\": 4.5,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"OO\": {\n",
    "        \"rating\": 3.0,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"F9\": {\n",
    "        \"rating\": 2.0,\n",
    "        \"category\": 3\n",
    "    },\n",
    "    \"YV\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"MQ\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"OH\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"HA\": {\n",
    "        \"rating\": 4.0,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"G4\": {\n",
    "        \"rating\": 2.5,\n",
    "        \"category\": 3\n",
    "    },\n",
    "    \"YX\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"AS\": {\n",
    "        \"rating\": 4.3,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"WN\": {\n",
    "        \"rating\": 4.0,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"9E\": {\n",
    "        \"rating\": 3.0,\n",
    "        \"category\": 2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ae3e3d-eb71-4925-818d-2df702b96bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def map_rating(carrier_code):\n",
    "    \"\"\"Looks up the sentiment rating, defaulting to a neutral 3.0 if not found.\"\"\"\n",
    "    # The rating is FloatType\n",
    "    return airline_sentiment_data.get(carrier_code, {'rating': 3.0}).get('rating')\n",
    "\n",
    "def map_category(carrier_code):\n",
    "    \"\"\"Looks up the airline category, defaulting to 2 (Regional) if not found.\"\"\"\n",
    "    # The category is IntegerType (1=Major, 2=Regional, 3=ULCC)\n",
    "    return airline_sentiment_data.get(carrier_code, {'category': 2}).get('category')\n",
    "\n",
    "rating_udf = udf(map_rating, FloatType())\n",
    "category_udf = udf(map_category, IntegerType())\n",
    "\n",
    "carrier_col_name = \"OP_UNIQUE_CARRIER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885dd0c4-94fa-4a84-b750-f4fc622088a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    'RATING', \n",
    "    rating_udf(col(carrier_col_name))\n",
    ").withColumn(\n",
    "    'AIRLINE_CATEGORY',\n",
    "    category_udf(col(carrier_col_name))\n",
    ")\n",
    "print(\"Train DataFrame updated with RATING and AIRLINE_CATEGORY.\")\n",
    "\n",
    "# val\n",
    "validation_df = validation_df.withColumn(\n",
    "    'RATING', \n",
    "    rating_udf(col(carrier_col_name))\n",
    ").withColumn(\n",
    "    'AIRLINE_CATEGORY',\n",
    "    category_udf(col(carrier_col_name))\n",
    ")\n",
    "print(\"Validation DataFrame updated with RATING and AIRLINE_CATEGORY.\")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    'RATING', \n",
    "    rating_udf(col(carrier_col_name))\n",
    ").withColumn(\n",
    "    'AIRLINE_CATEGORY',\n",
    "    category_udf(col(carrier_col_name))\n",
    ")\n",
    "print(\"Test DataFrame updated with RATING and AIRLINE_CATEGORY.\")\n",
    "\n",
    "# Example of how to check the new columns:\n",
    "train_df.select(carrier_col_name, \"RATING\", \"AIRLINE_CATEGORY\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb3a684-ce2b-404b-b333-374a90496d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Seasonality\n",
    "- for 1 year, is summer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a05d3f9a-5fbc-460b-92b7-3049ecca28d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81ed40f-ee65-4869-a4af-b7506a7b9cc4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763424469559}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_counts = validation_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in validation_df.columns])\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4f00cf-6cf1-4fd8-8978-eeffabd98c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checkpoint results with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca18eba0-7cbc-4be0-98d1-b1535e1decce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/feature_eng_ph3/training_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720ce5e9-f62d-41dd-9305-427dfe0ca5d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if input(\"CAREFUL: You're about to write to DBFS. Type 'y' to continue.\") == \"y\":\n",
    "    checkpoint_dataset(train_df, f\"{month_or_year}/feature_eng_ph3/training_splits/train\")\n",
    "    checkpoint_dataset(validation_df, f\"{month_or_year}/feature_eng_ph3/training_splits/validation\")\n",
    "    checkpoint_dataset(test_df, f\"{month_or_year}/feature_eng_ph3/training_splits/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce3464d9-fea8-421b-a750-e1c2d6c1607d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check data checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8246c2e7-8165-4baa-a468-1196ad262922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb8b6a2-ac88-4803-9539-4b6ebd201bd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "dataset_path = f\"{checkpoint_path}/5_year_custom_joined/feature_eng_ph3/training_splits/\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "check_train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "check_validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")\n",
    "check_test_df = spark.read.parquet(f\"{dataset_path}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667728ad-db3b-4661-93fb-52c943cdaece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check_train_df.columns == check_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4b7a6f-919a-435c-9796-c5e8cdfadffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in check_train_df.columns:\n",
    "    if col not in check_test_df.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769af6fd-5d02-4dc8-a0b5-7e2dbf0c2324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(check_train_df)\n",
    "# display(check_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d01de2a-03be-444e-aef6-19e7ae8d5408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check_train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2caf0a39-aaef-44fd-97a2-3c314d1adbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "move to modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5f1e40e-fcbd-4b5c-a11d-c2742e1bfb4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multitower features, 5 year training eda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00bddb75-086e-4ebb-a4cd-a459ba0846f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ad33a1-3047-4e1e-942c-e8178d8e9c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_multitower_df = spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1705adc-12f0-45ca-82cc-d9371b700520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_multitower_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de75d67-9bb1-4a2b-8c40-ad4910cb90c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get date range for multi-tower training\n",
    "train_multitower_df.describe(['FL_DATE']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b5101e5-5604-48f0-9ebb-3eca5a600443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Feature eng eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4cdaaa8-7e9a-4b81-8426-cde4b7951f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_data_df = train_multitower_df.select(\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"page_rank\",\n",
    "    \"out_degree\",\n",
    "    \"weighted_out_degree\",\n",
    "    \"closeness\",\n",
    "    \"betweenness\",\n",
    "    \"avg_origin_dep_delay\",\n",
    "    'prev_flight_delay_in_minutes',\n",
    "    'prev_flight_delay',\n",
    "    'RATING',\n",
    "    'OP_CARRIER_AIRLINE_ID',\n",
    "    'OP_CARRIER',\n",
    "    'avg_daily_route_flights',\n",
    "    'HourlyVisibility',\n",
    "    'weighted_in_degree',\n",
    "    'route'\n",
    ").distinct() # Use distinct to ensure one row per airport/key\n",
    "\n",
    "# Collect data to Pandas for plotting\n",
    "plot_data_pd = plot_data_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d96afc8-3173-4f92-9a04-a61ce1184ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaf09ef8-4a18-464e-8e83-5dcb34642e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### carrier vs rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3f5b71-7720-454e-a501-e3a2d837a18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your final_flight_results_df is the DataFrame containing all features\n",
    "\n",
    "# --- 1. PYSPARK AGGREGATION ---\n",
    "# Calculate the average RATING for each OP_CARRIER\n",
    "# We will also count how many flights are included for context (count is optional)\n",
    "carrier_summary_df = (\n",
    "    train_multitower_df\n",
    "    .groupBy(\"OP_CARRIER\")\n",
    "    .agg(\n",
    "        F.avg(\"RATING\").alias(\"Avg_Rating\"),\n",
    "        F.count(\"*\").alias(\"Total_Flights\")\n",
    "    )\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# --- 2. PREPARE DATA FOR PLOTTING ---\n",
    "# Sort the carriers by their average rating for a cleaner visual comparison\n",
    "carrier_summary_df.sort_values(by=\"Avg_Rating\", ascending=False, inplace=True)\n",
    "\n",
    "# --- 3. VISUALIZE WITH BAR CHART ---\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    x='OP_CARRIER',\n",
    "    y='Avg_Rating',\n",
    "    data=carrier_summary_df,\n",
    "    palette='viridis' # A sequential color palette\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Average Rating Score by Operating Carrier', fontsize=16)\n",
    "plt.xlabel('Operating Carrier Airline', fontsize=14)\n",
    "plt.ylabel('Carrier Rating', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59650aa-ffad-4ea1-91e2-03402046e0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Graph Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a69b0d-d771-46de-99d2-44ad9c42baeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### PageRank by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00d4b2f-1fd0-4771-9303-073b2dca402c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate the PageRank by state. Use the mean to get a representative value.\n",
    "page_rank_by_state_df = train_multitower_df.groupBy(\"ORIGIN_STATE_ABR\").agg(\n",
    "    F.mean(\"page_rank\").alias(\"avg_page_rank\")\n",
    ").withColumnRenamed(\"ORIGIN_STATE_ABR\", \"state_abbr\")\n",
    "\n",
    "# This aggregated DataFrame is small enough to collect and visualize.\n",
    "page_rank_pd = page_rank_by_state_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255e1e2c-c38c-4fb0-9ee9-c65bcd860790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Assuming the data collected in step 1 is named page_rank_pd\n",
    "\n",
    "fig = px.choropleth(\n",
    "    page_rank_pd, \n",
    "    locations='state_abbr', \n",
    "    locationmode=\"USA-states\", \n",
    "    color='avg_page_rank',\n",
    "    scope=\"usa\",\n",
    "    color_continuous_scale=\"Viridis\", # Choose a color scale\n",
    "    title='Average Airport PageRank by State (Network Influence)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94220e93-d682-4c4a-bd68-269852430bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### unique routes vs traffic volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7dd6a0c-609c-40eb-8b24-4b0b7e4ad540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Scatter Plot 2: Unique Routes vs. Traffic Volume ---\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    plot_data_pd['out_degree'], \n",
    "    plot_data_pd['weighted_out_degree'], \n",
    "    alpha=0.6, \n",
    "    color='darkblue',\n",
    "    s=20\n",
    ")\n",
    "\n",
    "plt.title('Unique Outbound Routes vs. Total Outbound Traffic Volume', fontsize=14)\n",
    "plt.xlabel('Out-Degree (Number of Unique Destinations)', fontsize=12)\n",
    "plt.ylabel('Weighted Out-Degree (Total Flights)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63817e95-999b-496a-8a36-de72584da77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab835b6-9e6c-4ec8-9228-3d9f93951793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### previous flight delay in minutes\n",
    "- by carrier\n",
    "- by origin\n",
    "- by route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e328213-1081-4f93-93c4-b4608461fe9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def get_top_n_avg_min_max_delay(df, group_col, n=15):\n",
    "\n",
    "    # 1. Calculate count and rank to identify top N categories\n",
    "    df_count = df.groupBy(group_col).count()\n",
    "    \n",
    "    # Get the list of the top N categories by count\n",
    "    top_n_categories = (\n",
    "        df_count\n",
    "        .orderBy(F.col(\"count\").desc())\n",
    "        .limit(n)\n",
    "        .select(group_col)\n",
    "        .rdd.flatMap(lambda x: x).collect()\n",
    "    )\n",
    "    \n",
    "    # 2. Filter the main DataFrame to include only the top categories\n",
    "    df_filtered = df.filter(F.col(group_col).isin(top_n_categories))\n",
    "    \n",
    "    # 3. Calculate the average, min, and max delay for the filtered set\n",
    "    df_agg = (\n",
    "        df_filtered.groupBy(group_col)\n",
    "        .agg(\n",
    "            # NEW: Add MIN and MAX aggregations\n",
    "            F.avg('prev_flight_delay_in_minutes').alias('Avg_Prev_Delay'),\n",
    "            F.min('prev_flight_delay_in_minutes').alias('Min_Prev_Delay'),\n",
    "            F.max('prev_flight_delay_in_minutes').alias('Max_Prev_Delay'),\n",
    "            F.count('*').alias('Count') \n",
    "        )\n",
    "        .orderBy(F.col('Avg_Prev_Delay').desc())\n",
    "    )\n",
    "    \n",
    "    # 4. Convert to Pandas for visualization/display\n",
    "    df_agg_pandas = df_agg.toPandas()\n",
    "    \n",
    "    return df_agg_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f03152-add7-4050-8608-e086400d86f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2642e13e-8987-4361-abaa-beb7205ff2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Carrier\n",
    "# Includes all carriers if total is <= 15, otherwise top 15\n",
    "df_carrier = get_top_n_avg_min_max_delay(train_multitower_df, 'OP_CARRIER', n=15) \n",
    "\n",
    "# 2. Origin (Top 15)\n",
    "df_origin = get_top_n_avg_min_max_delay(train_multitower_df, 'ORIGIN', n=15)\n",
    "df_origin['ORIGIN'] = df_origin['ORIGIN'].astype(str) # String conversion for clean plotting\n",
    "\n",
    "# 3. Route (Top 15)\n",
    "df_route = get_top_n_avg_min_max_delay(train_multitower_df, 'route', n=15)\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 18))\n",
    "plt.suptitle('Average Previous Flight Delay by Carrier, Origin, and Route (Top 15)', fontsize=18, y=1.02)\n",
    "\n",
    "# Subplot 1: Carrier\n",
    "sns.barplot(\n",
    "    ax=axes[0],\n",
    "    x='OP_CARRIER',\n",
    "    y='Avg_Prev_Delay',\n",
    "    data=df_carrier,\n",
    "    palette='viridis'\n",
    ")\n",
    "axes[0].set_title('Average Previous Delay by Operating Carrier', fontsize=14)\n",
    "axes[0].set_xlabel('Operating Carrier Airline', fontsize=12)\n",
    "axes[0].set_ylabel('Avg. Previous Delay (minutes)', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45) \n",
    "\n",
    "# Subplot 2: Origin\n",
    "sns.barplot(\n",
    "    ax=axes[1],\n",
    "    x='ORIGIN',\n",
    "    y='Avg_Prev_Delay',\n",
    "    data=df_origin,\n",
    "    palette='magma'\n",
    ")\n",
    "axes[1].set_title(f'Average Previous Delay by Origin Airport (Top {len(df_origin)})', fontsize=14)\n",
    "axes[1].set_xlabel('Origin Airport', fontsize=12)\n",
    "axes[1].set_ylabel('Avg. Previous Delay (minutes)', fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45) \n",
    "\n",
    "# Subplot 3: Route\n",
    "sns.barplot(\n",
    "    ax=axes[2],\n",
    "    x='route',\n",
    "    y='Avg_Prev_Delay',\n",
    "    data=df_route,\n",
    "    palette='cividis'\n",
    ")\n",
    "axes[2].set_title(f'Average Previous Delay by Route (Top {len(df_route)})', fontsize=14)\n",
    "axes[2].set_xlabel('Route', fontsize=12)\n",
    "axes[2].set_ylabel('Avg. Previous Delay (minutes)', fontsize=12)\n",
    "axes[2].tick_params(axis='x', rotation=45) \n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea707763-f409-4762-912e-6307ed687fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_carrier.describe().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38527c58-dddd-4edb-b181-7439c0ec7ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_origin.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aeccf20-e0e5-4ae2-8585-e6db1ea76aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_route.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae9525c-a34f-4516-982e-7d21163fa982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Weather - visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7c4c06-b44c-47f6-91af-47e6d3700b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURATION (Change these values) ---\n",
    "SFO_AIRPORT_ID = 14771 # Replace with SFO's actual ID if different\n",
    "TARGET_DATE = \"2016-03-15\" # Use your date of interest\n",
    "\n",
    "# Assuming 'train_multitower_df' is your PySpark DataFrame with \n",
    "from pyspark.sql.types import DateType# 'ORIGIN_AIRPORT_SEQ_ID', 'HourlyVisibility', and a timestamp column (e.g., 'FL_DATE_TIME')\n",
    "\n",
    "\n",
    "# 1. Filter the data for SFO and the specific date\n",
    "df_sfo_day = train_multitower_df.filter(\n",
    "    (F.col(\"ORIGIN_AIRPORT_SEQ_ID\") == SFO_AIRPORT_ID) &\n",
    "    (F.col(\"FL_DATE\").cast(DateType()) == TARGET_DATE)\n",
    ").select(\n",
    "    \"FL_DATE\",\n",
    "    \"HourlyVisibility\"\n",
    ")\n",
    "\n",
    "# 2. Extract the hour from the timestamp for the X-axis\n",
    "df_sfo_day = df_sfo_day.withColumn(\"HourOfDay\", F.hour(F.col(\"FL_DATE\")))\n",
    "\n",
    "# 3. Aggregate to ensure only one visibility reading per hour (using the average)\n",
    "df_sfo_agg = df_sfo_day.groupBy(\"HourOfDay\").agg(\n",
    "    F.avg(\"HourlyVisibility\").alias(\"Hourly_Visibility\")\n",
    ").orderBy(\"HourOfDay\")\n",
    "\n",
    "# 4. Convert to Pandas for plotting\n",
    "df_sfo_pd = df_sfo_agg.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a24f0e-ee25-4468-9b41-d31a04a96440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sfo_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b600ff-467c-4425-8c16-21352f37e49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- CRITICAL ---\n",
    "# You must have run the PySpark aggregation step above to create df_sfo_pd.\n",
    "# If you need to simulate data for testing, use this block:\n",
    "# TARGET_DATE = \"2024-03-15\"\n",
    "# df_sfo_pd = pd.DataFrame({\n",
    "#     'HourOfDay': np.arange(0, 24),\n",
    "#     'Avg_Visibility': np.clip(np.random.normal(9.5, 0.5, 24) - \n",
    "#                               np.concatenate([np.zeros(8), np.linspace(0, 7, 8), np.zeros(8)]), \n",
    "#                               a_min=0.1, a_max=10.0)\n",
    "# })\n",
    "# -----------------\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "# 1. Plot the Visibility Line\n",
    "sns.lineplot(\n",
    "    x='HourOfDay',\n",
    "    y='Hourly_Visibility',\n",
    "    data=df_sfo_pd,\n",
    "    marker='o',\n",
    "    linestyle='-',\n",
    "    color='blue',\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# 2. Add Critical Operational Thresholds\n",
    "# CAT I Minimum Visibility (approx 1/2 mile)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=1.5, label='CAT I (Critical Visibility)')\n",
    "ax.text(x=23, y=0.6, s='0.5 Miles', color='red', ha='right')\n",
    "\n",
    "# VFR Minimum Visibility (approx 3 miles, varies by airspace)\n",
    "ax.axhline(y=3.0, color='orange', linestyle=':', linewidth=1.5, label='Marginal VFR')\n",
    "ax.text(x=23, y=3.1, s='3.0 Miles', color='orange', ha='right')\n",
    "\n",
    "\n",
    "# 3. Titles and Labels\n",
    "plt.title(f'Hourly Visibility at SFO on {TARGET_DATE}', fontsize=16)\n",
    "plt.xlabel('Time of Day (Hour)', fontsize=14)\n",
    "plt.ylabel('Average Hourly Visibility (Miles)', fontsize=14)\n",
    "plt.xticks(np.arange(0, 24, 2)) # Show every 2nd hour\n",
    "plt.ylim(0, 10.5)\n",
    "plt.grid(axis='both', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e23d54-9734-40e4-becf-2f47a112be7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 2. Create the Binned Column using F.when / F.otherwise ---\n",
    "train_multitower_df_spark = train_multitower_df.withColumn(\n",
    "    \"Visibility_Bin\",\n",
    "    F.when(F.col(\"HourlyVisibility\") < 1.0, F.lit(\"< 1 Mi (Critical)\"))\n",
    "     .when((F.col(\"HourlyVisibility\") >= 1.0) & (F.col(\"HourlyVisibility\") < 3.0), F.lit(\"1-3 Mi (Low IFR)\"))\n",
    "     .when((F.col(\"HourlyVisibility\") >= 3.0) & (F.col(\"HourlyVisibility\") < 5.0), F.lit(\"3-5 Mi (Marginal VFR)\"))\n",
    "     .when((F.col(\"HourlyVisibility\") >= 5.0) & (F.col(\"HourlyVisibility\") < 10.0), F.lit(\"5-10 Mi\"))\n",
    "     # The 'otherwise' handles 10+ Mi (using a very large number as an implicit upper bound)\n",
    "     .otherwise(F.lit(\"10+ Mi (Clear)\")) \n",
    ")\n",
    "\n",
    "# --- 3. Aggregate in PySpark ---\n",
    "# Now you can use this new column for aggregation:\n",
    "df_agg_count_spark = (\n",
    "    train_multitower_df_spark.groupBy(\"Visibility_Bin\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    ")\n",
    "\n",
    "# --- 4. Convert to Pandas for Plotting (Final Step) ---\n",
    "df_agg_count_pd = df_agg_count_spark.toPandas().rename(columns={'count': 'Count'})\n",
    "\n",
    "# =========================================================================\n",
    "# === STEP 3: PLOTTING BLOCK ===\n",
    "# =========================================================================\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the counts\n",
    "sns.barplot(\n",
    "    x='Visibility_Bin',\n",
    "    y='Count',\n",
    "    data=df_agg_count_pd,\n",
    "    palette='Blues_d' \n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Distribution of Hourly Visibility in Training Data (2015-2018)', fontsize=16)\n",
    "plt.xlabel('Hourly Visibility Range', fontsize=14)\n",
    "plt.ylabel('Observation Count', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "# Format y-axis to show counts in a readable format (e.g., thousands)\n",
    "plt.ticklabel_format(style='plain', axis='y', scilimits=(0, 0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c560aa5b-17c6-4699-93b5-c286a94c26fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# === STEP 1: BINNING AND AGGREGATION (Pandas Native) ===\n",
    "# =========================================================================\n",
    "\n",
    "# --- 2. Create the Binned Column using F.when / F.otherwise ---\n",
    "train_multitower_df_spark = train_multitower_df.withColumn(\n",
    "    \"Visibility_Bin\",\n",
    "    F.when(F.col(\"HourlyVisibility\") < 1.0, F.lit(\"< 1 Mi (Critical)\"))\n",
    "     .when((F.col(\"HourlyVisibility\") >= 1.0) & (F.col(\"HourlyVisibility\") < 3.0), F.lit(\"1-3 Mi (Low IFR)\"))\n",
    "     .when((F.col(\"HourlyVisibility\") >= 3.0) & (F.col(\"HourlyVisibility\") < 5.0), F.lit(\"3-5 Mi (Marginal VFR)\"))\n",
    "     .when((F.col(\"HourlyVisibility\") >= 5.0) & (F.col(\"HourlyVisibility\") < 10.0), F.lit(\"5-10 Mi\"))\n",
    "     # The 'otherwise' handles 10+ Mi (using a very large number as an implicit upper bound)\n",
    "     .otherwise(F.lit(\"10+ Mi (Clear)\")) \n",
    ")\n",
    "\n",
    "# --- 3. Aggregate in PySpark ---\n",
    "# Now you can use this new column for aggregation:\n",
    "df_agg_count_spark = (\n",
    "    train_multitower_df_spark.groupBy(\"Visibility_Bin\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    ")\n",
    "\n",
    "# --- 4. Convert to Pandas for Plotting (Final Step) ---\n",
    "df_agg_count_pd = df_agg_count_spark.toPandas().rename(columns={'count': 'Count'})\n",
    "\n",
    "total_count = df_agg_count_pd['Count'].sum()\n",
    "df_agg_count_pd['Percentage'] = (df_agg_count_pd['Count'] / total_count) * 100\n",
    "\n",
    "# =========================================================================\n",
    "# === STEP 2: PLOTTING BLOCK WITH ANNOTATIONS ===\n",
    "# =========================================================================\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Plot the counts\n",
    "sns.barplot(\n",
    "    x='Visibility_Bin',\n",
    "    y='Count',\n",
    "    data=df_agg_count_pd,\n",
    "    palette='Blues_d',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# --- Add Annotations (Percentages) ---\n",
    "for index, row in df_agg_count_pd.iterrows():\n",
    "    # Get the height (Count) and the formatted percentage string\n",
    "    count_value = row['Count']\n",
    "    percentage_str = f'{row[\"Percentage\"]:.1f}%'\n",
    "    \n",
    "    # Place the text slightly above the bar\n",
    "    ax.text(\n",
    "        index,                  # X position (index of the bar)\n",
    "        count_value * 1.01,     # Y position (slightly above the bar)\n",
    "        percentage_str,         # Text to display\n",
    "        color='black',\n",
    "        ha=\"center\",            # Horizontal alignment\n",
    "        va=\"bottom\",            # Vertical alignment\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Distribution of Hourly Visibility in Training Data', fontsize=16)\n",
    "plt.xlabel('Hourly Visibility Range', fontsize=14)\n",
    "plt.ylabel('Observation Count', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "# Format y-axis to show counts in a readable format (e.g., thousands)\n",
    "plt.ticklabel_format(style='plain', axis='y', scilimits=(0, 0))\n",
    "plt.ylim(0, df_agg_count_pd['Count'].max() * 1.1) # Set Y limit higher to accommodate labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165dbb82-3291-4d4c-a4f1-5ae8bc8569a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Categorical - Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e398f98-3b30-4afb-82ab-a6a6b0c0ff1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your PySpark DataFrame is named train_multitower_df_spark\n",
    "\n",
    "# 1. Select the columns that define a route\n",
    "df_routes = train_multitower_df_spark.select(\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\"\n",
    ")\n",
    "\n",
    "# 2. Count the number of distinct (unique) rows in the selected columns\n",
    "unique_route_count = df_routes.distinct().count()\n",
    "\n",
    "# 3. Print the result\n",
    "print(f\"The total number of unique routes (Origin-Destination pairs) is: {unique_route_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a21bc51-f03f-4b7a-8740-c74ccd821e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ROUTE_COLUMN_NAME = \"route\" # <<< REPLACE THIS\n",
    "\n",
    "df_agg_routes_spark = (\n",
    "    train_multitower_df_spark.groupBy(ROUTE_COLUMN_NAME)\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "    .limit(20)\n",
    ")\n",
    "\n",
    "# Convert to Pandas for display/saving\n",
    "df_agg_routes_pd = df_agg_routes_spark.toPandas()\n",
    "\n",
    "display(df_agg_routes_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8e5c97-0542-4ee3-929c-66c355790387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Create the bar plot\n",
    "sns.barplot(\n",
    "    x=ROUTE_COLUMN_NAME,\n",
    "    y='count',\n",
    "    data=df_agg_routes_pd,\n",
    "    palette='viridis' # A good general-purpose color palette\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Top 20 Most Frequent Routes (Training 2015-2018)', fontsize=16)\n",
    "plt.xlabel('Route', fontsize=14)\n",
    "plt.ylabel('Total Count of Flights', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right') # Rotate X-labels for readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Format y-axis to show counts in a readable format\n",
    "plt.ticklabel_format(style='plain', axis='y', scilimits=(0, 0))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc392c48-8f42-4c35-8179-bf8436daa744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Airline category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f9e97d-5a61-4b93-a14a-42492d3fe5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the category remapping logic using F.when()\n",
    "df_category_counts = (\n",
    "    train_multitower_df.groupBy(\"AIRLINE_CATEGORY\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"Count\")\n",
    "    # Remap the category names using the F.when() structure\n",
    "    .withColumn(\n",
    "        \"AirlineCategoryRemapped\",\n",
    "        F.when(F.col(\"AIRLINE_CATEGORY\") == 1, '(1) Major Airlines')\n",
    "        .when(F.col(\"AIRLINE_CATEGORY\") == 2, '(2) National Airlines')\n",
    "        .when(F.col(\"AIRLINE_CATEGORY\") == 3, '(3) Regional Airlines')\n",
    "    )\n",
    "    .orderBy(F.col(\"Count\").desc())\n",
    ")\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "df_plot = df_category_counts.toPandas()\n",
    "\n",
    "# --- 2. PLOTTING BLOCK (Pandas/Matplotlib) ---\n",
    "\n",
    "# Define the explicit sort order for the plot\n",
    "plot_order = ['(1) Major Airlines', '(2) National Airlines', '(3) Regional Airlines']\n",
    "df_plot['AirlineCategoryRemapped'] = pd.Categorical(df_plot['AirlineCategoryRemapped'], categories=plot_order, ordered=True)\n",
    "df_plot.sort_values(by='AirlineCategoryRemapped', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    x='AirlineCategoryRemapped', # Use the new remapped column\n",
    "    y='Count',\n",
    "    data=df_plot,\n",
    "    palette='viridis' \n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Distribution of Flights by Airline Category', fontsize=16)\n",
    "plt.xlabel('Airline Category', fontsize=14)\n",
    "plt.ylabel('Total Flight Count', fontsize=14)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='y', scilimits=(0, 0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0373afe-fa6d-4be8-af85-a20ebd638865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_subset = train_multitower_df.select(\"AIRLINE_CATEGORY\", \"OP_CARRIER\")\n",
    "\n",
    "# To display the first few rows of the new DataFrame\n",
    "df_subset.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf173b3-4c1d-406d-9042-ee85b14b0267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATEGORY_COL = \"AIRLINE_CATEGORY\"\n",
    "CARRIER_COL = \"OP_CARRIER\" # Assuming this is the column for the individual carrier code\n",
    "\n",
    "# 1. Select the two columns and get all distinct combinations\n",
    "df_mapping_spark = train_multitower_df.select(CATEGORY_COL, CARRIER_COL).distinct()\n",
    "\n",
    "# 2. Sort the results for a clean presentation: \n",
    "#    Sort primarily by Category, then alphabetically by Carrier\n",
    "df_mapping_spark = df_mapping_spark.orderBy(\n",
    "    F.col(CATEGORY_COL).asc(),\n",
    "    F.col(CARRIER_COL).asc()\n",
    ")\n",
    "\n",
    "# 3. Convert to Pandas for display/printing the Markdown table\n",
    "df_mapping_pd = df_mapping_spark.toPandas()\n",
    "\n",
    "# 4. Rename columns for the final table display\n",
    "df_mapping_pd.rename(columns={\n",
    "    CATEGORY_COL: 'Airline Category', \n",
    "    CARRIER_COL: 'Carrier Code'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09c12e2-6fd9-4625-8f50-6a9bd44ad1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "carrier_mapping_dict = {\n",
    "    '(1) Major Airlines': ['AA', 'AS', 'B6', 'DL', 'HA', 'UA', 'WN'],\n",
    "    '(2) Regional Airlines': ['9E', 'EV', 'MQ', 'OH', 'OO', 'US', 'VX', 'YV', 'YX'],\n",
    "    '(3) Ultra Low Cost Airlines': ['F9', 'G4', 'NK']\n",
    "}\n",
    "\n",
    "# --- Markdown Table Generation ---\n",
    "\n",
    "markdown_table = \"##  Airline Carrier Category Mapping\\n\\n\"\n",
    "markdown_table += \"This table shows the assumed classification for each carrier code.\\n\\n\"\n",
    "markdown_table += \"| Category Code | Airline Category | Carrier Codes |\\n\"\n",
    "markdown_table += \"|:---:|:---|:---|\\n\"\n",
    "\n",
    "for category_name, carriers in carrier_mapping_dict.items():\n",
    "    # Split the category name to extract the code and description\n",
    "    # Example: '(1) Major Airlines' -> code='1', description='Major Airlines'\n",
    "    code = category_name.split(' ')[0].strip('()')\n",
    "    description = category_name.split(' ', 1)[1]\n",
    "    \n",
    "    # Join the carrier list into a comma-separated string\n",
    "    carrier_string = ', '.join(carriers)\n",
    "    \n",
    "    # Add the row to the table string\n",
    "    markdown_table += f\"| {code} | {description} | {carrier_string} |\\n\"\n",
    "\n",
    "# In Databricks, printing the string will typically render the Markdown \n",
    "# correctly in the cell output.\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20075c97-5091-43a1-8952-b6a688c8e0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cyclical fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a867e22d-e08a-4705-987c-c428433f9528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# --- Assuming your dataframe is named 'df_features' ---\n",
    "# NOTE: Replace 'your_dataframe' with the actual name of your DataFrame\n",
    "# NOTE: Replace 'HourlyAltimeterSetting' with the actual feature column name if different\n",
    "# NOTE: Replace 'Delay_Target_Minutes' with the actual target column name\n",
    "# NOTE: Replace 'ORIGIN_AIRPORT_CODE' with the actual airport code column name\n",
    "\n",
    "# 1. Define the variables\n",
    "FEATURE_COLUMN = 'HourlyAltimeterSetting'\n",
    "TARGET_COLUMN = 'DEP_DELAY'\n",
    "AIRPORT_FILTER = 'ATL' # Focus on a high-volume hub for a clear test case\n",
    "\n",
    "# df_filtered_pd = train_multitower_df[train_multitower_df['ORIGIN_AIRPORT_CODE'] == AIRPORT_FILTER].copy()\n",
    "\n",
    "# Convert the filtered Spark DataFrame to Pandas\n",
    "df_filtered_pd = df_filtered.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_filtered_pd,\n",
    "    x=FEATURE_COLUMN,\n",
    "    y=TARGET_COLUMN,\n",
    "    alpha=0.3,\n",
    "    s=20\n",
    ")\n",
    "\n",
    "plt.title(f'Leakage Check: \"{FEATURE_COLUMN}\" vs. Delay at {AIRPORT_FILTER}', fontsize=16)\n",
    "plt.xlabel(f'{FEATURE_COLUMN} (Barometric Pressure)', fontsize=12)\n",
    "plt.ylabel('Flight Delay (Minutes)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "SUSPICIOUS_VALUE = 2.80\n",
    "plt.axvline(x=SUSPICIOUS_VALUE, color='red', linestyle='-', linewidth=2, label=f'MI Spike Value ({SUSPICIOUS_VALUE})')\n",
    "plt.ylim(0, 180)\n",
    "plt.xlim(20,35)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c27c55-98b9-434c-8988-94002b8a7ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a synthetic DataFrame representing the 7 days of the week (0=Monday, 6=Sunday)\n",
    "days_of_week = pd.Series(range(7))\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "# Calculate the sine and cosine components\n",
    "dow_sin = np.sin(2 * np.pi * days_of_week / 7)\n",
    "dow_cos = np.cos(2 * np.pi * days_of_week / 7)\n",
    "\n",
    "# Create the synthetic DataFrame (mt_test_df) for plotting\n",
    "mt_test_df = pd.DataFrame({\n",
    "    'DOW': days_of_week,\n",
    "    'DayName': day_names,\n",
    "    'dow_sin': dow_sin,\n",
    "    'dow_cos': dow_cos\n",
    "})\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Scatter plot the points\n",
    "plt.scatter(mt_test_df['dow_cos'], mt_test_df['dow_sin'], s=100)\n",
    "\n",
    "# Add labels for each point (Day of Week)\n",
    "for i, day in enumerate(mt_test_df['DayName']):\n",
    "    # Adjust position slightly for better visibility\n",
    "    plt.annotate(day, (mt_test_df['dow_cos'][i] * 1.1, mt_test_df['dow_sin'][i] * 1.1),\n",
    "                 fontsize=10, ha='center')\n",
    "\n",
    "# Ensure the aspect ratio is equal to make the circle look round\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Add a circle outline to show the unit circle\n",
    "circle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle='--', alpha=0.5)\n",
    "ax.add_artist(circle)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-1.2, 1.2)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Cyclical Encoding of Day of Week (DOW)')\n",
    "plt.xlabel('dow_cos')\n",
    "plt.ylabel('dow_sin')\n",
    "plt.grid(True, linestyle=':', alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d323e999-2899-4aa2-9e3d-a1e8f3704dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    \"OP_CARRIER\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"route\",\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"CRS_DEP_MINUTES\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "]\n",
    "\n",
    "temporal_cols = [\n",
    "    \"utc_timestamp\",\n",
    "    \"prev_flight_delay_in_minutes\",\n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",\n",
    "]\n",
    "\n",
    "weather_cols = [\n",
    "    \"HourlyDryBulbTemperature\",\n",
    "    \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\",\n",
    "    \"HourlyAltimeterSetting\",\n",
    "    \"HourlyVisibility\",\n",
    "    \"HourlyStationPressure\",\n",
    "    \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyPrecipitation\",\n",
    "    \"HourlyCloudCoverage\",\n",
    "    \"HourlyCloudElevation\",\n",
    "    \"HourlyWindSpeed\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097daaac-61b2-4bf1-80ff-36f12033d0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical = []"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4093368127199234,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(SO) ph3_feature-engineering_5year-testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
