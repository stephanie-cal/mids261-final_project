{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0db5928-9c77-4e1d-a2ac-f43858d4d211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def checkpoint_dataset(dataset, file_path):\n",
    "#     # Create folder\n",
    "#     section = \"2\"\n",
    "#     number = \"2\"\n",
    "#     folder_path = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "#     dbutils.fs.mkdirs(folder_path)\n",
    "#     # Save df_weather as a parquet file\n",
    "#     dataset.write.parquet(f\"{folder_path}/{file_path}.parquet\")\n",
    "#     print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca31ebf3-241b-4b34-b298-84e3f1c46a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom Join Dataset - 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ab9809-79af-40a1-9adb-ce2273026d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b669106-ecd2-494e-8477-d8503cf43d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in custom joined data\n",
    "custom_joined_path = 'dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V3/custom_join_v3_1y.parquet'\n",
    "\n",
    "df = spark.read.parquet(custom_joined_path)\n",
    "\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "print(df.count())\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "924d73f8-797f-419e-a4c6-a2973ba7b1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Things to keep in mind\n",
    "- Predict two hours before\n",
    "- Remove all the delay columns\n",
    "- Are we only predicting departure delays or arrival delays also? For example, the pilot misses the landing, and has to circle back for 20 minutes. Should we solve for that? I don't think we should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d49ea99-57cc-4ba9-b438-7386aa04e939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessing / Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5999fa0d-91b2-4e26-a0ff-5133dd88042c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.cache() # cache joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8d68c2-a1d0-487b-8d7f-8fb65a48a2cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine date and scheduled departure time\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe271d1e-f7a8-4575-840c-5873bd34aaa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Split 3 month joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fd7fc9-335e-4473-9f1f-a0ffdbbab4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "TRAIN_SIZE = 0.70\n",
    "VALIDATION_SIZE = 0.10\n",
    "\n",
    "# REMOVE ALL CANCELLED FLIGHTS\n",
    "df = df.filter(F.col(\"CANCELLED\") != 1)\n",
    "\n",
    "df = df.sort('utc_timestamp')\n",
    "\n",
    "# Add row number based on timestamp order\n",
    "window = Window.orderBy('utc_timestamp')\n",
    "df = df.withColumn(\"row_num\", F.row_number().over(window))\n",
    "\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate split points\n",
    "train_end = int(total_rows * TRAIN_SIZE)\n",
    "validation_end = int(total_rows * (TRAIN_SIZE + VALIDATION_SIZE))  # 70% + 10%\n",
    "\n",
    "# Split based on row number\n",
    "train_df = df.filter(F.col(\"row_num\") <= train_end)\n",
    "validation_df = df.filter((F.col(\"row_num\") > train_end) & (F.col(\"row_num\") <= validation_end))\n",
    "test_df = df.filter(F.col(\"row_num\") > validation_end)\n",
    "\n",
    "# Drop the helper column\n",
    "train_df = train_df.drop(\"row_num\")\n",
    "validation_df = validation_df.drop(\"row_num\")\n",
    "test_df = test_df.drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f6c1ba-2796-4811-a921-6afd8ad45615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the last utc_timestamp from train_df\n",
    "last_flight_ts = train_df.agg(F.max(\"utc_timestamp\").alias(\"last_ts\")).collect()[0][\"last_ts\"]\n",
    "\n",
    "# Add a 2 hour gap\n",
    "gap_ts = F.timestamp_add(\"HOUR\", F.lit(2), F.lit(last_flight_ts))\n",
    "\n",
    "# Filter validation_df to keep everything after the gap timestamp\n",
    "# validation_after_gap_df = validation_df.filter(F.col(\"utc_timestamp\") > gap_ts)\n",
    "validation_df = validation_df.filter(F.col(\"utc_timestamp\") > gap_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b028b59-602f-406d-9192-de9224ef8674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4721fde3-e6e7-49b5-b0a0-e04b625d2010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if input(\"CAREFUL: You're about to write to DBFS. Type 'y' to continue.\") == \"y\":\n",
    "    checkpoint_dataset(train_df, \"1_year_custom_joined/raw_data/training_splits/train\")\n",
    "    checkpoint_dataset(validation_df, \"1_year_custom_joined/raw_data/training_splits/validation\")\n",
    "    checkpoint_dataset(test_df, \"1_year_custom_joined/raw_data/training_splits/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33656b4d-4319-4f22-a283-0326d462b666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### check checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7428a617-73d4-4405-b82c-7187c50bb863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/raw_data/training_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "772c5ce8-150f-4366-acb8-823a80ae2293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d6f9c3-47ea-4ff0-8ed4-67f891e00385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "month_or_year = \"3_month_custom_joined\"\n",
    "\n",
    "dataset_path = f\"{checkpoint_path}/{month_or_year}/raw_data/training_splits\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")\n",
    "test_df = spark.read.parquet(f\"{dataset_path}/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3aea631-5944-45ce-9096-2a1cacc346a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ignore weather rows with nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f39192-106c-4853-8367-a1d0a83404e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])\n",
    "\n",
    "validation_df = validation_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])\n",
    "\n",
    "test_df = test_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9ad4ff-0453-41cc-8c32-577b10bcdcdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## CRS_DEP_TIME is local time so we can use this feature \n",
    "## But in order to use it, we have to convert it to minutes since midnight\n",
    "## Otherwise the timing will be off b/c it's not true UTC\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"CRS_DEP_MINUTES\", \n",
    "    (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    \"CRS_DEP_MINUTES\", \n",
    "    (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"CRS_DEP_MINUTES\", \n",
    "    (F.floor(F.col(\"CRS_DEP_TIME\") / 100) * 60 + (F.col(\"CRS_DEP_TIME\") % 100))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b48fba-396a-4a0f-9280-78250938628d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Eng.\n",
    "\n",
    "#### Was the previous flight delayed? And by how much was the previous flight delayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96047694-161f-431a-9fdf-f9a11b3618eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.cache()\n",
    "validation_df = validation_df.cache()\n",
    "test_df = test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5a7658-38d6-4b8d-bab5-12780012133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_4h = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-14400, -7200) # 4 hours to 2 hours before\n",
    "\n",
    "train_df = train_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )\n",
    "validation_df = validation_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )\n",
    "\n",
    "test_df = test_df \\\n",
    "    .withColumn(\"origin_delays_4h\", F.count(F.when(F.col(\"DEP_DELAY_NEW\") > 15, 1)) \\\n",
    "        .over(window_4h)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88bc6f0-ba4d-4358-84c2-bf54aa9dfcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n",
    "    \n",
    "validation_df = validation_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n",
    "    \n",
    "test_df = test_df \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.lag(\"DEP_DELAY_NEW\", 1) \\\n",
    "        .over(Window.partitionBy(\"TAIL_NUM\") \\\n",
    "        .orderBy(\"utc_timestamp\"))) \\\n",
    "    .withColumn(\"prev_flight_delay_in_minutes\", F.when(F.col(\"prev_flight_delay_in_minutes\").isNull(), -1) \\\n",
    "        .otherwise(F.col(\"prev_flight_delay_in_minutes\"))) \\\n",
    "    .withColumn(\"prev_flight_delay\", F.when(F.col(\"prev_flight_delay_in_minutes\") > 15, 1) \\\n",
    "        .otherwise(F.lit(0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af24ebb-0fc2-43f9-83a1-5a64ad2ef18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Delay time for flights at departure locations over the past 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f50f7c-6df1-46b6-bc9e-2425dfde8da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_origin = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b62b4d0-e8b9-4a4d-8c07-b5ce60740deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Number of delayed flights at departure and carrier location over the last 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4028168e-a6bd-4a35-b476-ec7a0809f2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_origin_carrier = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\", \"OP_UNIQUE_CARRIER\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n",
    "\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cc01fa3-a466-4ad2-a2ab-613b1b24faff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] number of delays in route in the last 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f904db4-4a82-4605-babb-0026d37d950d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec504be-ec13-4ba4-8635-6cc8efce7b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_route = Window \\\n",
    "    .partitionBy(\"route\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_df = train_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_df = validation_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') \n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "test_df = test_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e36fafe1-655c-4323-bfda-c1246f631f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] - number of flights per day for one plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc32923f-0f9f-465a-be55-24d18076540a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_flights_24h = Window \\\n",
    "  .partitionBy(\"TAIL_NUM\", \"FL_DATE\") \\\n",
    "  .orderBy(F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")\n",
    "test_df = test_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b536a756-4f44-4c1a-9c2d-ae429367a689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] time between landed and scheduled flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f0dcf-57c5-4c1c-aa58-784daa759ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def hhmm_to_time_str(col):\n",
    "    padded = F.lpad(F.col(col).cast(\"string\"), 4, \"0\")\n",
    "    return F.concat_ws(\":\", padded.substr(1, 2), padded.substr(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f12936-a110-463f-90dd-94f906887b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df = train_df.withColumn(\n",
    "#     \"CRS_ARR_TIME_STR\",\n",
    "#     hhmm_to_time_str(\"ARR_TIME\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_STR\",\n",
    "#     hhmm_to_time_str(\"WHEELS_ON\")\n",
    "# )\n",
    "\n",
    "# train_df = train_df.withColumn(\n",
    "#     \"CRS_ARR_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"CRS_ARR_TIME_STR\", \"HH:mm\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"WHEELS_ON_STR\", \"HH:mm\")\n",
    "# )\n",
    "\n",
    "# train_df = train_df.withColumn(\n",
    "#     \"LANDING_TIME_DIFF_MINUTES\",\n",
    "#     F.coalesce(\n",
    "#         (\n",
    "#             (F.col(\"WHEELS_ON_TIMESTAMP\").cast(\"long\") - \n",
    "#              F.col(\"CRS_ARR_TIMESTAMP\").cast(\"long\")) / 60\n",
    "#         ),\n",
    "#         F.lit(0)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# validation_df = validation_df.withColumn(\n",
    "#     \"CRS_ARR_TIME_STR\",\n",
    "#     hhmm_to_time_str(\"ARR_TIME\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_STR\",\n",
    "#     hhmm_to_time_str(\"WHEELS_ON\")\n",
    "# )\n",
    "\n",
    "# validation_df = validation_df.withColumn(\n",
    "#     \"CRS_ARR_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"CRS_ARR_TIME_STR\", \"HH:mm\")\n",
    "# ).withColumn(\n",
    "#     \"WHEELS_ON_TIMESTAMP\",\n",
    "#     F.to_timestamp(\"WHEELS_ON_STR\", \"HH:mm\")\n",
    "# )\n",
    "\n",
    "# validation_df = validation_df.withColumn(\n",
    "#     \"LANDING_TIME_DIFF_MINUTES\",\n",
    "#     F.coalesce(\n",
    "#         (\n",
    "#             (F.col(\"WHEELS_ON_TIMESTAMP\").cast(\"long\") - \n",
    "#              F.col(\"CRS_ARR_TIMESTAMP\").cast(\"long\")) / 60\n",
    "#         ),\n",
    "#         F.lit(0)\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a41b91-86a5-4b39-9247-9f566e63bbd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_turnaround = Window \\\n",
    "    .partitionBy(\"TAIL_NUM\") \\\n",
    "    .orderBy(F.col(\"WHEELS_ON\").cast(\"long\")) \n",
    "\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"next_scheduled_dep_ts\", \n",
    "    F.lead(\"CRS_DEP_TIME\", 1).over(window_turnaround)\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    F.coalesce(\n",
    "        (F.col(\"next_scheduled_dep_ts\").cast(\"long\") - F.col(\"WHEELS_ON\").cast(\"long\")) / 60,\n",
    "        F.lit(-999) \n",
    "    )\n",
    ").drop(\"next_scheduled_dep_ts\")\n",
    "\n",
    "train_df.select(\"TAIL_NUM\", \"WHEELS_ON\", \"CRS_DEP_TIME\", \"LANDING_TIME_DIFF_MINUTES\").orderBy(\"TAIL_NUM\", \"WHEELS_ON\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6274962-9a22-4fa0-a388-eff57540f38d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validation_df = validation_df.withColumn(\n",
    "    \"next_scheduled_dep_ts\", \n",
    "    F.lead(\"CRS_DEP_TIME\", 1).over(window_turnaround)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    F.coalesce(\n",
    "        (F.col(\"next_scheduled_dep_ts\").cast(\"long\") - F.col(\"WHEELS_ON\").cast(\"long\")) / 60,\n",
    "        F.lit(-999) \n",
    "    )\n",
    ").drop(\"next_scheduled_dep_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b90248-0f5e-43ba-ab27-941faf666042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\n",
    "    \"next_scheduled_dep_ts\", \n",
    "    F.lead(\"CRS_DEP_TIME\", 1).over(window_turnaround)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    F.coalesce(\n",
    "        (F.col(\"next_scheduled_dep_ts\").cast(\"long\") - F.col(\"WHEELS_ON\").cast(\"long\")) / 60,\n",
    "        F.lit(-999) \n",
    "    )\n",
    ").drop(\"next_scheduled_dep_ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5b48dd8-cdd6-4c41-946e-ca93b38dd6b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Average Delay time by airport\n",
    "- by origin airport and by destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d81c599-5aa0-4c80-9600-8784f8f490e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_delay_by_airport_train = train_df.groupBy(\"DEST_AIRPORT_SEQ_ID\").agg(\n",
    "    F.avg(\"ARR_DELAY\").alias(\"AVG_ARR_DELAY\")\n",
    ")\n",
    "\n",
    "avg_delay_by_airport_val = validation_df.groupBy(\"DEST_AIRPORT_SEQ_ID\").agg(\n",
    "    F.avg(\"ARR_DELAY\").alias(\"AVG_ARR_DELAY\")\n",
    ")\n",
    "\n",
    "avg_delay_by_airport_test = test_df.groupBy(\"DEST_AIRPORT_SEQ_ID\").agg(\n",
    "    F.avg(\"ARR_DELAY\").alias(\"AVG_ARR_DELAY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22deb43-774c-4526-94b2-304b367f70d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df.select(\"DEST\", \"ARR_DELAY\", \"AVG_ARR_DELAY\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9657d8d4-253d-4884-b28d-770fd61bbc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "window_7d_origin = Window \\\n",
    "    .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\") \\\n",
    "    .orderBy(F.col(\"utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days (604800s) to -4 hours (14400s)\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'avg_delay_origin_7d_raw', \n",
    "    F.avg('ARR_DELAY').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'AVG_ARR_DELAY_ORIGIN', \n",
    "    F.coalesce(F.col('avg_delay_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_delay_origin_7d_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'avg_delay_origin_7d_raw', \n",
    "    F.avg('ARR_DELAY').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'AVG_ARR_DELAY_ORIGIN', \n",
    "    F.coalesce(F.col('avg_delay_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_delay_origin_7d_raw')\n",
    "\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'avg_delay_origin_7d_raw', \n",
    "    F.avg('ARR_DELAY').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'AVG_ARR_DELAY_ORIGIN', \n",
    "    F.coalesce(F.col('avg_delay_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_delay_origin_7d_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b81c2209-ed8b-4612-ac98-c598ad244aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Average taxi-out time by airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb15f98-3575-47f6-8ac9-3dbcd278e204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn(\n",
    "    'avg_taxi_out_origin_7d_raw', \n",
    "    F.avg('TAXI_OUT').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'AVG_TAXI_OUT_ORIGIN', \n",
    "    F.coalesce(F.col('avg_taxi_out_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_taxi_out_origin_7d_raw') \n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'avg_taxi_out_origin_7d_raw', \n",
    "    F.avg('TAXI_OUT').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'AVG_TAXI_OUT_ORIGIN', \n",
    "    F.coalesce(F.col('avg_taxi_out_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_taxi_out_origin_7d_raw')\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'avg_taxi_out_origin_7d_raw', \n",
    "    F.avg('TAXI_OUT').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'AVG_TAXI_OUT_ORIGIN', \n",
    "    F.coalesce(F.col('avg_taxi_out_origin_7d_raw'), F.lit(0))\n",
    ").drop('avg_taxi_out_origin_7d_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10343c24-d4e5-45d1-980a-9f15f0a39eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Is holiday?\n",
    "US Holidays only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0fa9ee0-332d-475e-b1b4-c99a13ba932f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ee3ff1-20d2-4231-9311-f6ae352894ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9c5388-7d93-4d7b-9453-a0c95a52c0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, to_date\n",
    "from datetime import date, timedelta\n",
    "\n",
    "holidays_2019 = {\n",
    "    date(2019, 1, 1),   # New Year's Day\n",
    "    date(2019, 1, 21),  # MLK Jr.'s Birthday\n",
    "    date(2019, 2, 18),  # Washington's Birthday\n",
    "    date(2019, 5, 27),  # Memorial Day\n",
    "    date(2019, 7, 4),   # Independence Day\n",
    "    date(2019, 9, 2),   # Labor Day\n",
    "    date(2019, 10, 14), # Columbus Day\n",
    "    date(2019, 11, 11), # Veterans Day\n",
    "    date(2019, 11, 28), # Thanksgiving Day\n",
    "    date(2019, 12, 25)  # Christmas Day\n",
    "}\n",
    "\n",
    "holidays_2019_str = [\n",
    "    d.strftime('%Y-%m-%d')\n",
    "    for d in holidays_2019\n",
    "]\n",
    "\n",
    "def check_holiday_window(flight_date, holidays_set, window_days=3):\n",
    "    \"\"\"Checks if a flight_date falls within a window_days radius of any holiday.\"\"\"\n",
    "    if flight_date is None:\n",
    "        return 0\n",
    "    \n",
    "    # Check current date and dates +/- window_days\n",
    "    for i in range(-window_days, window_days + 1):\n",
    "        target_date = flight_date + timedelta(days=i)\n",
    "        if target_date in holidays_set:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6935db-8245-4526-b267-c1a217ea2ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    'IS_HOLIDAY', \n",
    "    (when(col(\"FL_DATE\").isin(holidays_2019_str), 1).otherwise(0)).cast(\"integer\")\n",
    ")\n",
    "\n",
    "# validation\n",
    "validation_df = validation_df.withColumn(\n",
    "    'IS_HOLIDAY', \n",
    "    (when(col(\"FL_DATE\").isin(holidays_2019_str), 1).otherwise(0)).cast(\"integer\")\n",
    ")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    'IS_HOLIDAY', \n",
    "    (when(col(\"FL_DATE\").isin(holidays_2019_str), 1).otherwise(0)).cast(\"integer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c233b6-e91f-4863-882b-fb414ef6ab3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.select('IS_HOLIDAY').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f075443-77ca-416a-8366-102716e16191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Within 3 days of a holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f396a9a4-c303-4531-be1d-180802a0a3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get window\n",
    "is_in_holiday_window_udf = udf(\n",
    "    lambda x: check_holiday_window(x, holidays_2019, 3), \n",
    "    IntegerType()\n",
    ")\n",
    "\n",
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    \"FL_DATE_DT\", \n",
    "    to_date(col(\"FL_DATE\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    'IS_HOLIDAY_WINDOW', \n",
    "    is_in_holiday_window_udf(col(\"FL_DATE_DT\"))\n",
    ")\n",
    "\n",
    "train_df = train_df.drop(\"FL_DATE_DT\")\n",
    "\n",
    "# val\n",
    "validation_df = validation_df.withColumn(\n",
    "    \"FL_DATE_DT\", \n",
    "    to_date(col(\"FL_DATE\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "validation_df = validation_df.withColumn(\n",
    "    'IS_HOLIDAY_WINDOW', \n",
    "    is_in_holiday_window_udf(col(\"FL_DATE_DT\"))\n",
    ")\n",
    "\n",
    "validation_df = validation_df.drop(\"FL_DATE_DT\")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    \"FL_DATE_DT\", \n",
    "    to_date(col(\"FL_DATE\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    'IS_HOLIDAY_WINDOW', \n",
    "    is_in_holiday_window_udf(col(\"FL_DATE_DT\"))\n",
    ")\n",
    "\n",
    "test_df = test_df.drop(\"FL_DATE_DT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7872b5-ac92-45b5-b0ad-96e7db825682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df.select('IS_HOLIDAY_WINDOW').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e047651-2087-497d-b897-972fb052c32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73fd1abe-ee77-489c-93ba-a6c2ae8e7f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Airport Size\n",
    "Based on FAA Hub categories\n",
    "<br>\n",
    "hub category = airport's annual passenger boardings/US annual passenger boardings\n",
    "\n",
    "- [0] Large Hub (P-L)\tHandles 1.00% or more of total U.S. annual boardings.\n",
    "- [1] Medium Hub (P-M)\tHandles 0.25% to less than 1.00% of total U.S. annual boardings.\n",
    "- [2] Small Hub (P-S)\tHandles 0.05% to less than 0.25% of total U.S. annual boardings.\n",
    "- [3] Non-Hub Primary (P-N)\tHandles less than 0.05% but has at least 10,000 annual boardings.\n",
    "- [4] Non-Primary Commercial Service (CS)\tHas between 2,500 and 10,000 annual boardings.\n",
    "- [5] Other\tIncludes Reliever and General Aviation (GA) airports, or codes that were not valid airport identifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b8c039-ecd3-49ce-845f-172b6ecf3a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airport_classification_data = {\n",
    "    # Large Hub (0)\n",
    "    \"ATL\": 0, \"BOS\": 0, \"CLT\": 0, \"DCA\": 0, \"DEN\": 0, \"DFW\": 0, \"DTW\": 0, \"EWR\": 0, \n",
    "    \"FLL\": 0, \"IAD\": 0, \"IAH\": 0, \"JFK\": 0, \"LAS\": 0, \"LAX\": 0, \"LGA\": 0, \"MCO\": 0, \n",
    "    \"MIA\": 0, \"MSP\": 0, \"ORD\": 0, \"PHL\": 0, \"PHX\": 0, \"SAN\": 0, \"SEA\": 0, \"SFO\": 0, \n",
    "    \"SLC\": 0,\n",
    "\n",
    "    # Medium Hub (1)\n",
    "    \"ABQ\": 1, \"ANC\": 1, \"BNA\": 1, \"BWI\": 1, \"BUR\": 1, \"CVG\": 1, \"DAL\": 1, \"FAI\": 1, \n",
    "    \"GEG\": 1, \"HNL\": 1, \"HOU\": 1, \"IND\": 1, \"MCI\": 1, \"MDW\": 1, \"MEM\": 1, \"MSY\": 1, \n",
    "    \"OAK\": 1, \"OGG\": 1, \"ONT\": 1, \"PDX\": 1, \"PIT\": 1, \"RDU\": 1, \"RNO\": 1, \"RSW\": 1, \n",
    "    \"SJC\": 1, \"SJU\": 1, \"SNA\": 1, \"TPA\": 1, \"TUS\": 1, \"XNA\": 1,\n",
    "    \n",
    "    # Small Hub (2)\n",
    "    \"ABE\": 2, \"ABY\": 2, \"ACV\": 2, \"AGS\": 2, \"ALB\": 2, \"ALO\": 2, \"AMA\": 2, \"APN\": 2,\n",
    "    \"ART\": 2, \"ASE\": 2, \"ATW\": 2, \"AVP\": 2, \"AZO\": 2, \"BFL\": 2, \"BGM\": 2, \"BIL\": 2, \n",
    "    \"BIS\": 2, \"BJI\": 2, \"BLI\": 2, \"BOI\": 2, \"BQK\": 2, \"BRO\": 2, \"BTR\": 2, \"BTV\": 2, \n",
    "    \"BZN\": 2, \"CAE\": 2, \"CAK\": 2, \"CDC\": 2, \"CID\": 2, \"CIU\": 2, \"CLE\": 2, \"CLL\": 2, \n",
    "    \"CMH\": 2, \"CMX\": 2, \"CNY\": 2, \"COD\": 2, \"COS\": 2, \"COU\": 2, \"CPR\": 2, \"CRW\": 2, \n",
    "    \"CSG\": 2, \"CWA\": 2, \"CYS\": 2, \"DAB\": 2, \"DAY\": 2, \"DHN\": 2, \"DLH\": 2, \"DSM\": 2, \n",
    "    \"DRO\": 2, \"EAU\": 2, \"EGE\": 2, \"EKO\": 2, \"ELM\": 2, \"ELP\": 2, \"ERI\": 2, \"ESC\": 2, \n",
    "    \"EUG\": 2, \"EVV\": 2, \"EWN\": 2, \"EYW\": 2, \"FAR\": 2, \"FAT\": 2, \"FAY\": 2, \"FCA\": 2, \n",
    "    \"FLG\": 2, \"FSD\": 2, \"FSM\": 2, \"FWA\": 2, \"GCK\": 2, \"GFK\": 2, \"GJT\": 2, \"GNV\": 2, \n",
    "    \"GPT\": 2, \"GRB\": 2, \"GRI\": 2, \"GRK\": 2, \"GRR\": 2, \"GSO\": 2, \"GSP\": 2, \"GTF\": 2, \n",
    "    \"GUM\": 2, \"HA\": 2, \"HDN\": 2, \"HIB\": 2, \"HLN\": 2, \"HRL\": 2, \"HSV\": 2, \"HTS\": 2, \n",
    "    \"HVN\": 2, \"HYA\": 2, \"IDA\": 2, \"ITH\": 2, \"JAC\": 2, \"JAN\": 2, \"JMS\": 2, \"JNU\": 2, \n",
    "    \"KTN\": 2, \"LAN\": 2, \"LAR\": 2, \"LAW\": 2, \"LEX\": 2, \"LFT\": 2, \"LGB\": 2, \"LIH\": 2,\n",
    "    \"LIT\": 2, \"LNK\": 2, \"LRD\": 2, \"LSE\": 2, \"LWS\": 2, \"LYH\": 2, \"MAF\": 2, \"MBS\": 2, \n",
    "    \"MGM\": 2, \"MHK\": 2, \"MLI\": 2, \"MLU\": 2, \"MMH\": 2, \"MOB\": 2, \"MOT\": 2, \"MQT\": 2, \n",
    "    \"MSO\": 2, \"MTJ\": 2, \"MVY\": 2, \"OAJ\": 2, \"OGS\": 2, \"OME\": 2, \"OTH\": 2, \"OWB\": 2, \n",
    "    \"PAH\": 2, \"PBG\": 2, \"PGV\": 2, \"PHF\": 2, \"PIB\": 2, \"PIH\": 2, \"PIR\": 2, \"PLN\": 2, \n",
    "    \"PNS\": 2, \"PPG\": 2, \"PRC\": 2, \"PSC\": 2, \"PSG\": 2, \"PSM\": 2, \"PSP\": 2, \"PUB\": 2, \n",
    "    \"PVU\": 2, \"RDM\": 2, \"RHI\": 2, \"RKS\": 2, \"RST\": 2, \"ROW\": 2, \"SAF\": 2, \"SBP\": 2, \n",
    "    \"SCC\": 2, \"SCE\": 2, \"SGU\": 2, \"SHD\": 2, \"SIT\": 2, \"SLN\": 2, \"SMX\": 2, \"SPN\": 2, \n",
    "    \"SPI\": 2, \"STC\": 2, \"STS\": 2, \"SUX\": 2, \"SWF\": 2, \"SWO\": 2, \"TLH\": 2, \"TOL\": 2, \n",
    "    \"TRI\": 2, \"TVC\": 2, \"TXK\": 2, \"TYR\": 2, \"UIN\": 2, \"VLD\": 2, \"WRG\": 2, \"WYS\": 2, \n",
    "    \"YAK\": 2, \"YUM\": 2, \"RFD\": 2, \"LBE\": 2, \"DRT\": 2, \n",
    "    \n",
    "    # Non-Hub Primary (4)\n",
    "    # Note: Many smaller airports fluctuate between categories, placed here for the enumeration request.\n",
    "    \"ABR\": 4, \"ACK\": 4, \"AKN\": 4, \"AZA\": 4, \"BGM\": 4, \"BKG\": 4, \"BRW\": 4, \"CDV\": 4, \n",
    "    \"GTR\": 4, \"LBL\": 4, \"LCK\": 4, \"LWB\": 4, \"MEI\": 4, \"OGD\": 4, \"OME\": 4, \"OTH\": 4, \n",
    "    \"PIE\": 4, \"PVU\": 4, \"RFD\": 4, \"RHI\": 4, \"SLN\": 4, \"STT\": 4, \"SUN\": 4, \"SWO\": 4, \n",
    "    \"TTN\": 4, \"VEL\": 4, \"WRG\": 4, \"YAK\": 4, \"YUM\": 4,\n",
    "    \n",
    "    # Non-Primary Commercial Service (5)\n",
    "    \"ATY\": 5, \"BFF\": 5, \"BTM\": 5, \"CIU\": 5, \"DBQ\": 5, \"DLG\": 5, \"GST\": 5, \"IMT\": 5, \n",
    "    \"INL\": 5, \"LWF\": 5, \"MQT\": 5, \"PIR\": 5, \"PSM\": 5, \"TOL\": 5, \n",
    "\n",
    "    # Other/General Aviation/Reliever (6)\n",
    "    \"BRD\": 6, \"CMI\": 6, \"HHH\": 6, \"HYS\": 6, \"LWL\": 6, \"MMH\": 6, \"PAE\": 6, \"PSE\": 6, \n",
    "    \"PSP\": 6, \"RKS\": 6, \"USA\": 6, \"VEL\": 6,\n",
    "    \n",
    "    # Missing/Uncertain Codes (Set to 6 for consistency)\n",
    "    \"9E\": 6, \"EV\": 6, \"MQ\": 6, \"NK\": 6, \"OO\": 6, \"WN\": 6, \"YV\": 6, \"YX\": 6, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378f6b7f-a216-43bc-8efd-2500b53ca0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def map_airport_class(airport_code):\n",
    "    \"\"\"Looks up the hub class for a given airport code, defaulting to 6.\"\"\"\n",
    "    global airport_classification_data  # Good practice if running in Spark\n",
    "    return airport_classification_data.get(airport_code, 6)\n",
    "\n",
    "hub_class_udf = udf(map_airport_class, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1abc762-fa81-4c90-bfb5-f8e9d79a62e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    'AIRPORT_HUB_CLASS', \n",
    "    hub_class_udf(col(\"ORIGIN\"))\n",
    ")\n",
    "\n",
    "# validation\n",
    "validation_df = validation_df.withColumn(\n",
    "    'AIRPORT_HUB_CLASS', \n",
    "    hub_class_udf(col(\"ORIGIN\"))\n",
    ")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    'AIRPORT_HUB_CLASS', \n",
    "    hub_class_udf(col(\"ORIGIN\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab7c9c8c-3764-49ac-a1a5-bb3ba299c6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Airline Sentiment\n",
    "- get list of unique airlines\n",
    "- rate airline sentiment/perception on liechert scale 1-5 in some llm\n",
    "  - based on:\n",
    "    - on-time performance\n",
    "    - cancellation rate\n",
    "    - involuntary denied boarding rate\n",
    "    - value\n",
    "    - level of trust\n",
    "    - mishandled baggage rate\n",
    "    - pre/post flight experience\n",
    "    - boarding process\n",
    "    - flight crew service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb2dc13b-bb85-486c-922b-78459b87e4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airline_sentiment_data = {\n",
    "    \"UA\": {\n",
    "        \"rating\": 3.8,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"NK\": {\n",
    "        \"rating\": 1.8,\n",
    "        \"category\": 3\n",
    "    },\n",
    "    \"AA\": {\n",
    "        \"rating\": 3.5,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"EV\": {\n",
    "        \"rating\": 3.0,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"B6\": {\n",
    "        \"rating\": 4.2,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"DL\": {\n",
    "        \"rating\": 4.5,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"OO\": {\n",
    "        \"rating\": 3.0,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"F9\": {\n",
    "        \"rating\": 2.0,\n",
    "        \"category\": 3\n",
    "    },\n",
    "    \"YV\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"MQ\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"OH\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"HA\": {\n",
    "        \"rating\": 4.0,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"G4\": {\n",
    "        \"rating\": 2.5,\n",
    "        \"category\": 3\n",
    "    },\n",
    "    \"YX\": {\n",
    "        \"rating\": 2.8,\n",
    "        \"category\": 2\n",
    "    },\n",
    "    \"AS\": {\n",
    "        \"rating\": 4.3,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"WN\": {\n",
    "        \"rating\": 4.0,\n",
    "        \"category\": 1\n",
    "    },\n",
    "    \"9E\": {\n",
    "        \"rating\": 3.0,\n",
    "        \"category\": 2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ae3e3d-eb71-4925-818d-2df702b96bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def map_rating(carrier_code):\n",
    "    \"\"\"Looks up the sentiment rating, defaulting to a neutral 3.0 if not found.\"\"\"\n",
    "    # The rating is FloatType\n",
    "    return airline_sentiment_data.get(carrier_code, {'rating': 3.0}).get('rating')\n",
    "\n",
    "def map_category(carrier_code):\n",
    "    \"\"\"Looks up the airline category, defaulting to 2 (Regional) if not found.\"\"\"\n",
    "    # The category is IntegerType (1=Major, 2=Regional, 3=ULCC)\n",
    "    return airline_sentiment_data.get(carrier_code, {'category': 2}).get('category')\n",
    "\n",
    "rating_udf = udf(map_rating, FloatType())\n",
    "category_udf = udf(map_category, IntegerType())\n",
    "\n",
    "carrier_col_name = \"OP_UNIQUE_CARRIER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885dd0c4-94fa-4a84-b750-f4fc622088a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_df = train_df.withColumn(\n",
    "    'RATING', \n",
    "    rating_udf(col(carrier_col_name))\n",
    ").withColumn(\n",
    "    'AIRLINE_CATEGORY',\n",
    "    category_udf(col(carrier_col_name))\n",
    ")\n",
    "print(\"Train DataFrame updated with RATING and AIRLINE_CATEGORY.\")\n",
    "\n",
    "# val\n",
    "validation_df = validation_df.withColumn(\n",
    "    'RATING', \n",
    "    rating_udf(col(carrier_col_name))\n",
    ").withColumn(\n",
    "    'AIRLINE_CATEGORY',\n",
    "    category_udf(col(carrier_col_name))\n",
    ")\n",
    "print(\"Validation DataFrame updated with RATING and AIRLINE_CATEGORY.\")\n",
    "\n",
    "# test\n",
    "test_df = test_df.withColumn(\n",
    "    'RATING', \n",
    "    rating_udf(col(carrier_col_name))\n",
    ").withColumn(\n",
    "    'AIRLINE_CATEGORY',\n",
    "    category_udf(col(carrier_col_name))\n",
    ")\n",
    "print(\"Test DataFrame updated with RATING and AIRLINE_CATEGORY.\")\n",
    "\n",
    "# Example of how to check the new columns:\n",
    "train_df.select(carrier_col_name, \"RATING\", \"AIRLINE_CATEGORY\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb3a684-ce2b-404b-b333-374a90496d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Feature] Seasonality\n",
    "- for 1 year, is summer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a05d3f9a-5fbc-460b-92b7-3049ecca28d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81ed40f-ee65-4869-a4af-b7506a7b9cc4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763424469559}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_counts = validation_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in validation_df.columns])\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4f00cf-6cf1-4fd8-8978-eeffabd98c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checkpoint results with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca18eba0-7cbc-4be0-98d1-b1535e1decce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/feature_eng_ph3/training_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720ce5e9-f62d-41dd-9305-427dfe0ca5d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if input(\"CAREFUL: You're about to write to DBFS. Type 'y' to continue.\") == \"y\":\n",
    "    checkpoint_dataset(train_df, f\"{month_or_year}/feature_eng_ph3/training_splits/train\")\n",
    "    checkpoint_dataset(validation_df, f\"{month_or_year}/feature_eng_ph3/training_splits/validation\")\n",
    "    checkpoint_dataset(test_df, f\"{month_or_year}/feature_eng_ph3/training_splits/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce3464d9-fea8-421b-a750-e1c2d6c1607d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check data checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8246c2e7-8165-4baa-a468-1196ad262922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb8b6a2-ac88-4803-9539-4b6ebd201bd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "dataset_path = f\"{checkpoint_path}/1_year_custom_joined/feature_eng_ph3/training_splits/\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "check_train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "check_validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667728ad-db3b-4661-93fb-52c943cdaece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check_train_df.columns == check_validation_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4b7a6f-919a-435c-9796-c5e8cdfadffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in check_train_df.columns:\n",
    "    if col not in check_validation_df.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769af6fd-5d02-4dc8-a0b5-7e2dbf0c2324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(check_train_df)\n",
    "# display(check_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d01de2a-03be-444e-aef6-19e7ae8d5408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check_train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2caf0a39-aaef-44fd-97a2-3c314d1adbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "move to modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f1e40e-fcbd-4b5c-a11d-c2742e1bfb4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4093368127199489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) (SO) ph3_feature-engineering_3_month",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
