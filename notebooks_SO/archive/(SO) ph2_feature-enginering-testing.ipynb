{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a55b6d72-d489-40ec-be62-8e2841189ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# feature engineering sandbox\n",
    "\n",
    "Feature eng ideas:\n",
    "- Number of delayed flights in departure and arrival location (total or 4 hours before, 6 hours before, etc.)\n",
    "- Number of delays in the route in the last 30 days\n",
    "- Number of flights plane has flown that day\n",
    "- Total number of flights plan has flown until a certain time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e428154a-318a-45de-88a1-b903e475185c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b661aad0-e655-467d-b6be-78d990386f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PIN_THREAD'] = 'false'\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8806642-f739-47a7-8b6b-49bfcbd9a53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d58ba87-e89b-4909-b384-16a361813b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86d06817-4633-48de-801d-edaf9a69815b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9e194d-0380-45bc-84ea-1b690c469078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0b7816-564c-4a2d-b535-a5c0fc61b110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EDA - training custom join with graph features, 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c78637-49da-43ad-9ee1-e05d273b2e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/feature_eng_ph3/training_splits/train.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e74368c-a7eb-4ced-94dd-1294499d0e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "month_or_year = \"1_year_custom_joined\"\n",
    "\n",
    "# 1_year_custom_joined/graph_feature_splits\n",
    "dataset_path = f\"{checkpoint_path}/{month_or_year}/graph_feature_splits/train\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "train_df = spark.read.parquet(f\"{dataset_path}\")\n",
    "# validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")\n",
    "# test_df = spark.read.parquet(f\"{dataset_path}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef647ae9-cb3a-4f74-97a4-eca19d9b303a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109f7d9d-06a4-4dfe-b539-fdfce024d5b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1474703 # seattle\n",
    "# 1288903 # las vegas\n",
    "# 1289208 # LA\n",
    "# 1402702 # Palm Beach, FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad9b29b-7605-4143-9d2f-57f012733120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757fd65d-aca2-4c50-bb3e-a14a416d2fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# graph in_betweeness\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. AGGREGATION (PySpark): Count the frequency of each unique in_degree value\n",
    "# This prepares the data for the histogram/bar chart.\n",
    "in_degree_counts_df = train_df.groupBy(\"in_degree\").count().orderBy(\"in_degree\")\n",
    "\n",
    "# 2. COLLECTION: Move the aggregated data to the driver for plotting\n",
    "# NOTE: Ensure the resulting table is small enough for your driver memory.\n",
    "# If in_degree has too many unique values, you might need to filter first.\n",
    "in_degree_counts_pd = in_degree_counts_df.toPandas()\n",
    "\n",
    "# 3. VISUALIZATION (Matplotlib): Create the Bar Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot in_degree (x-axis) vs. count (y-axis)\n",
    "plt.bar(in_degree_counts_pd['in_degree'], in_degree_counts_pd['count'], color='#5B9BD5', width=0.8)\n",
    "\n",
    "# Applying a log scale to the y-axis is crucial for visualizing the skewness\n",
    "plt.yscale('log') \n",
    "\n",
    "plt.title('Distribution of Airport In-Degree', fontsize=16)\n",
    "plt.xlabel('In-Degree (Number of Unique Incoming Routes)', fontsize=12)\n",
    "plt.ylabel('Count of Airports (Log Scale)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show() # Use plt.savefig('in_degree_histogram.png') in a production environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2fb2338-3e76-48f9-b32a-2016a9ef2f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Page Rank, Choropleth Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e774d0c2-9550-4a8b-96e9-88a678ac0db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate the PageRank by state. Use the mean to get a representative value.\n",
    "page_rank_by_state_df = train_df.groupBy(\"ORIGIN_STATE_ABR\").agg(\n",
    "    F.mean(\"page_rank\").alias(\"avg_page_rank\")\n",
    ").withColumnRenamed(\"ORIGIN_STATE_ABR\", \"state_abbr\")\n",
    "\n",
    "# This aggregated DataFrame is small enough to collect and visualize.\n",
    "page_rank_pd = page_rank_by_state_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "badd2756-d21e-4c3a-9259-e67b09acd64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Assuming the data collected in step 1 is named page_rank_pd\n",
    "\n",
    "fig = px.choropleth(\n",
    "    page_rank_pd, \n",
    "    locations='state_abbr', \n",
    "    locationmode=\"USA-states\", \n",
    "    color='avg_page_rank',\n",
    "    scope=\"usa\",\n",
    "    color_continuous_scale=\"Viridis\", # Choose a color scale\n",
    "    title='Average Airport PageRank by State (Network Influence)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71fbebee-408b-4498-a046-fdb221b773f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a84cf5a-db0f-438a-b0bf-eaff90ec3568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_data_df = train_df.select(\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"page_rank\",\n",
    "    \"out_degree\",\n",
    "    \"weighted_out_degree\",\n",
    "    \"closeness\",\n",
    "    \"betweenness\",\n",
    "    \"avg_origin_dep_delay\"\n",
    ").distinct() # Use distinct to ensure one row per airport/key\n",
    "\n",
    "# Collect data to Pandas for plotting\n",
    "plot_data_pd = plot_data_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7954b867-6441-4cc7-8c93-f24af1901ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Scatter Plot 1: PageRank vs. Average Departure Delay ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    plot_data_pd['page_rank'], \n",
    "    plot_data_pd['avg_origin_dep_delay'], \n",
    "    alpha=0.6, \n",
    "    color='darkred',\n",
    "    s=20 # Set size\n",
    ")\n",
    "\n",
    "plt.title('Network Influence (PageRank) vs. Average Departure Delay', fontsize=14)\n",
    "plt.xlabel('PageRank (Network Influence)', fontsize=12)\n",
    "plt.ylabel('Average Origin Departure Delay (Minutes)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ea1bd7-6e1f-48fc-a997-61445778bc4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Scatter Plot 2: Unique Routes vs. Traffic Volume ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    plot_data_pd['out_degree'], \n",
    "    plot_data_pd['weighted_out_degree'], \n",
    "    alpha=0.6, \n",
    "    color='darkblue',\n",
    "    s=20\n",
    ")\n",
    "\n",
    "plt.title('Unique Outbound Routes vs. Total Outbound Traffic Volume', fontsize=14)\n",
    "plt.xlabel('Out-Degree (Number of Unique Destinations)', fontsize=12)\n",
    "plt.ylabel('Weighted Out-Degree (Total Flights)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656a3762-bd96-4574-9bb3-a8809f80a800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Scatter Plot 3: Closeness vs. Betweenness Centrality ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    plot_data_pd['closeness'], \n",
    "    plot_data_pd['betweenness'], \n",
    "    alpha=0.6, \n",
    "    color='forestgreen',\n",
    "    s=20\n",
    ")\n",
    "\n",
    "plt.title('Closeness vs. Betweenness Centrality', fontsize=14)\n",
    "plt.xlabel('Closeness Centrality (Efficiency)', fontsize=12)\n",
    "plt.ylabel('Betweenness Centrality (Bridging Role)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db6d2eea-6ea3-43a1-9396-f0e6063b7a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### KDE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ef79fa-57a9-4762-8151-f9034d526e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# and joined to the main flight DataFrame, 'df'.\n",
    "plot_data_df = train_df.select(\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"avg_origin_dep_delay\"\n",
    ").distinct() # Use distinct to ensure one row per airport\n",
    "\n",
    "# Collect the data to a Pandas DataFrame for visualization\n",
    "plot_data_pd = plot_data_df.toPandas()\n",
    "\n",
    "# 4. Create the KDE Plot using Seaborn/Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the KDE (Kernel Density Estimate)\n",
    "sns.kdeplot(\n",
    "    data=plot_data_pd, \n",
    "    x='avg_origin_dep_delay', \n",
    "    fill=True, \n",
    "    alpha=0.6, \n",
    "    color='purple',\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "plt.title('KDE of Average Origin Departure Delay by Airport', fontsize=16)\n",
    "plt.xlabel('Average Departure Delay (Minutes)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cc988e-d83a-4713-a373-6c0157835078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_date = train_df.select(F.min(\"FL_DATE\")).collect()[0][0]\n",
    "\n",
    "# --- 1. Define Filter Criteria ---\n",
    "\n",
    "# Find the earliest date in the dataset (or choose any specific date)\n",
    "first_date = train_df.select(F.date_trunc('day', F.min(\"utc_timestamp\")).alias(\"day\")).collect()[0][0]\n",
    "\n",
    "# Find the ID of the airport with the maximum out_degree (likely a major hub)\n",
    "# single_airport_id = train_df.groupBy(\"ORIGIN_AIRPORT_SEQ_ID\").agg(\n",
    "#     F.max(\"out_degree\").alias(\"max_out_degree\")\n",
    "# ).orderBy(F.desc(\"max_out_degree\")).select(\"ORIGIN_AIRPORT_SEQ_ID\").limit(1).collect()[0][0]\n",
    "single_airport_id = 1295304 # ny\n",
    "\n",
    "# 1323202 #chicago\n",
    "# 1379608 # oakland\n",
    "# 1295304 # ny\n",
    "# 1402702 # Palm Beach, FL\n",
    "# 1474703 # seattle\n",
    "# 1288903 # las vegas\n",
    "# 1289208 # LA\n",
    "\n",
    "# --- 2. Filter, Process, and Select Data ---\n",
    "\n",
    "plot_data_df = train_df.filter(\n",
    "    # Filter by the single date AND the single location\n",
    "    (F.date_trunc('day', F.col(\"utc_timestamp\")) == F.lit(first_date)) &\n",
    "    (F.col(\"ORIGIN_AIRPORT_SEQ_ID\") == single_airport_id)\n",
    ").select(\n",
    "    \"utc_timestamp\",\n",
    "    \"HourlyDryBulbTemperature\",\n",
    "    \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\"\n",
    ").withColumn(\n",
    "    # Create the 'hour' column for the X-axis\n",
    "    \"hour\", \n",
    "    F.hour(F.col(\"utc_timestamp\"))\n",
    ").orderBy(\n",
    "    \"hour\"\n",
    ").distinct() \n",
    "\n",
    "# --- 3. Convert for Visualization ---\n",
    "\n",
    "# Collect the clean, 24-row dataset to Pandas\n",
    "weather_data_pd = plot_data_df.toPandas()\n",
    "\n",
    "print(f\"Data prepared for Airport ID: {single_airport_id} on date: {first_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b50f3df-f284-450f-a5b6-f37897a12735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_weather_diurnal_cycle_scatter(weather_data_pd):\n",
    "    \"\"\"\n",
    "    Generates a dual-axis plot showing only the scatter points \n",
    "    for the diurnal cycle of temperature and relative humidity.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # --- Primary Y-Axis (Temperature) ---\n",
    "    color_temp = 'tab:red'\n",
    "    ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "    ax1.set_ylabel('Temperature (F) (Dry Bulb / Dew Point)', color=color_temp, fontsize=12)\n",
    "\n",
    "    # Plot Dry Bulb Temperature (ONLY SCATTER)\n",
    "    ax1.scatter(weather_data_pd['hour'], weather_data_pd['HourlyDryBulbTemperature'], \n",
    "                color=color_temp, marker='o', label='Dry Bulb Temp.')\n",
    "\n",
    "    # Plot Dew Point Temperature (ONLY SCATTER)\n",
    "    ax1.scatter(weather_data_pd['hour'], weather_data_pd['HourlyDewPointTemperature'], \n",
    "                color='red', marker='x', label='Dew Point Temp.')\n",
    "\n",
    "    ax1.tick_params(axis='y', labelcolor=color_temp)\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # --- Secondary Y-Axis (Relative Humidity) ---\n",
    "    ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "    color_humidity = 'tab:blue'\n",
    "    ax2.set_ylabel('Relative Humidity (%)', color=color_humidity, fontsize=12)\n",
    "\n",
    "    # Plot Relative Humidity (ONLY SCATTER)\n",
    "    ax2.scatter(weather_data_pd['hour'], weather_data_pd['HourlyRelativeHumidity'], \n",
    "                color=color_humidity, marker='s', label='Relative Humidity')\n",
    "\n",
    "    ax2.tick_params(axis='y', labelcolor=color_humidity)\n",
    "\n",
    "    # --- Final Touches ---\n",
    "    plt.title('Hourly Weather Data (New York, NY - Jan 1, 2019)', fontsize=16)\n",
    "    fig.tight_layout() \n",
    "    \n",
    "    # Combine legends from both axes\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    # Display or save the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_weather_diurnal_cycle_scatter(weather_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ebdc05-6d2c-4d50-8164-2d6d623de446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd168eed-ce09-4283-b155-9e2fee4d396d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae2a6ff-b9af-4f0c-ba08-9a0a93d190c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "custom_join_3m_path = \"dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V2/custom_join_v2_3m.parquet\"\n",
    "custom_join_1y_path ='dbfs:/mnt/mids-w261/daniel_costa@berkeley.edu/Custom_Joins/V2/custom_join_v2_1y.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60014901-be1b-4d62-9bc2-1c1462e7a716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_3m = spark.read.parquet(custom_join_3m_path)\n",
    "\n",
    "join_data_3m_df = join_data_3m.cache()\n",
    "\n",
    "# drop null flight_uid\n",
    "join_data_3m_df = join_data_3m.dropna(subset=['flight_uid'])\n",
    "display(join_data_3m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3a42c1-2ccf-4f85-9340-1df1ff675676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_1y = spark.read.parquet(custom_join_1y_path)\n",
    "\n",
    "join_data_1y_df = join_data_1y.cache()\n",
    "\n",
    "# drop null flight_uid\n",
    "join_data_1y_df = join_data_1y.dropna(subset=['flight_uid'])\n",
    "display(join_data_1y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b431a0-b4bc-4453-b6bd-929a5c64ab4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add utc time for departure date\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    \"crs_dep_utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f1c8a3-a3a0-46af-be86-c26d279f4e1d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763342614917}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(join_data_3m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8b5d26-164f-4f35-85b6-33aabb04e35a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add utc time for departure date\n",
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    \"crs_dep_utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811ccea2-fc89-4067-9443-d0ab9f743185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a66ef77-00ca-4e7c-8e0f-0c222ae0d062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Drop some hourly fields for now from weather columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14954328-1cc1-4c56-b610-38d1a0a131cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_3m_df = join_data_3m_df.dropna(subset=[\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a616205-7c8a-4d75-b4d6-87e420d48aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filter cancelled flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff63b40e-0148-4242-a2f3-de5a342931e9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763391829544}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_3m_df = join_data_3m_df.filter(F.col(\"CANCELLED\") != 1)\n",
    "display(join_data_3m_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1554e9d-af07-4415-94fc-1633e20c5bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for nulls\n",
    "for column_name in join_data_3m_df.columns:\n",
    "    print(f\"{column_name} ------> {join_data_3m_df.filter(F.col(column_name).isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2daf0da-1f41-4ec3-90f7-cb6c897e44e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Feature eng ideas:\n",
    "\n",
    "- Number of delayed flights in departure location (total or 4 hours before, 6 hours before, etc.)\n",
    "  - per airline?\n",
    "- Number of delays in the route in the last 30 days\n",
    "- Number of flights plane has flown that day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d43a6be-5bf2-484f-b5ac-bf6859ea5cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature - number of delayed flights at depature location over the last 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eceeccbe-5240-4503-ad21-5c8395634a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, asc\n",
    "\n",
    "# add column for total flight delay in the last 7 days for each origin\n",
    "window_7d_origin = window_4h = Window \\\n",
    "    .partitionBy(\"ORIGIN\") \\\n",
    "    .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n",
    "# display(join_data_3m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9c3ca8-6eb6-4386-9b3b-cc9fe789ff27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply window to 1y data\n",
    "\n",
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n",
    "display(join_data_1y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee14b1aa-af67-4f0d-852e-ff09c83440fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature - number of delayed flights at depature and carrier location over the last 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323825a5-0413-4347-a82d-3cc34945d414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_origin_carrier = window_4h = Window \\\n",
    "    .partitionBy(\"ORIGIN\", \"OP_UNIQUE_CARRIER\") \\\n",
    "    .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n",
    "\n",
    "# display(join_data_3m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3be84c5-0f55-4b7f-b555-0e64acc37fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n",
    "\n",
    "# display(join_data_1y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74b453b2-6adb-4752-bc6e-84eb13bbc5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature - Number of delays in the route in the last 7 days\n",
    "- route: origin to destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c4bcffe-b254-4785-9f37-9da6613fd16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")\n",
    "\n",
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f33370-0ad3-4293-a42f-befb77c0d3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_route = window_4h = Window \\\n",
    "    .partitionBy(\"route\") \\\n",
    "    .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') \n",
    "\n",
    "# display(join_data_3m_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dcf95cd-c6a0-4aa0-9a10-659ca26964f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') \n",
    "\n",
    "# display(join_data_1y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10f8fdb3-ceb4-4eca-804b-c812d824d9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature - number of flights per day for one plane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630a9f2e-a2f4-46aa-a82d-c21c7198702f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_flights_24h = Window \\\n",
    "  .partitionBy(\"TAIL_NUM\", \"FL_DATE\") \\\n",
    "  .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\"))\n",
    "\n",
    "join_data_3m_df = join_data_3m_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")\n",
    "\n",
    "# display(join_data_3m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47fc67d-4259-4bac-8403-3e8e848d136f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"route\":101,\"delay_origin_7d_rows\":205},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763382001282}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data_1y_df = join_data_1y_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")\n",
    "\n",
    "display(join_data_1y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db037fe3-3977-4847-a7e5-61fedce2b102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for nulls\n",
    "for column_name in join_data_3m_df.columns:\n",
    "    print(f\"{column_name} ------> {join_data_3m_df.filter(F.col(column_name).isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ed93c0-3525-4f95-93b2-b0b50b273551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get Splits from checkpoint - 3 month data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa4e3f2c-11f8-4b97-a827-2061301e4455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "dataset_path = f\"{checkpoint_path}/3_month_custom_joined/raw_data/training_splits\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a9e980-18ba-452e-9173-a53f8022b168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_3m_df = train_df.cache()\n",
    "validation_3m_df = validation_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d2e316e-94d9-4a00-864e-99bb20a888cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Convert departure time to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d858cd-5deb-4b7d-af59-47897313f7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add utc time for departure date\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    \"crs_dep_utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    \"crs_dep_utc_timestamp\",\n",
    "    F.to_timestamp(\n",
    "        F.concat(\n",
    "            F.col(\"FL_DATE\"),\n",
    "            F.lit(\" \"),\n",
    "            F.lpad(F.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
    "        ),\n",
    "        \"yyyy-MM-dd HHmm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412ec092-2587-44d4-be01-179d12f6aba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### feature - total delay time for flights at departure locations over the past 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f291b392-a0bb-413b-a1e1-b3f1db3a25cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# incorporate features in splits\n",
    "window_7d_origin = Window \\\n",
    "    .partitionBy(\"ORIGIN\") \\\n",
    "    .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    'delay_origin_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    'delay_origin_7d', \n",
    "    F.coalesce(F.col('delay_origin_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_origin_7d_sum_raw') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4129bc69-68c4-4b84-a27b-9df747cd3a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature - number of delayed flights at depature and carrier location over the last 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51902f6e-d8b2-46cb-a218-901180b991b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_origin_carrier = Window \\\n",
    "    .partitionBy(\"ORIGIN\", \"OP_UNIQUE_CARRIER\") \\\n",
    "    .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n",
    "\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    'delay_origin_carrier_7d_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_origin_carrier)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    'delay_origin_carrier_7d', \n",
    "    F.coalesce(F.col('delay_origin_carrier_7d_raw'), F.lit(0))\n",
    ").drop('delay_origin_carrier_7d_raw') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80584555-c073-4f14-a641-b87c79fba801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature - number of delays in route in the last 7 days\n",
    "- route: origin to destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93466628-0ee0-4f1f-afee-a67e9126c627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_3m_df = train_3m_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")\n",
    "\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "  \"route\",\n",
    "  F.concat(F.col(\"ORIGIN\"), F.lit(\"-\"), F.col(\"DEST\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be15282-f109-431b-a0f2-4be5c2ef9286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_7d_route = Window \\\n",
    "    .partitionBy(\"route\") \\\n",
    "    .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\")) \\\n",
    "    .rangeBetween(-604800, -14400) # -7 days, -4 hours\n",
    "\n",
    "\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') \n",
    "\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    'delay_route_7d_sum_raw', \n",
    "    F.sum('DEP_DELAY_NEW').over(window_7d_route)\n",
    ")\n",
    "\n",
    "# Handle the nulls by coalescing the raw feature with 0\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    'delay_route_7d', \n",
    "    F.coalesce(F.col('delay_route_7d_sum_raw'), F.lit(0))\n",
    ").drop('delay_route_7d_sum_raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ac2f66-5b3a-4c62-94bb-2c7c4717c97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature - number of flights per day for one plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfb2bbf-bb40-46db-8e6c-8042ea00b09e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_flights_24h = Window \\\n",
    "  .partitionBy(\"TAIL_NUM\", \"FL_DATE\") \\\n",
    "  .orderBy(F.col(\"crs_dep_utc_timestamp\").cast(\"long\"))\n",
    "\n",
    "train_3m_df = train_3m_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")\n",
    "\n",
    "validation_3m_df = validation_3m_df.withColumn(\n",
    "    'flight_count_24h', \n",
    "    F.count(\"*\").over(window_flights_24h)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6aaa001-0ad2-4cd5-b148-c0917f51fca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for nulls\n",
    "for column_name in train_3m_df.columns:\n",
    "    print(f\"{column_name} ------> {train_3m_df.filter(F.col(column_name).isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93dd70b7-f870-4977-86a5-cfa689fe38b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2c84d2b-06d0-458e-b89a-d75963b24f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428ba457-6377-42da-bdc8-73043d2d9782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get baseline columns\n",
    "\n",
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    # \"CRS_ELAPSED_TIME\",\n",
    "    # \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"crs_dep_utc_timestamp\",\n",
    "    # \"prev_flight_delay_in_minutes\",\n",
    "    # \"prev_flight_delay\",\n",
    "    # \"origin_delays_4h\",\n",
    "    'HourlyDryBulbTemperature',\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed'  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0133337b-16b4-49e6-910a-6dc0b1637e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2622f506-9ccf-4135-ae79-4e4d8eb5b1e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"tail_num_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        'HourlyDryBulbTemperature',\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'  \n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937a5e10-d766-4585-9190-503845fc2482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# linear regression baseline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.spark.autolog()\n",
    "with mlflow.start_run(run_name=\"lr - weather baseline 3m\"):\n",
    "    MODEL_NAME = \"LR_WEATHER_BASELINE_3M\"\n",
    "\n",
    "    linear_reg = LinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"DEP_DELAY_NEW\",\n",
    "        # Linear Regression has different parameters than Random Forest\n",
    "        maxIter=10, \n",
    "        regParam=0.3\n",
    "    )\n",
    "\n",
    "    # rf = RandomForestRegressor(\n",
    "    #     featuresCol=\"features\",  \n",
    "    #     labelCol=\"DEP_DELAY_NEW\",   \n",
    "    #     numTrees=20,\n",
    "    #     maxDepth=10\n",
    "    # )\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(stages=[\n",
    "        carrier_indexer, origin_indexer, dest_indexer, tail_num_indexer,\n",
    "        carrier_encoder, origin_encoder, dest_encoder, tail_num_encoder,\n",
    "        assembler,\n",
    "        linear_reg\n",
    "        # rf\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(train_3m_df)\n",
    "    training_predictions = model.transform(train_3m_df)\n",
    "    validation_predictions = model.transform(validation_3m_df)\n",
    "\n",
    "    mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_t = mae_evaluator.evaluate(training_predictions)\n",
    "    mae_v = mae_evaluator.evaluate(validation_predictions)\n",
    "    # Calculate RMSE\n",
    "    rmse_t = rmse_evaluator.evaluate(training_predictions)\n",
    "    rmse_v = rmse_evaluator.evaluate(validation_predictions)\n",
    "\n",
    "    signature = infer_signature(train_df, training_predictions)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        model, \n",
    "        MODEL_NAME,\n",
    "        input_example=train_df.limit(1).toPandas(),\n",
    "        signature=signature,\n",
    "        registered_model_name=\"flight_delay_prediction_baseline\"\n",
    "        )\n",
    "\n",
    "    mlflow.log_metric(\"train_mae\", mae_t)\n",
    "    mlflow.log_metric(\"validation_mae\", mae_v)\n",
    "    mlflow.log_metric(\"train_rmse\", rmse_t)\n",
    "    mlflow.log_metric(\"validation_rmse\", rmse_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce34f33-355a-4f0e-9905-13930c0cd938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Initialize the MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Identify the target experiment\n",
    "experiment_name = \"flight_delay_prediction_baseline\"\n",
    "experiment = client.get_experiment_by_name(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "534b37f3-b110-4844-b8f5-516b38dacb2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if experiment:\n",
    "    # Set the filter string to only include runs that successfully finished\n",
    "    success_filter = \"attribute.status = 'FINISHED'\"\n",
    "\n",
    "    # Search runs using the filter\n",
    "    successful_runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=success_filter\n",
    "    )\n",
    "\n",
    "    # Count the resulting list\n",
    "    num_successful_runs = len(successful_runs)\n",
    "\n",
    "    print(f\"Total successful runs in '{experiment_name}': {num_successful_runs}\")\n",
    "\n",
    "    # You can inspect the first successful run for verification\n",
    "    if successful_runs:\n",
    "        print(f\"Example successful Run ID: {successful_runs[0].info.run_id}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Experiment '{experiment_name}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514ea725-a4ae-45eb-8637-4831dd1d6f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Initialize the client\n",
    "client = MlflowClient()\n",
    "\n",
    "# --- Step 1: Retrieve All Experiments ---\n",
    "# 'ACTIVE_ONLY' is the best type to avoid counting deleted experiments\n",
    "all_experiments = client.search_experiments(view_type='ACTIVE_ONLY')\n",
    "\n",
    "total_runs = 0\n",
    "# Create a list to store run counts per experiment for detail\n",
    "experiment_run_counts = {}\n",
    "\n",
    "# --- Step 2: Iterate and Sum Runs ---\n",
    "for exp in all_experiments:\n",
    "    try:\n",
    "        # Search all runs within the current experiment\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[exp.experiment_id],\n",
    "            filter_string=\"\" # No filter applied, count all\n",
    "        )\n",
    "        \n",
    "        num_runs_in_exp = len(runs)\n",
    "        total_runs += num_runs_in_exp\n",
    "        experiment_run_counts[exp.name] = num_runs_in_exp\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle cases where an experiment might be corrupted or inaccessible\n",
    "        print(f\"Skipping experiment {exp.name} due to error: {e}\")\n",
    "\n",
    "print(\"--- MLflow Run Summary ---\")\n",
    "print(f\"Total Runs Logged Across All Experiments: **{total_runs}**\")\n",
    "print(\"---\")\n",
    "# Optionally display the breakdown\n",
    "# print(\"Breakdown by Experiment:\")\n",
    "# for name, count in experiment_run_counts.items():\n",
    "#     print(f\"  - {name}: {count} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77c26a2-a881-4051-9470-4dad4fdda1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get all *active* experiments\n",
    "all_experiments = client.search_experiments(view_type='ACTIVE_ONLY') \n",
    "\n",
    "# 2. Iterate through each experiment\n",
    "for exp in all_experiments:\n",
    "    # 3. Get all runs within that experiment (no status filter applied here)\n",
    "    runs = client.search_runs(experiment_ids=[exp.experiment_id], filter_string=\"attributes.status = 'FINISHED'\") \n",
    "    \n",
    "    # 4. Sum the count\n",
    "    total_runs += len(runs)\n",
    "\n",
    "print(total_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ba191ce-5cd9-41e2-b6ca-4ea41b617659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e7ea1f-84c6-446b-a991-76c366a37d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8173636742502553,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(SO) ph2_feature-enginering-testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
