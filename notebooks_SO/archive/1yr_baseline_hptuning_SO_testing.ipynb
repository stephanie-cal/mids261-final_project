{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d6a5ac-8bbf-47d4-822e-27b3f1b35460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# XGBOOST model baseline - 1 year - hyperparameter tuning testing\n",
    "- run model from 1_year_combined data with feature engineering\n",
    "  - TAIL_NUM causes OOM error, comment out for now\n",
    "- featuring engineering handled in https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1792055957780055?o=4021782157704243\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets - custom join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7b274b-c8d2-4949-9a8a-2b58e6a17564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/feature_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d6f9c3-47ea-4ff0-8ed4-67f891e00385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dbfs:/student-groups/Group_2_2\"\n",
    "# dataset_path = f\"{checkpoint_path}/1_year_custom_joined/raw_data/training_splits\"\n",
    "dataset_path = f\"{checkpoint_path}/1_year_custom_joined/feature_eng/training_splits\"\n",
    "\n",
    "# Read datasets from checkpoint\n",
    "train_df = spark.read.parquet(f\"{dataset_path}/train.parquet\")\n",
    "validation_df = spark.read.parquet(f\"{dataset_path}/validation.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4825eb9-9441-4e5d-a2c3-6adc6cea10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71052af-525f-4775-a2cc-79142b51f875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"utc_timestamp\",\n",
    "    \"CRS_DEP_MINUTES\",            # feature eng start\n",
    "    \"prev_flight_delay_in_minutes\", \n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",        # feature eng end\n",
    "    'HourlyDryBulbTemperature',     # weather start\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed'               # weather end\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be39599a-9158-45a6-a9af-d94b74ab56c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(baselines_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95703f41-91b6-4e80-b7a7-b12e7acc17c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.columns == validation_df.columns)\n",
    "# print(train_df.columns == baselines_columns)\n",
    "\n",
    "# for col in baselines_columns:\n",
    "#     if col not in train_df.columns:\n",
    "#         print(col)\n",
    "\n",
    "for col in train_df.columns:\n",
    "    if col not in validation_df.columns:\n",
    "        print(col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d9c263-ad93-4094-9f1d-45388954d17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.filter(F.col(\"DEP_DELAY_NEW\").isNotNull()).select(baselines_columns)\n",
    "validation_df = validation_df.filter(F.col(\"DEP_DELAY_NEW\").isNotNull()).select(baselines_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff4688-854e-4a7f-9429-b74fdc925f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde39db-852d-4ce5-882d-a3f7f7cf3f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed'                   # weather end\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70368be9-d8ed-4557-8841-52d5e1751acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95004c4-11f3-4f51-8802-e341cff071ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 400\n",
    "train_df = train_df.repartition(NUM_PARTITIONS).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be653215-8965-4f23-bc26-6db14cc0b802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# --- Model Estimators ---\n",
    "preprocessing_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    assembler \n",
    "]\n",
    "\n",
    "# A. XGBoost Regressor\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=2, \n",
    ")\n",
    "\n",
    "# B. Random Forest Regressor\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"DEP_DELAY_NEW\"\n",
    ")\n",
    "\n",
    "# C. Linear Regression\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"DEP_DELAY_NEW\"\n",
    ")\n",
    "\n",
    "# --- Parameter Grids ---\n",
    "# A. XGBoost Grid\n",
    "xgb_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb.max_depth, [4]) \\\n",
    "    .addGrid(xgb.n_estimators, [20]) \\\n",
    "    .addGrid(xgb.learning_rate, [0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# B. Random Forest Grid\n",
    "rf_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
    "    .addGrid(rf.maxBins, [20, 32, 40]) \\\n",
    "    .build()\n",
    "\n",
    "# C. Linear Regression Grid (Regularization/ElasticNet)\n",
    "lr_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# --- Evaluator (Use one metric for optimization) ---\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" \n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183b0901-3ce1-479f-b620-1da5a5a5eff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_names = [p.name for p in xgb_grid[0].keys()]\n",
    "\n",
    "hyperparam_xgb_df = pd.DataFrame(\n",
    "    columns=param_names + ['train_mae', 'validation_mae', 'train_rmse', 'validation_rmse']\n",
    ")\n",
    "\n",
    "hyperparam_xgb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2e3116-3f08-4b5b-ac61-f64ea4f5e339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# step through grid\n",
    "alg = 'xgb'\n",
    "MODEL_NAME = \"TEST_XGB_1y_HPTUNE\"\n",
    "for params_ in xgb_grid:\n",
    "\n",
    "    estimator_with_params = xgb.copy(params_)\n",
    "    pipeline = Pipeline(stages=preprocessing_stages + [estimator_with_params])\n",
    "\n",
    "    param_str = \"_\".join([f\"{p.name}_{params_[p]}\" for p in params_])\n",
    "    run_name_suffix = f\"{alg}_{param_str}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"HPTune_{run_name_suffix}\"):\n",
    "        print(f\"Starting tuning for {run_name_suffix}...\")\n",
    "\n",
    "        model = pipeline.fit(train_df, params_)\n",
    "\n",
    "        training_predictions = model.transform(train_df)\n",
    "        validation_predictions = model.transform(validation_df)\n",
    "\n",
    "        mae_evaluator = RegressionEvaluator(\n",
    "            labelCol=\"DEP_DELAY_NEW\",      \n",
    "            predictionCol=\"prediction\", \n",
    "            metricName=\"mae\"           \n",
    "        )\n",
    "\n",
    "        rmse_evaluator = RegressionEvaluator(\n",
    "            labelCol=\"DEP_DELAY_NEW\",      \n",
    "            predictionCol=\"prediction\", \n",
    "            metricName=\"rmse\"\n",
    "        )\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae_t = mae_evaluator.evaluate(training_predictions)\n",
    "        mae_v = mae_evaluator.evaluate(validation_predictions)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse_t = rmse_evaluator.evaluate(training_predictions)\n",
    "        rmse_v = rmse_evaluator.evaluate(validation_predictions)\n",
    "\n",
    "        signature = infer_signature(train_df, training_predictions)\n",
    "\n",
    "        mlflow.spark.log_model(\n",
    "            model, \n",
    "            MODEL_NAME,\n",
    "            input_example=train_df.limit(1).toPandas(),\n",
    "            signature=signature,\n",
    "            registered_model_name=\"flight_delay_prediction_baseline\"\n",
    "            )\n",
    "\n",
    "        mlflow.log_metric(\"train_mae\", mae_t)\n",
    "        mlflow.log_metric(\"validation_mae\", mae_v)\n",
    "        mlflow.log_metric(\"train_rmse\", rmse_t)\n",
    "        mlflow.log_metric(\"validation_rmse\", rmse_v)\n",
    "\n",
    "        results_row_data = {\n",
    "            'train_mae': mae_t, \n",
    "            'validation_mae': mae_v, \n",
    "            'train_rmse': rmse_t, \n",
    "            'validation_rmse': rmse_v\n",
    "        }\n",
    "\n",
    "        for p in params_.keys():\n",
    "            results_row_data[p.name] = params_[p]\n",
    "\n",
    "        results_row = pd.DataFrame([results_row_data], columns=hyperparam_xgb_df.columns)\n",
    "        hyperparam_xgb_df = pd.concat([hyperparam_xgb_df, results_row], ignore_index=True)\n",
    "\n",
    "        print('tuning summary:')\n",
    "        print(hyperparam_xgb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59bc036-2de1-46fd-9d01-bcd2cd04dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hyperparam_xgb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ec1562-fe12-4a7a-8e60-b1fb06ba560d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Testing below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0ab693-0145-4cab-8efd-8b2bf3496f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_tuning(estimator, param_grid, run_name_suffix, signature):\n",
    "    \"\"\"Encapsulates the tuning process for a single model.\"\"\"\n",
    "    \n",
    "    # Create the model-specific pipeline\n",
    "    model_pipeline = Pipeline(stages=preprocessing_stages + [estimator])\n",
    "    \n",
    "    # Create the CrossValidator object\n",
    "    cv = CrossValidator(\n",
    "        estimator=model_pipeline,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=mae_evaluator,\n",
    "        numFolds=5, \n",
    "        parallelism=3 \n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"HPTune_{run_name_suffix}\"):\n",
    "        print(f\"Starting tuning for {run_name_suffix}...\")\n",
    "        \n",
    "        cv_model = cv.fit(train_df)\n",
    "        best_model = cv_model.bestModel\n",
    "        \n",
    "        # Log the final best model and its performance\n",
    "        best_metric = cv_model.avgMetrics[np.argmin(cv_model.avgMetrics)] #  MAE\n",
    "        \n",
    "        mlflow.log_metric(f\"best_validation_{mae_evaluator.getMetricName()}\", best_metric)\n",
    "        \n",
    "        # Log the best pipeline model found by CrossValidator\n",
    "        mlflow.spark.log_model(\n",
    "            best_model, \n",
    "            artifact_path=f\"best_model_{run_name_suffix}\",\n",
    "            signature=signature,\n",
    "            registered_model_name=\"flight_delay_prediction_final\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Tuning complete. Best MAE: {best_metric}\")\n",
    "        return cv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18c0b4e-d293-4477-a74f-4cdf3606b7a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_pipeline_signature(df, preprocessing_stages, model_estimator):\n",
    "    \"\"\"Creates a temporary pipeline and infers the signature based on the estimator.\"\"\"\n",
    "    \n",
    "    # Create a temporary pipeline\n",
    "    temp_pipeline = Pipeline(stages=preprocessing_stages + [model_estimator])\n",
    "    \n",
    "    # Fit and transform a small sample for schema inference\n",
    "    sample_df = df.limit(10)\n",
    "    \n",
    "    # Fit the pipeline to the sample\n",
    "    temp_model = temp_pipeline.fit(sample_df) \n",
    "    \n",
    "    # Transform the sample\n",
    "    predictions_df = temp_model.transform(sample_df)\n",
    "    \n",
    "    # Infer the signature\n",
    "    signature = infer_signature(\n",
    "        # sample_df.select(\"features\").toPandas(), \n",
    "        predictions_df.select(\"features\").toPandas(),\n",
    "        predictions_df.select(\"prediction\").toPandas()\n",
    "    )\n",
    "    \n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ace927-f2ff-4e7b-b411-9f4669b77798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Linear Regression Signature\n",
    "# lr_signature = create_pipeline_signature(train_df, preprocessing_stages, lr)\n",
    "\n",
    "# 2. XGBoost Signature\n",
    "# xgb_signature = create_pipeline_signature(train_df, preprocessing_stages, xgb)\n",
    "\n",
    "# 3. Random Forest Signature (Assuming rf_grid is defined)\n",
    "# rf_signature = create_pipeline_signature(train_df, preprocessing_stages, rf)\n",
    "\n",
    "# --- 4. Execute Tuning ---\n",
    "# lr_cv_model = run_tuning(lr, lr_grid, \"LinearRegression\", lr_signature)\n",
    "# xgb_cv_model = run_tuning(xgb, xgb_grid, \"XGBoost\", xgb_signature)\n",
    "# rf_cv_model = run_tuning(rf, rf_grid, \"RandomForest\", rf_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d4c0b1-2358-4635-b700-55c594caf930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1970ea5e-6635-479a-bba6-f217d3898b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Evaluator (Use one metric for optimization) ---\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" \n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "def run_tuning(estimator, param_grid, run_name_suffix):\n",
    "    \"\"\"Encapsulates the tuning process for a single model.\"\"\"\n",
    "    \n",
    "    # Create the model-specific pipeline\n",
    "    model_pipeline = Pipeline(stages=preprocessing_stages + [estimator])\n",
    "    \n",
    "    # Create the CrossValidator object\n",
    "    cv = CrossValidator(\n",
    "        estimator=model_pipeline,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=mae_evaluator,\n",
    "        numFolds=5, \n",
    "        parallelism=3 \n",
    "    )\n",
    "\n",
    "    signature = create_pipeline_signature(train_df, preprocessing_stages, lr)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"HPTune_{run_name_suffix}\"):\n",
    "        print(f\"Starting tuning for {run_name_suffix}...\")\n",
    "        \n",
    "        cv_model = cv.fit(train_df)\n",
    "        best_model = cv_model.bestModel\n",
    "        \n",
    "        # Log the final best model and its performance\n",
    "        best_rmse = cv_model.avgMetrics[np.argmin(cv_model.avgMetrics)]\n",
    "        \n",
    "        mlflow.log_metric(\"best_validation_rmse\", best_rmse)\n",
    "        \n",
    "        # Log the best pipeline model found by CrossValidator\n",
    "        mlflow.spark.log_model(\n",
    "        best_model, \n",
    "        artifact_path=f\"best_model_{run_name_suffix}\",\n",
    "        signature=signature, \n",
    "        registered_model_name=\"flight_delay_prediction_final\"\n",
    "    )\n",
    "        \n",
    "        print(f\"Tuning complete. Best RMSE: {best_rmse}\")\n",
    "        return cv_model\n",
    "\n",
    "# ---  Execute ---\n",
    "# xgb_cv_model = run_tuning(xgb, xgb_grid, \"XGBoost\")\n",
    "# rf_cv_model = run_tuning(rf, rf_grid, \"RandomForest\")\n",
    "lr_cv_model = run_tuning(lr, lr_grid, \"LinearRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fec4455-ee64-415b-a54e-600691c42e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2261bf73-37b9-4449-8bd0-445fd78d1659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# baseline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.spark.autolog()\n",
    "with mlflow.start_run(run_name=\"test_XGB-1y_hp_tune\"):\n",
    "    MODEL_NAME = \"TEST_XGB_1y_HPTUNE\"\n",
    "\n",
    "    # linear_reg = LinearRegression(\n",
    "    #     featuresCol=\"features\",\n",
    "    #     labelCol=\"DEP_DELAY_NEW\",\n",
    "    #     # Linear Regression has different parameters than Random Forest\n",
    "    #     maxIter=10, \n",
    "    #     regParam=0.3\n",
    "    # )\n",
    "    # rf = RandomForestRegressor(\n",
    "    #     featuresCol=\"features\",  \n",
    "    #     labelCol=\"DEP_DELAY_NEW\",   \n",
    "    #     numTrees=20,\n",
    "    #     maxDepth=10\n",
    "    # )\n",
    "\n",
    "    xgb_regressor = SparkXGBRegressor(\n",
    "        features_col=\"features\",\n",
    "        label_col=\"DEP_DELAY_NEW\",\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(stages=[\n",
    "        carrier_indexer, origin_indexer, dest_indexer, \n",
    "        carrier_encoder, origin_encoder, dest_encoder, \n",
    "        assembler,\n",
    "        # linear_reg\n",
    "        # rf\n",
    "        xgb_regressor\n",
    "    ])\n",
    "\n",
    "    # pipeline = Pipeline(stages=[\n",
    "    #     carrier_indexer, origin_indexer, dest_indexer, tail_num_indexer,\n",
    "    #     carrier_encoder, origin_encoder, dest_encoder, tail_num_encoder,\n",
    "    #     assembler,\n",
    "    #     # linear_reg\n",
    "    #     # rf\n",
    "    #     xgb_regressor\n",
    "    # ])\n",
    "\n",
    "    model = pipeline.fit(train_df)\n",
    "    training_predictions = model.transform(train_df)\n",
    "    validation_predictions = model.transform(validation_df)\n",
    "\n",
    "    mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"           \n",
    "    )\n",
    "\n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_t = mae_evaluator.evaluate(training_predictions)\n",
    "    mae_v = mae_evaluator.evaluate(validation_predictions)\n",
    "    # Calculate RMSE\n",
    "    rmse_t = rmse_evaluator.evaluate(training_predictions)\n",
    "    rmse_v = rmse_evaluator.evaluate(validation_predictions)\n",
    "\n",
    "    signature = infer_signature(train_df, training_predictions)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        model, \n",
    "        MODEL_NAME,\n",
    "        input_example=train_df.limit(1).toPandas(),\n",
    "        signature=signature,\n",
    "        registered_model_name=\"flight_delay_prediction_baseline\"\n",
    "        )\n",
    "\n",
    "    mlflow.log_metric(\"train_mae\", mae_t)\n",
    "    mlflow.log_metric(\"validation_mae\", mae_v)\n",
    "    mlflow.log_metric(\"train_rmse\", rmse_t)\n",
    "    mlflow.log_metric(\"validation_rmse\", rmse_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc20216a-d892-45d1-8042-3f13d3f2496b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23315981-aa97-4026-9ef1-91d11da10bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    assembler,\n",
    "    lr\n",
    "    # # rf\n",
    "    # xgb_regressor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78bc3e74-3b2a-4d46-9f50-8c51b4d11f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e732e7e-3b42-4296-85e1-5c3d78506174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for params_ in lr_grid:\n",
    "    pipeline.fit(train_df, params_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7cbaef-9b5b-437b-8074-25b5ac3fc909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1792055957780492,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1yr_baseline_hptuning_SO_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
