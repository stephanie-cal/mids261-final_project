{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d6a5ac-8bbf-47d4-822e-27b3f1b35460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NN MLP model baseline - 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f0190e-ca20-42f1-a87e-a3db6d8ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c9b79-14bf-4db7-906c-23b1354bd52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-classifier\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fa16c1-a344-469e-aca4-26f972599b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cfa43-b780-451f-b18a-a36522496d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128ec4a2-9ea7-4229-b0f4-794a0f63cc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets - custom join\n",
    "- get checkpoint data\n",
    "  - 5 year combined join, with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7b274b-c8d2-4949-9a8a-2b58e6a17564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls dbfs:/student-groups/Group_2_2/1_year_custom_joined/feature_eng\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/cv_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/training_splits/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/models/\"))\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/cv_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/feature_eng_ph3/training_splits/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14e4bb2-29a5-4815-bc3e-3033a5fb5383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet/\").limit(100))\n",
    "\n",
    "# display(spark.read.parquet(\"dbfs:/student-groups/Group_2_2/5_year_custom_joined/feature_eng_ph3/training_splits/test.parquet/\").limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4825eb9-9441-4e5d-a2c3-6adc6cea10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71052af-525f-4775-a2cc-79142b51f875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baselines_columns = [\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"YEAR\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\",\n",
    "    # \"TAIL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DEP_DELAY_NEW\",\n",
    "    \"DEP_DEL15\",\n",
    "    \"utc_timestamp\",\n",
    "    \"CRS_DEP_MINUTES\",            # feature eng start\n",
    "    \"prev_flight_delay_in_minutes\", \n",
    "    \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",        # feature eng end\n",
    "    'HourlyDryBulbTemperature',     # weather start\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyAltimeterSetting',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyStationPressure',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyCloudCoverage',\n",
    "    'HourlyCloudElevation',\n",
    "    'HourlyWindSpeed',               # weather end\n",
    "    'page_rank',               # phase 3 new features start\n",
    "    'out_degree',\n",
    "    'in_degree',\n",
    "    'weighted_out_degree',\n",
    "    'weighted_in_degree',\n",
    "    'N_RUNWAYS',\n",
    "    'betweenness_unweighted',\n",
    "    'closeness',\n",
    "    'betweenness',\n",
    "    'avg_origin_dep_delay',\n",
    "    'avg_dest_arr_delay',\n",
    "    'avg_daily_route_flights',\n",
    "    'avg_route_delay',\n",
    "    'avg_hourly_flights',\n",
    "    \"IS_HOLIDAY\",\n",
    "    \"IS_HOLIDAY_WINDOW\",\n",
    "    \"AIRPORT_HUB_CLASS\",\n",
    "    \"RATING\",\n",
    "    \"AIRLINE_CATEGORY\" # phase 3 new features end\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ff4688-854e-4a7f-9429-b74fdc925f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Categorical encoding\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "holiday_indexer = StringIndexer(inputCol=\"IS_HOLIDAY\", outputCol=\"holiday_idx\", handleInvalid=\"keep\")\n",
    "holiday_window_indexer = StringIndexer(inputCol=\"IS_HOLIDAY_WINDOW\", outputCol=\"holiday_window_idx\", handleInvalid=\"keep\")\n",
    "airport_hub_indexer = StringIndexer(inputCol=\"AIRPORT_HUB_CLASS\", outputCol=\"airport_hub_idx\", handleInvalid=\"keep\")\n",
    "airline_category_indexer = StringIndexer(inputCol=\"AIRLINE_CATEGORY\", outputCol=\"airline_category_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_idx\", outputCol=\"holiday_vec\")\n",
    "holiday_window_encoder = OneHotEncoder(inputCol=\"holiday_window_idx\", outputCol=\"holiday_window_vec\")\n",
    "airport_hub_encoder = OneHotEncoder(inputCol=\"airport_hub_idx\", outputCol=\"airport_hub_vec\")\n",
    "airline_category_encoder = OneHotEncoder(inputCol=\"airline_category_idx\", outputCol=\"airline_category_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde39db-852d-4ce5-882d-a3f7f7cf3f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights',\n",
    "        \"holiday_vec\",\n",
    "        \"holiday_window_vec\",\n",
    "        \"airport_hub_vec\",\n",
    "        \"RATING\",\n",
    "        \"airline_category_vec\"               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70368be9-d8ed-4557-8841-52d5e1751acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be653215-8965-4f23-bc26-6db14cc0b802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Model Estimators ---\n",
    "preprocessing_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    assembler \n",
    "]\n",
    "\n",
    "# --- Evaluator (Use one metric for optimization) ---\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" \n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"DEP_DELAY_NEW\",      \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"    \n",
    ")\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f2_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedFMeasure\"\n",
    ")\n",
    "f2_evaluator.setBeta(2.0)\n",
    "\n",
    "f2_evaluator_label = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"fMeasureByLabel\"\n",
    ")\n",
    "f2_evaluator_label.setMetricLabel(1.0).setBeta(2.0)\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36887f53-3436-46c9-bc0e-aaca9fe12246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_specific_fold(path: str, fold_id: int, split_type: str):\n",
    "    \"\"\"\n",
    "    Read a specific fold from partitioned parquet data.\n",
    "    Falls back to filtering if direct partition read fails.\n",
    "    \"\"\"\n",
    "    fold_path = f\"{path}/fold_id={fold_id}/split_type={split_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Try direct partition read\n",
    "        return spark.read.parquet(fold_path)\n",
    "    except:\n",
    "        # Fallback: read all data and filter\n",
    "        print(f\"Direct read failed for fold {fold_id}, using filter method...\")\n",
    "        all_data = spark.read.parquet(path)\n",
    "        return all_data.filter(\n",
    "            (all_data.fold_id == fold_id) & \n",
    "            (all_data.split_type == split_type)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e7ba9c-de95-4b40-8ee1-a0c4713f8f0b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765167891201}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_or_year = \"5_year_custom_joined\"\n",
    "# cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/fe_graph_and_holiday/cv_splits\" \n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f80131a4-e462-4796-9f80-00e4c0c990ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Plain MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59bc036-2de1-46fd-9d01-bcd2cd04dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Aggressive Optimization Config ---\n",
    "OPTIMAL_PARTITIONS = 480  # 48 cores * 10 tasks\n",
    "n_folds = 10               \n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\"\n",
    "\n",
    "# --- 2. Safety UDF (Prevent Crashing on Inf/NaN) ---\n",
    "@F.udf(returnType=BooleanType())\n",
    "def vector_is_valid(v):\n",
    "    if v is None: return False\n",
    "    if np.any(np.isinf(v.values)): return False\n",
    "    if np.any(np.isnan(v.values)): return False\n",
    "    if np.max(np.abs(v.values)) > 1e30: return False\n",
    "    return True\n",
    "\n",
    "# --- 3. Pipeline Definition ---\n",
    "# Since this is the BASELINE, we process raw features (no XGBoost input)\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "holiday_indexer = StringIndexer(inputCol=\"IS_HOLIDAY\", outputCol=\"holiday_idx\", handleInvalid=\"keep\")\n",
    "holiday_window_indexer = StringIndexer(inputCol=\"IS_HOLIDAY_WINDOW\", outputCol=\"holiday_window_idx\", handleInvalid=\"keep\")\n",
    "airport_hub_indexer = StringIndexer(inputCol=\"AIRPORT_HUB_CLASS\", outputCol=\"airport_hub_idx\", handleInvalid=\"keep\")\n",
    "airline_category_indexer = StringIndexer(inputCol=\"AIRLINE_CATEGORY\", outputCol=\"airline_category_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_idx\", outputCol=\"holiday_vec\")\n",
    "holiday_window_encoder = OneHotEncoder(inputCol=\"holiday_window_idx\", outputCol=\"holiday_window_vec\")\n",
    "airport_hub_encoder = OneHotEncoder(inputCol=\"airport_hub_idx\", outputCol=\"airport_hub_vec\")\n",
    "airline_category_encoder = OneHotEncoder(inputCol=\"airline_category_idx\", outputCol=\"airline_category_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights',\n",
    "        \"holiday_vec\",\n",
    "        \"holiday_window_vec\",\n",
    "        \"airport_hub_vec\",\n",
    "        \"RATING\",\n",
    "        \"airline_category_vec\"               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "baseline_pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    tail_num_indexer, holiday_indexer, holiday_window_indexer, \n",
    "    airport_hub_indexer, airline_category_indexer,\n",
    "    \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    tail_num_encoder, holiday_encoder, holiday_window_encoder, \n",
    "    airport_hub_encoder, airline_category_encoder,\n",
    "    \n",
    "    assembler, \n",
    "    scaler\n",
    "])\n",
    "\n",
    "# --- 4. Global Loading & Preprocessing ---\n",
    "print(f\"Loading 20% Sample from {month_or_year}...\")\n",
    "\n",
    "# Load + Sample + Repartition\n",
    "full_cv_df = spark.read.parquet(cv_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20, seed=RANDOM_SEED) \\\n",
    "    .repartition(OPTIMAL_PARTITIONS) \\\n",
    "    .cache()\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline...\")\n",
    "feat_model = baseline_pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming, Cleaning & Persisting Vectors...\")\n",
    "# Transform -> Filter Invalid Vectors -> Persist to Disk\n",
    "featurized_df = feat_model.transform(full_cv_df) \\\n",
    "    .select(\"scaled_features\", \"DEP_DEL15\", \"fold_id\", \"split_type\") \\\n",
    "    .filter(vector_is_valid(F.col(\"scaled_features\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {featurized_df.count()} rows for Baseline Training.\")\n",
    "\n",
    "# --- 5. Training Loop (With Class Balancing) ---\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"f1\")\n",
    "f2_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedFMeasure\", beta=2.0)\n",
    "f2_label_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"fMeasureByLabel\", metricLabel=1.0, beta=2.0)\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedRecall\")\n",
    "\n",
    "print(f\"Starting Baseline MLP Training (Balanced)...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"POST_PRESENTATION_MLP_BASELINE_5_YR_BALANCED_10CV\") as run:\n",
    "    \n",
    "    mlflow.log_param(\"model\", \"MLP Classifier\")\n",
    "    mlflow.log_param(\"type\", \"Baseline (No Stack)\")\n",
    "    mlflow.log_param(\"strategy\", \"Balanced Training + Global Prep\")\n",
    "    \n",
    "    fold_metrics = {\n",
    "        'train_f1': [], 'train_f2': [], 'train_f2_label': [], 'train_precision': [], 'train_recall': [],\n",
    "        'val_f1': [], 'val_f2': [], 'val_f2_label': [], 'val_precision': [], 'val_recall': []\n",
    "    }\n",
    "    \n",
    "    input_dim = len(featurized_df.first()[\"scaled_features\"])\n",
    "    print(f\"Detected Input Dimension: {input_dim}\")\n",
    "    mlflow.log_param(\"input_dim\", input_dim)\n",
    "\n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        with mlflow.start_run(run_name=f\"fold_{fold_id}\", nested=True):\n",
    "            print(f\"  Processing Fold {fold_id}/{n_folds}...\")\n",
    "            \n",
    "            # 1. Split Data\n",
    "            # Note: For baseline, we use the pre-defined 'split_type' from your CV generation\n",
    "            train_raw = featurized_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"train\"))\n",
    "            val_vec = featurized_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"validation\"))\n",
    "            \n",
    "            # 2. BALANCE TRAINING DATA (Critical Step)\n",
    "            train_pos = train_raw.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "            train_neg = train_raw.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "            \n",
    "            pos_count = train_pos.count()\n",
    "            neg_count = train_neg.count()\n",
    "            \n",
    "            # Downsample negatives to match positives\n",
    "            fraction = pos_count / neg_count\n",
    "            train_neg_sampled = train_neg.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "            \n",
    "            train_balanced = train_pos.union(train_neg_sampled).repartition(OPTIMAL_PARTITIONS)\n",
    "            \n",
    "            print(f\"    Fold {fold_id}: Training on Balanced Data ({pos_count} Pos)\")\n",
    "            \n",
    "            # 3. Define & Train MLP\n",
    "            layers = [input_dim, 64, 2] \n",
    "            mlp = MultilayerPerceptronClassifier(\n",
    "                featuresCol=\"scaled_features\",\n",
    "                labelCol=\"DEP_DEL15\",\n",
    "                maxIter=100,\n",
    "                layers=layers,\n",
    "                blockSize=128,\n",
    "                stepSize=0.03\n",
    "            )\n",
    "            \n",
    "            mlp_model = mlp.fit(train_balanced)\n",
    "            \n",
    "            # 4. Predict & Evaluate\n",
    "            # Predict on Training Set (Balanced)\n",
    "            train_preds = mlp_model.transform(train_balanced).select(\"prediction\", \"DEP_DEL15\").cache()\n",
    "            train_preds.count() # Materialize\n",
    "            \n",
    "            # Predict on Validation Set (Unbalanced/Real)\n",
    "            val_preds = mlp_model.transform(val_vec).select(\"prediction\", \"DEP_DEL15\").cache()\n",
    "            val_preds.count() # Materialize\n",
    "            \n",
    "            metrics = {\n",
    "                # Training Metrics\n",
    "                \"train_f1\": f1_evaluator.evaluate(train_preds),\n",
    "                \"train_f2\": f2_evaluator.evaluate(train_preds),\n",
    "                \"train_f2_label\": f2_label_evaluator.evaluate(train_preds),\n",
    "                \"train_precision\": precision_evaluator.evaluate(train_preds),\n",
    "                \"train_recall\": recall_evaluator.evaluate(train_preds),\n",
    "                \n",
    "                # Validation Metrics\n",
    "                \"val_f1\": f1_evaluator.evaluate(val_preds),\n",
    "                \"val_f2\": f2_evaluator.evaluate(val_preds),\n",
    "                \"val_f2_label\": f2_label_evaluator.evaluate(val_preds),\n",
    "                \"val_precision\": precision_evaluator.evaluate(val_preds),\n",
    "                \"val_recall\": recall_evaluator.evaluate(val_preds),\n",
    "            }\n",
    "            \n",
    "            # mlflow.log_metrics(metrics)\n",
    "            print(f\"    Fold {fold_id}: Train F2-Delay={metrics['train_f2_label']:.4f}, Val F2-Delay={metrics['val_f2_label']:.4f}\")\n",
    "            \n",
    "            for k in fold_metrics.keys():\n",
    "                fold_metrics[k].append(metrics[k])\n",
    "            \n",
    "            val_preds.unpersist()\n",
    "\n",
    "    # Log Averages\n",
    "    avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "    mlflow.log_metrics(avg_metrics)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Average Train Delay-F2: {avg_metrics['avg_train_f2_label']:.4f}\")\n",
    "    print(f\"Average Val Delay-F2:   {avg_metrics['avg_val_f2_label']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- 6. RETRAIN FINAL MODEL ON FULL DATA ---\n",
    "    print(\"Retraining Final Model on Full Balanced Data...\")\n",
    "    \n",
    "    # Balance the entire dataset (using the global sample)\n",
    "    full_pos = featurized_df.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "    full_neg = featurized_df.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "    \n",
    "    global_fraction = full_pos.count() / full_neg.count()\n",
    "    full_balanced = full_pos.union(\n",
    "        full_neg.sample(withReplacement=False, fraction=global_fraction, seed=42)\n",
    "    ).repartition(OPTIMAL_PARTITIONS)\n",
    "    \n",
    "    final_mlp = MultilayerPerceptronClassifier(\n",
    "        featuresCol=\"scaled_features\",\n",
    "        labelCol=\"DEP_DEL15\",\n",
    "        layers=[input_dim, 64, 2],\n",
    "        blockSize=128,\n",
    "        maxIter=100,\n",
    "        stepSize=0.03\n",
    "    )\n",
    "    \n",
    "    final_model = final_mlp.fit(full_balanced)\n",
    "    \n",
    "    # Log Model Artifact\n",
    "    mlflow.spark.log_model(final_model, \"model\")\n",
    "    print(f\"Final Model Saved to Run: {run.info.run_id}\")\n",
    "\n",
    "# Cleanup\n",
    "featurized_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9afbeb4-4a8e-444f-b880-c63561c9d05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# stacked approach\n",
    "- build XGBoost model and do hyperparamter tuning to find the best hyperparams\n",
    "- generate the XGBoost regression delay field and output it using the held out data\n",
    "- use the XGBoost delay field as the input for the NN/MLP model while doing hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b532a244-29b9-41e6-8b0e-63f353ba165b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import BooleanType\n",
    "import itertools\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# --- 1. Config ---\n",
    "TRAIN_PARTITIONS = 6 \n",
    "n_folds = 10\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\"\n",
    "temp_materialize_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/temp_xgb_training\"\n",
    "\n",
    "# --- 2. Robust Safety Checks ---\n",
    "# This REPLACES bad values instead of checking for them\n",
    "@F.udf(returnType=VectorUDT())\n",
    "def sanitize_vector(v):\n",
    "    if v is None: return Vectors.dense([])\n",
    "    \n",
    "    # Convert to numpy for fast processing\n",
    "    arr = v.toArray()\n",
    "    \n",
    "    # Replace NaN, Infinity, -Infinity with 0.0\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Clip huge values to prevent float32 overflow in XGBoost\n",
    "    arr = np.clip(arr, -1e30, 1e30)\n",
    "    \n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "# --- 3. Data Loading & Pipeline Definition ---\n",
    "print(f\"Loading and optimizing data from {month_or_year}...\")\n",
    "\n",
    "# Sample 20% (Stable)\n",
    "full_cv_df = spark.read.parquet(cv_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20, seed=RANDOM_SEED) \\\n",
    "    .na.fill(0) \\\n",
    "    .repartition(480) \\\n",
    "    .cache()\n",
    "\n",
    "# --- Feature Definitions ---\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "holiday_indexer = StringIndexer(inputCol=\"IS_HOLIDAY\", outputCol=\"holiday_idx\", handleInvalid=\"keep\")\n",
    "holiday_window_indexer = StringIndexer(inputCol=\"IS_HOLIDAY_WINDOW\", outputCol=\"holiday_window_idx\", handleInvalid=\"keep\")\n",
    "airport_hub_indexer = StringIndexer(inputCol=\"AIRPORT_HUB_CLASS\", outputCol=\"airport_hub_idx\", handleInvalid=\"keep\")\n",
    "airline_category_indexer = StringIndexer(inputCol=\"AIRLINE_CATEGORY\", outputCol=\"airline_category_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_idx\", outputCol=\"holiday_vec\")\n",
    "holiday_window_encoder = OneHotEncoder(inputCol=\"holiday_window_idx\", outputCol=\"holiday_window_vec\")\n",
    "airport_hub_encoder = OneHotEncoder(inputCol=\"airport_hub_idx\", outputCol=\"airport_hub_vec\")\n",
    "airline_category_encoder = OneHotEncoder(inputCol=\"airline_category_idx\", outputCol=\"airline_category_vec\")\n",
    "\n",
    "# RE-DEFINE Assembler to ensure we output \"features\" (Standardizing name)\n",
    "assembler_xgb = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights',\n",
    "        \"holiday_vec\",\n",
    "        \"holiday_window_vec\",\n",
    "        \"airport_hub_vec\",\n",
    "        \"RATING\",\n",
    "        \"airline_category_vec\"               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"raw_features\" # Explicit output name\n",
    ")\n",
    "\n",
    "# Re-build pipeline stages list to use this specific assembler\n",
    "# We exclude the Scaler (not needed for XGBoost)\n",
    "xgb_pipeline_stages = [\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    tail_num_indexer, holiday_indexer, holiday_window_indexer, \n",
    "    airport_hub_indexer, airline_category_indexer,\n",
    "    \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    tail_num_encoder, holiday_encoder, holiday_window_encoder, \n",
    "    airport_hub_encoder, airline_category_encoder,\n",
    "    \n",
    "    assembler_xgb\n",
    "]\n",
    "\n",
    "global_pipeline = Pipeline(stages=xgb_pipeline_stages)\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline...\")\n",
    "feat_model = global_pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming & Cleaning...\")\n",
    "raw_transformed = feat_model.transform(full_cv_df)\n",
    "\n",
    "# CRITICAL FIX: Filter BOTH Features and Labels\n",
    "# clean_df = raw_transformed \\\n",
    "#     .select(\"features\", \"DEP_DELAY_NEW\", \"fold_id\", \"split_type\") \\\n",
    "#     .filter(F.col(\"DEP_DELAY_NEW\").isNotNull()) \\\n",
    "#     .filter(~F.isnan(F.col(\"DEP_DELAY_NEW\"))) \\\n",
    "#     .filter(vector_is_valid(F.col(\"features\"))) \\\n",
    "#     .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "clean_df = raw_transformed \\\n",
    "    .withColumn(\"features\", sanitize_vector(F.col(\"raw_features\"))) \\\n",
    "    .select(\"features\", \"DEP_DELAY_NEW\", \"fold_id\", \"split_type\") \\\n",
    "    .filter(F.col(\"DEP_DELAY_NEW\").isNotNull()) \\\n",
    "    .filter(~F.isnan(F.col(\"DEP_DELAY_NEW\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {clean_df.count()} CLEAN rows for Tuning.\")\n",
    "\n",
    "# --- 4. Define Parameter Grid ---\n",
    "grid_search_params = {\n",
    "    \"max_depth\": [6], \n",
    "    \"n_estimators\": [50, 100], \n",
    "    \"learning_rate\": [0.05, 0.1]\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid_search_params.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "print(f\"Total Parameter Combinations: {len(param_combinations)}\")\n",
    "\n",
    "# --- 5. Run Tuning Loop (Fold-First) ---\n",
    "results_list = []\n",
    "fold_scores = {i: {\n",
    "    'train_mae': [], 'train_rmse': [],\n",
    "    'val_mae': [], 'val_rmse': []\n",
    "} for i in range(len(param_combinations))}\n",
    "\n",
    "print(f\"Starting Robust XGBoost Tuning...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"POST_PRESENTATION_XGB_STACKED_5_YR_10CV\") as parent_run:\n",
    "    mlflow.log_param(\"n_combinations\", len(param_combinations))\n",
    "    \n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        print(f\"\\n=== Processing Fold {fold_id}/{n_folds} ===\")\n",
    "        \n",
    "        # A. Materialize Clean Training Data\n",
    "        fold_train_path = f\"{temp_materialize_path}/fold_{fold_id}\"\n",
    "        print(f\"  Materializing training data to {fold_train_path}...\")\n",
    "        \n",
    "        # Write clean data\n",
    "        clean_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"train\")) \\\n",
    "            .repartition(TRAIN_PARTITIONS) \\\n",
    "            .write.mode(\"overwrite\").parquet(fold_train_path)\n",
    "            \n",
    "        # Read back\n",
    "        train_vec = spark.read.parquet(fold_train_path).repartition(TRAIN_PARTITIONS)\n",
    "        \n",
    "        val_vec = clean_df.filter((F.col(\"fold_id\") == fold_id) & (F.col(\"split_type\") == \"validation\"))\n",
    "        \n",
    "        # INNER LOOP: Parameters\n",
    "        for idx, params in enumerate(param_combinations):\n",
    "            param_str = f\"depth{params['max_depth']}_est{params['n_estimators']}_lr{params['learning_rate']}\"\n",
    "            \n",
    "            xgb = SparkXGBRegressor(\n",
    "                features_col=\"features\",\n",
    "                label_col=\"DEP_DELAY_NEW\",\n",
    "                num_workers=6, \n",
    "                tree_method=\"hist\", \n",
    "                max_depth=params['max_depth'],\n",
    "                n_estimators=params['n_estimators'],\n",
    "                learning_rate=params['learning_rate'],\n",
    "                missing=0.0  # FIX: Explicitly handle sparse zeros/missing\n",
    "            )\n",
    "            \n",
    "            model = xgb.fit(train_vec)\n",
    "\n",
    "            # --- CALCULATE METRICS (Train vs Val) ---\n",
    "            \n",
    "            # 1. Training Metrics (Check for Overfitting)\n",
    "            train_preds = model.transform(train_vec)\n",
    "            t_mae = mae_evaluator.evaluate(train_preds)\n",
    "            t_rmse = rmse_evaluator.evaluate(train_preds)\n",
    "            \n",
    "            # 2. Validation Metrics (Generalization)\n",
    "            val_preds = model.transform(val_vec)\n",
    "            v_mae = mae_evaluator.evaluate(val_preds)\n",
    "            v_rmse = rmse_evaluator.evaluate(val_preds)\n",
    "            \n",
    "            # Store\n",
    "            fold_scores[idx]['train_mae'].append(t_mae)\n",
    "            fold_scores[idx]['train_rmse'].append(t_rmse)\n",
    "            \n",
    "            fold_scores[idx]['val_mae'].append(v_mae)\n",
    "            fold_scores[idx]['val_rmse'].append(v_rmse)\n",
    "            \n",
    "            print(f\"  Combo {idx+1}: Train MAE={t_mae:.2f} / Val MAE={v_mae:.2f}\")\n",
    "            print(f\"  Combo {idx+1}: Train RMSE={t_rmse:.2f} / Val RMSE={v_rmse:.2f}\")\n",
    "\n",
    "    # --- 6. Aggregate Results ---\n",
    "    print(\"\\n=== Aggregating Results ===\")\n",
    "    for idx, params in enumerate(param_combinations):\n",
    "        # Calculate Averages\n",
    "        avg_scores = {k: np.mean(v) for k, v in fold_scores[idx].items()}\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"combo_{idx}_summary\", nested=True):\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metrics(avg_scores)\n",
    "        \n",
    "        results_list.append({\n",
    "            **params,\n",
    "            \"avg_train_mae\": avg_scores['train_mae'],\n",
    "            \"avg_val_mae\": avg_scores['val_mae'],\n",
    "            \"avg_train_rmse\": avg_scores['train_rmse'],\n",
    "            \"avg_val_rmse\": avg_scores['val_rmse']\n",
    "        })\n",
    "        print(f\"Combo {idx+1} Final: Val MAE={avg_scores['val_mae']:.4f}, Train MAE={avg_scores['train_mae']:.4f}\")\n",
    "        print(f\"Combo {idx+1} Final: Val RMSE={avg_scores['val_rmse']:.4f}, Train RMSE={avg_scores['train_rmse']:.4f}\")\n",
    "\n",
    "# --- 8. Identify Best Parameters ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "best_row = results_df.loc[results_df['avg_val_mae'].idxmin()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"WINNER FOUND: {best_row.to_dict()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_depth = int(best_row['max_depth'])\n",
    "best_estimators = int(best_row['n_estimators'])\n",
    "best_lr = float(best_row['learning_rate'])\n",
    "\n",
    "# # --- 9. Retrain & Log BEST Model ---\n",
    "# print(\"Retraining Final Model with Best Parameters...\")\n",
    "\n",
    "# # Define Final Model\n",
    "# final_xgb = SparkXGBRegressor(\n",
    "#     features_col=\"features\",\n",
    "#     label_col=\"DEP_DELAY_NEW\",\n",
    "#     num_workers=6,\n",
    "#     tree_method=\"hist\",\n",
    "#     max_depth=best_depth,\n",
    "#     n_estimators=best_estimators,\n",
    "#     learning_rate=best_lr,\n",
    "#     missing=0.0\n",
    "# )\n",
    "\n",
    "# # Pipeline includes the Assembler + Model (Indexers were pre-applied, but safer to include if needed for inference)\n",
    "# # Since we used 'clean_df' which already has vectors, we fit on that directly.\n",
    "# # BUT for a reusable model, we usually want the whole pipeline.\n",
    "# # For simplicity here, we log the model trained on the processed vectors.\n",
    "# final_model = final_xgb.fit(clean_df.repartition(TRAIN_PARTITIONS))\n",
    "\n",
    "# with mlflow.start_run(run_name=\"POST_PRESENTATION_FINAL_BEST_XGB_MODEL\") as run:\n",
    "#     mlflow.log_params(best_row.to_dict())\n",
    "#     mlflow.spark.log_model(final_model, \"model\")\n",
    "#     print(f\"Final model saved to MLflow Run: {run.info.run_id}\")\n",
    "\n",
    "# # Cleanup\n",
    "# dbutils.fs.rm(temp_materialize_path, recurse=True)\n",
    "# clean_df.unpersist()\n",
    "# full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61d1f76-4446-408d-a6e6-afb705bb7d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 9. Retrain & Log BEST Model ---\n",
    "print(\"Retraining Final Model with Best Parameters...\")\n",
    "\n",
    "# Define Final Model\n",
    "final_xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=6,\n",
    "    tree_method=\"hist\",\n",
    "    max_depth=best_depth,\n",
    "    n_estimators=best_estimators,\n",
    "    learning_rate=best_lr,\n",
    "    missing=0.0\n",
    ")\n",
    "\n",
    "# Pipeline includes the Assembler + Model (Indexers were pre-applied, but safer to include if needed for inference)\n",
    "# Since we used 'clean_df' which already has vectors, we fit on that directly.\n",
    "# BUT for a reusable model, we usually want the whole pipeline.\n",
    "# For simplicity here, we log the model trained on the processed vectors.\n",
    "final_model = final_xgb.fit(clean_df.repartition(TRAIN_PARTITIONS).cache())\n",
    "\n",
    "with mlflow.start_run(run_name=\"POST_PRESENTATION_FINAL_BEST_XGB_MODEL\") as run:\n",
    "    mlflow.log_params(best_row.to_dict())\n",
    "    mlflow.spark.log_model(final_model, \"model\")\n",
    "    print(f\"Final model saved to MLflow Run: {run.info.run_id}\")\n",
    "\n",
    "# Cleanup\n",
    "dbutils.fs.rm(temp_materialize_path, recurse=True)\n",
    "clean_df.unpersist()\n",
    "full_cv_df.unpersist()\n",
    "\n",
    "\n",
    "\n",
    "# # --- 9. Retrain & Log BEST Model (STABILITY FIX) ---\n",
    "# print(\"Preparing data for Final Model...\")\n",
    "\n",
    "# # FIX 1: Materialize to disk first. \n",
    "# # This separates the \"Shuffle\" (heavy) from the \"Training\" (fragile).\n",
    "# temp_final_train_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/temp_xgb_final_train\"\n",
    "# clean_df.repartition(TRAIN_PARTITIONS).write.mode(\"overwrite\").parquet(temp_final_train_path)\n",
    "# final_train_data = spark.read.parquet(temp_final_train_path)\n",
    "\n",
    "# print(\"Retraining Final Model...\")\n",
    "\n",
    "# # FIX 2: Disable Autologging to prevent worker crashes\n",
    "# # XGBoost's barrier execution hates external hooks.\n",
    "# mlflow.spark.autolog(disable=True)\n",
    "\n",
    "# final_xgb = SparkXGBRegressor(\n",
    "#     features_col=\"features\",\n",
    "#     label_col=\"DEP_DELAY_NEW\",\n",
    "#     num_workers=6,\n",
    "#     tree_method=\"hist\",\n",
    "#     max_depth=best_depth,\n",
    "#     n_estimators=best_estimators,\n",
    "#     learning_rate=best_lr,\n",
    "#     missing=0.0\n",
    "# )\n",
    "\n",
    "# # Fit on the stable, materialized data\n",
    "# final_model = final_xgb.fit(final_train_data)\n",
    "\n",
    "# # Re-enable autologging (optional, for future cells)\n",
    "# mlflow.spark.autolog(disable=False)\n",
    "\n",
    "# with mlflow.start_run(run_name=\"POST_PRESENTATION_FINAL_BEST_XGB_MODEL\") as run:\n",
    "#     mlflow.log_params(best_row.to_dict())\n",
    "    \n",
    "#     # Log the model artifact manually\n",
    "#     mlflow.spark.log_model(final_model, \"model\")\n",
    "#     print(f\"Final model saved to MLflow Run: {run.info.run_id}\")\n",
    "\n",
    "# # Cleanup\n",
    "# dbutils.fs.rm(temp_materialize_path, recurse=True)\n",
    "# dbutils.fs.rm(temp_final_train_path, recurse=True)\n",
    "# clean_df.unpersist()\n",
    "# full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f492fbe-b9b0-47fe-88b1-69c48f9b24ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from functools import reduce\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# --- 1. Config ---\n",
    "TRAIN_PARTITIONS = 6 \n",
    "n_folds = 10\n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "cv_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/cv_splits\"\n",
    "output_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/stacked_input_optimized\"  \n",
    "\n",
    "# --- 2. Safety UDF ---\n",
    "@F.udf(returnType=VectorUDT())\n",
    "def sanitize_vector(v):\n",
    "    if v is None: return Vectors.dense([])\n",
    "    \n",
    "    # Convert to numpy for fast processing\n",
    "    arr = v.toArray()\n",
    "    \n",
    "    # Replace NaN, Infinity, -Infinity with 0.0\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Clip huge values to prevent float32 overflow in XGBoost\n",
    "    arr = np.clip(arr, -1e30, 1e30)\n",
    "    \n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "# --- 4. Global Data Loading ---\n",
    "print(f\"Loading 20% Sample from {month_or_year}...\")\n",
    "\n",
    "# 20% Sample (0.20) is the proven stable size\n",
    "full_cv_df = spark.read.parquet(cv_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20, seed=RANDOM_SEED) \\\n",
    "    .na.fill(0) \\\n",
    "    .repartition(480) \\\n",
    "    .cache()\n",
    "\n",
    "# --- 5. Feature Pipeline ---\n",
    "carrier_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "origin_indexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT_SEQ_ID\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "dest_indexer = StringIndexer(inputCol=\"DEST_AIRPORT_SEQ_ID\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "tail_num_indexer = StringIndexer(inputCol=\"TAIL_NUM\", outputCol=\"tail_num_idx\", handleInvalid=\"keep\")\n",
    "holiday_indexer = StringIndexer(inputCol=\"IS_HOLIDAY\", outputCol=\"holiday_idx\", handleInvalid=\"keep\")\n",
    "holiday_window_indexer = StringIndexer(inputCol=\"IS_HOLIDAY_WINDOW\", outputCol=\"holiday_window_idx\", handleInvalid=\"keep\")\n",
    "airport_hub_indexer = StringIndexer(inputCol=\"AIRPORT_HUB_CLASS\", outputCol=\"airport_hub_idx\", handleInvalid=\"keep\")\n",
    "airline_category_indexer = StringIndexer(inputCol=\"AIRLINE_CATEGORY\", outputCol=\"airline_category_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_idx\", outputCol=\"holiday_vec\")\n",
    "holiday_window_encoder = OneHotEncoder(inputCol=\"holiday_window_idx\", outputCol=\"holiday_window_vec\")\n",
    "airport_hub_encoder = OneHotEncoder(inputCol=\"airport_hub_idx\", outputCol=\"airport_hub_vec\")\n",
    "airline_category_encoder = OneHotEncoder(inputCol=\"airline_category_idx\", outputCol=\"airline_category_vec\")\n",
    "\n",
    "assembler_xgb = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights',\n",
    "        \"holiday_vec\",\n",
    "        \"holiday_window_vec\",\n",
    "        \"airport_hub_vec\",\n",
    "        \"RATING\",\n",
    "        \"airline_category_vec\"               # phase 3 new features end\n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    carrier_indexer, origin_indexer, dest_indexer, \n",
    "    tail_num_indexer, holiday_indexer, holiday_window_indexer, \n",
    "    airport_hub_indexer, airline_category_indexer,\n",
    "    \n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    tail_num_encoder, holiday_encoder, holiday_window_encoder, \n",
    "    airport_hub_encoder, airline_category_encoder,\n",
    "    \n",
    "    assembler_xgb\n",
    "])\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline...\")\n",
    "feat_model = pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming & Cleaning...\")\n",
    "# clean_df = feat_model.transform(full_cv_df) \\\n",
    "#     .filter(F.col(\"DEP_DELAY_NEW\").isNotNull()) \\\n",
    "#     .filter(~F.isnan(F.col(\"DEP_DELAY_NEW\"))) \\\n",
    "#     .filter(vector_is_valid(F.col(\"features\"))) \\\n",
    "#     .persist(StorageLevel.DISK_ONLY)\n",
    "clean_df = feat_model.transform(full_cv_df) \\\n",
    "    .withColumn(\"features\", sanitize_vector(F.col(\"raw_features\"))) \\\n",
    "    .filter(F.col(\"DEP_DELAY_NEW\").isNotNull()) \\\n",
    "    .filter(~F.isnan(F.col(\"DEP_DELAY_NEW\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {clean_df.count()} CLEAN rows for Stacking.\")\n",
    "\n",
    "# --- 6. Stacking Generation Loop ---\n",
    "out_of_fold_predictions = []\n",
    "# Updated history to track both Train and Val\n",
    "metrics_history = {\n",
    "    'train_mae': [], 'train_rmse': [],\n",
    "    'val_mae': [], 'val_rmse': []\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting Stacked Feature Generation (Depth={best_depth}, Est={best_estimators})...\")\n",
    "\n",
    "# we're retraining the same model here since the model we saved above has already seen the whole dataset, so we want to only generate results for the held-out folds, so we'll do that below\n",
    "with mlflow.start_run(run_name=\"POST_PRESENTATION_FINAL_BEST_XGB_STACKED_OUTPUT_5_YR_10CV\") as run: \n",
    "    # Log Parameters\n",
    "    mlflow.log_param(\"xgb_max_depth\", best_depth)\n",
    "    mlflow.log_param(\"xgb_n_estimators\", best_estimators)\n",
    "    mlflow.log_param(\"xgb_learning_rate\", best_lr)\n",
    "    mlflow.log_param(\"sample_fraction\", 0.20)\n",
    "    \n",
    "    # Define Base Model\n",
    "    xgb = SparkXGBRegressor(\n",
    "        features_col=\"features\",\n",
    "        label_col=\"DEP_DELAY_NEW\",\n",
    "        num_workers=6,\n",
    "        tree_method=\"hist\", \n",
    "        max_depth=best_depth,\n",
    "        n_estimators=best_estimators,\n",
    "        learning_rate=best_lr,\n",
    "        missing=0.0\n",
    "    )\n",
    "\n",
    "    for fold_id in range(1, n_folds + 1):\n",
    "        print(f\"\\nProcessing Fold {fold_id}/{n_folds}...\")\n",
    "        \n",
    "        # 1. Prepare Training Data\n",
    "        train_vec = clean_df.filter((F.col(\"fold_id\") != fold_id)) \\\n",
    "            .sample(withReplacement=False, fraction=0.20) \\\n",
    "            .repartition(TRAIN_PARTITIONS)\n",
    "        \n",
    "        # 2. Prepare Validation Data\n",
    "        val_vec = clean_df.filter((F.col(\"fold_id\") == fold_id))\n",
    "        \n",
    "        # 3. Fit Model\n",
    "        model = xgb.fit(train_vec)\n",
    "        \n",
    "        # 4. TRAINING METRICS (New)\n",
    "        train_preds = model.transform(train_vec)\n",
    "        t_mae = mae_evaluator.evaluate(train_preds)\n",
    "        t_rmse = rmse_evaluator.evaluate(train_preds)\n",
    "        \n",
    "        metrics_history['train_mae'].append(t_mae)\n",
    "        metrics_history['train_rmse'].append(t_rmse)\n",
    "        \n",
    "        # 5. VALIDATION METRICS\n",
    "        val_preds = model.transform(val_vec)\n",
    "        v_mae = mae_evaluator.evaluate(val_preds)\n",
    "        v_rmse = rmse_evaluator.evaluate(val_preds)\n",
    "        \n",
    "        metrics_history['val_mae'].append(v_mae)\n",
    "        metrics_history['val_rmse'].append(v_rmse)\n",
    "        \n",
    "        print(f\"  Fold {fold_id}: Train MAE={t_mae:.2f} | Val MAE={v_mae:.2f}\")\n",
    "        print(f\"  Fold {fold_id}: Train RMSE={t_rmse:.2f} | Val RMSE={v_rmse:.2f}\")\n",
    "        \n",
    "        # Log Fold Metrics\n",
    "        # mlflow.log_metric(f\"fold_{fold_id}_train_mae\", t_mae)\n",
    "        # mlflow.log_metric(f\"fold_{fold_id}_val_mae\", v_mae)\n",
    "        # mlflow.log_metric(f\"fold_{fold_id}_train_rmse\", t_rmse)\n",
    "        # mlflow.log_metric(f\"fold_{fold_id}_val_rmse\", v_rmse)\n",
    "        \n",
    "        # 6. Format Output (Drop features to save space)\n",
    "        val_preds_clean = val_preds \\\n",
    "            .withColumnRenamed(\"prediction\", \"xgb_predicted_delay\") \\\n",
    "            .drop(\"features\") \n",
    "        \n",
    "        out_of_fold_predictions.append(val_preds_clean)\n",
    "\n",
    "    # Log Average Metrics\n",
    "    mlflow.log_metric(\"avg_train_mae\", np.mean(metrics_history['train_mae']))\n",
    "    mlflow.log_metric(\"avg_train_rmse\", np.mean(metrics_history['train_rmse']))\n",
    "    mlflow.log_metric(\"avg_val_mae\", np.mean(metrics_history['val_mae']))\n",
    "    mlflow.log_metric(\"avg_val_rmse\", np.mean(metrics_history['val_rmse']))\n",
    "    \n",
    "    # --- 7. Train & Log FINAL Model ---\n",
    "    print(\"\\nTraining Final XGBoost Model on Full Data...\")\n",
    "    \n",
    "    final_train = clean_df.repartition(TRAIN_PARTITIONS)\n",
    "    final_model = xgb.fit(final_train)\n",
    "    \n",
    "    mlflow.spark.log_model(final_model, \"stacked_xgb_model\")\n",
    "    print(f\"Final Model Saved to MLflow Run: {run.info.run_id}\")\n",
    "\n",
    "    # --- 8. Save Stacked Dataset ---\n",
    "    print(\"\\nUnioning and Saving Stacked Dataset...\")\n",
    "    \n",
    "    stacked_dataset = reduce(lambda df1, df2: df1.union(df2), out_of_fold_predictions)\n",
    "\n",
    "    cols_to_drop = [\n",
    "        \"features\", \"raw_features\", \n",
    "        \"carrier_vec\", \"origin_vec\", \"dest_vec\", \"tail_num_vec\", \n",
    "        \"holiday_vec\", \"holiday_window_vec\", \"airport_hub_vec\", \"airline_category_vec\"\n",
    "    ]\n",
    "    final_output = stacked_dataset.drop(*cols_to_drop)\n",
    "\n",
    "    final_output.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"SUCCESS: Stacked dataset saved to {output_path}\")\n",
    "\n",
    "# Cleanup\n",
    "clean_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcc6ee5-06bd-48e2-a877-355009bd9514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import BooleanType\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Optimization Config ---\n",
    "OPTIMAL_PARTITIONS = 480 \n",
    "# We use this for repartitioning inside the loop to keep tasks small and fast for MLP\n",
    "TRAIN_PARTITIONS = 480 \n",
    "n_folds = 10 \n",
    "month_or_year = \"5_year_custom_joined\"\n",
    "stacked_input_path = f\"dbfs:/student-groups/Group_2_2/{month_or_year}/stacked_input_optimized\"\n",
    "\n",
    "# --- 2. Safety UDF ---\n",
    "@F.udf(returnType=BooleanType())\n",
    "def vector_is_valid(v):\n",
    "    if v is None: return False\n",
    "    if np.any(np.isinf(v.values)): return False\n",
    "    if np.any(np.isnan(v.values)): return False\n",
    "    if np.max(np.abs(v.values)) > 1e30: return False\n",
    "    return True\n",
    "\n",
    "# --- 3. Global Data Loading & Preprocessing ---\n",
    "print(f\"Loading and optimizing data from {month_or_year}...\")\n",
    "\n",
    "# Sample 20%\n",
    "full_cv_df = spark.read.parquet(stacked_input_path) \\\n",
    "    .sample(withReplacement=False, fraction=0.20, seed=RANDOM_SEED) \\\n",
    "    .repartition(OPTIMAL_PARTITIONS) \\\n",
    "    .cache()\n",
    "\n",
    "# Define Pipeline Stages\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_idx\", outputCol=\"carrier_vec\")\n",
    "origin_encoder = OneHotEncoder(inputCol=\"origin_idx\", outputCol=\"origin_vec\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_idx\", outputCol=\"dest_vec\")\n",
    "tail_num_encoder = OneHotEncoder(inputCol=\"tail_num_idx\", outputCol=\"tail_num_vec\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_idx\", outputCol=\"holiday_vec\")\n",
    "holiday_window_encoder = OneHotEncoder(inputCol=\"holiday_window_idx\", outputCol=\"holiday_window_vec\")\n",
    "airport_hub_encoder = OneHotEncoder(inputCol=\"airport_hub_idx\", outputCol=\"airport_hub_vec\")\n",
    "airline_category_encoder = OneHotEncoder(inputCol=\"airline_category_idx\", outputCol=\"airline_category_vec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"QUARTER\",\n",
    "        \"MONTH\", \n",
    "        \"YEAR\",\n",
    "        \"DAY_OF_MONTH\",\n",
    "        \"DAY_OF_WEEK\",\n",
    "        \"carrier_vec\",\n",
    "        \"origin_vec\",\n",
    "        \"dest_vec\",\n",
    "        \"CRS_ELAPSED_TIME\",\n",
    "        \"DISTANCE\",\n",
    "        # \"tail_num_vec\",\n",
    "        \"CRS_DEP_MINUTES\",                 # feature eng start\n",
    "        \"prev_flight_delay_in_minutes\",\n",
    "        \"prev_flight_delay\",\n",
    "        \"origin_delays_4h\",\n",
    "        \"delay_origin_7d\",\n",
    "        \"delay_origin_carrier_7d\",\n",
    "        \"delay_route_7d\",\n",
    "        \"flight_count_24h\",\n",
    "        \"LANDING_TIME_DIFF_MINUTES\",\n",
    "        \"AVG_ARR_DELAY_ORIGIN\",\n",
    "        \"AVG_TAXI_OUT_ORIGIN\",              # feature eng end\n",
    "        'HourlyDryBulbTemperature',         # weather start\n",
    "        'HourlyDewPointTemperature',\n",
    "        'HourlyRelativeHumidity',\n",
    "        'HourlyAltimeterSetting',\n",
    "        'HourlyVisibility',\n",
    "        'HourlyStationPressure',\n",
    "        'HourlyWetBulbTemperature',\n",
    "        'HourlyPrecipitation',\n",
    "        'HourlyCloudCoverage',\n",
    "        'HourlyCloudElevation',\n",
    "        'HourlyWindSpeed',                   # weather end\n",
    "        'page_rank',               # phase 3 new features start\n",
    "        'out_degree',\n",
    "        'in_degree',\n",
    "        'weighted_out_degree',\n",
    "        'weighted_in_degree',\n",
    "        'N_RUNWAYS',\n",
    "        'betweenness_unweighted',\n",
    "        'closeness',\n",
    "        'betweenness',\n",
    "        'avg_origin_dep_delay',\n",
    "        'avg_dest_arr_delay',\n",
    "        'avg_daily_route_flights',\n",
    "        'avg_route_delay',\n",
    "        'avg_hourly_flights',\n",
    "        \"holiday_vec\",\n",
    "        \"holiday_window_vec\",\n",
    "        \"airport_hub_vec\",\n",
    "        \"RATING\",\n",
    "        \"airline_category_vec\",               # phase 3 new features end\n",
    "        \"xgb_predicted_delay\" \n",
    "    ],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Build & Fit Pipeline ONCE\n",
    "global_pipeline = Pipeline(stages=[\n",
    "    carrier_encoder, origin_encoder, dest_encoder, \n",
    "    tail_num_encoder, holiday_encoder, holiday_window_encoder, \n",
    "    airport_hub_encoder, airline_category_encoder,\n",
    "    \n",
    "    assembler, \n",
    "    scaler\n",
    "])\n",
    "\n",
    "print(\"Fitting Global Feature Pipeline (One-Time Fit)...\")\n",
    "feat_model = global_pipeline.fit(full_cv_df)\n",
    "\n",
    "print(\"Transforming & Caching Vectors...\")\n",
    "featurized_df = feat_model.transform(full_cv_df) \\\n",
    "    .select(\"scaled_features\", \"DEP_DEL15\", \"fold_id\", \"split_type\") \\\n",
    "    .filter(vector_is_valid(F.col(\"scaled_features\"))) \\\n",
    "    .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Materialized {featurized_df.count()} rows for Architecture Search.\")\n",
    "\n",
    "# --- 4. Define Architectures ---\n",
    "hidden_layer_grid = [\n",
    "    # [32],               \n",
    "    [64, 32],           \n",
    "    [128, 64],          \n",
    "    [64, 32, 16]\n",
    "]\n",
    "\n",
    "# --- 5. Evaluators ---\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"f1\")\n",
    "f2_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedFMeasure\", beta=2.0)\n",
    "f2_label_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"fMeasureByLabel\", metricLabel=1.0, beta=2.0)\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", metricName=\"weightedRecall\")\n",
    "\n",
    "# --- 6. Tuning Loop ---\n",
    "results_list = []\n",
    "\n",
    "print(f\"Starting MLP Architecture Search (Target: Delay-Class F2)...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"POST_PRESENTATION_MLP_5YR_STACKED_BALANCED_10CV\") as parent_run: \n",
    "    \n",
    "    input_dim = len(featurized_df.first()[\"scaled_features\"])\n",
    "    print(f\"Detected Input Dimension: {input_dim}\")\n",
    "    mlflow.log_param(\"input_dim\", input_dim)\n",
    "    \n",
    "    for idx, hidden_config in enumerate(hidden_layer_grid):\n",
    "        \n",
    "        config_str = \"-\".join(map(str, hidden_config)) \n",
    "        print(f\"\\n--- Testing Arch {idx+1}/{len(hidden_layer_grid)}: Hidden=[{config_str}] ---\")\n",
    "        \n",
    "        # Track both Train and Val metrics\n",
    "        fold_metrics = {\n",
    "            'train_f1': [], 'train_f2': [], 'train_f2_label': [], 'train_precision': [], 'train_recall': [],\n",
    "            'val_f1': [], 'val_f2': [], 'val_f2_label': [], 'val_precision': [], 'val_recall': []\n",
    "        }\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"arch_{config_str}\", nested=True) as child_run:\n",
    "            mlflow.log_param(\"hidden_layers\", str(hidden_config))\n",
    "            \n",
    "            for fold_id in range(1, n_folds + 1):\n",
    "                # 1. Split Data\n",
    "                train_raw = featurized_df.filter((F.col(\"fold_id\") != fold_id))\n",
    "                val_vec = featurized_df.filter((F.col(\"fold_id\") == fold_id))\n",
    "                \n",
    "                # 2. BALANCE TRAINING DATA\n",
    "                train_pos = train_raw.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "                train_neg = train_raw.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "                \n",
    "                pos_count = train_pos.count()\n",
    "                neg_count = train_neg.count()\n",
    "                fraction = pos_count / neg_count\n",
    "                \n",
    "                train_neg_sampled = train_neg.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "                train_balanced = train_pos.union(train_neg_sampled).repartition(TRAIN_PARTITIONS)\n",
    "                \n",
    "                print(f\"    Fold {fold_id}: Training on Balanced Data ({pos_count} Pos)\")\n",
    "                \n",
    "                # 3. Define & Fit\n",
    "                full_layers = [input_dim] + hidden_config + [2]\n",
    "                \n",
    "                mlp = MultilayerPerceptronClassifier(\n",
    "                    featuresCol=\"scaled_features\",\n",
    "                    labelCol=\"DEP_DEL15\",\n",
    "                    layers=full_layers,\n",
    "                    blockSize=128,\n",
    "                    maxIter=100,\n",
    "                    stepSize=0.03\n",
    "                )\n",
    "                \n",
    "                mlp_model = mlp.fit(train_balanced)\n",
    "                \n",
    "                # 4. Predict & Evaluate (TRAIN - Check Overfitting)\n",
    "                train_preds = mlp_model.transform(train_balanced).select(\"prediction\", \"DEP_DEL15\")\n",
    "                \n",
    "                # 5. Predict & Evaluate (VAL - Check Performance)\n",
    "                val_preds = mlp_model.transform(val_vec).select(\"prediction\", \"DEP_DEL15\")\n",
    "                \n",
    "                metrics = {\n",
    "                    \"val_f1\": f1_evaluator.evaluate(val_preds),\n",
    "                    \"val_f2\": f2_evaluator.evaluate(val_preds),\n",
    "                    \"val_f2_label\": f2_label_evaluator.evaluate(val_preds),\n",
    "                    \"val_precision\": precision_evaluator.evaluate(val_preds),\n",
    "                    \"val_recall\": recall_evaluator.evaluate(val_preds),\n",
    "                    \"train_f1\": f1_evaluator.evaluate(train_preds),\n",
    "                    \"train_f2\": f2_evaluator.evaluate(train_preds),\n",
    "                    \"train_f2_label\": f2_label_evaluator.evaluate(train_preds),\n",
    "                    \"train_precision\": precision_evaluator.evaluate(train_preds),\n",
    "                    \"train_recall\": recall_evaluator.evaluate(train_preds)\n",
    "                }\n",
    "                \n",
    "                # Log fold metrics\n",
    "                # mlflow.log_metrics(metrics)\n",
    "                \n",
    "                print(f\"    Result: Train F2-Delay={metrics['train_f2_label']:.4f} | Val F2-Delay={metrics['val_f2_label']:.4f}\")\n",
    "                \n",
    "                for k in fold_metrics.keys():\n",
    "                    fold_metrics[k].append(metrics[k])\n",
    "            \n",
    "            # Aggregate Results\n",
    "            avg_metrics = {f\"avg_{k}\": np.mean(v) for k, v in fold_metrics.items()}\n",
    "            mlflow.log_metrics(avg_metrics)\n",
    "            \n",
    "            results_list.append({\n",
    "                \"hidden_config\": str(hidden_config),\n",
    "                \"avg_train_f2_label\": avg_metrics['avg_train_f2_label'],\n",
    "                \"avg_val_f2_label\": avg_metrics['avg_val_f2_label']\n",
    "            })\n",
    "            \n",
    "            print(f\"  Arch [{config_str}] Final Train Delay F2 = {avg_metrics['avg_train_f2_label']:.4f}\")\n",
    "            print(f\"  Arch [{config_str}] Final Val Delay F2 = {avg_metrics['avg_val_f2_label']:.4f}\")\n",
    "\n",
    "# --- 7. Select Winner & Log Final Model ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "best_row = results_df.loc[results_df['avg_val_f2_label'].idxmax()] \n",
    "best_config_str = best_row['hidden_config'] # e.g. \"[64, 32]\"\n",
    "best_config_list = eval(best_config_str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"WINNER FOUND: {best_config_str}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Retraining Final Stacked Model...\")\n",
    "\n",
    "# 1. Balance Full Dataset (Downsample global majority)\n",
    "full_pos = featurized_df.filter(F.col(\"DEP_DEL15\") == 1.0)\n",
    "full_neg = featurized_df.filter(F.col(\"DEP_DEL15\") == 0.0)\n",
    "fraction = full_pos.count() / full_neg.count()\n",
    "\n",
    "# Re-combine to get the Training Set for the Final Model\n",
    "full_balanced = full_pos.union(full_neg.sample(False, fraction, seed=42)).repartition(OPTIMAL_PARTITIONS)\n",
    "\n",
    "# 2. Train Final Model\n",
    "final_layers = [input_dim] + best_config_list + [2]\n",
    "final_mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    layers=final_layers,\n",
    "    blockSize=128,\n",
    "    maxIter=100,\n",
    "    stepSize=0.03\n",
    ")\n",
    "final_model = final_mlp.fit(full_balanced)\n",
    "\n",
    "# --- NEW: Calculate Final Training Metrics ---\n",
    "print(\"Calculating Final Training Metrics...\")\n",
    "final_train_preds = final_model.transform(full_balanced).select(\"prediction\", \"DEP_DEL15\").cache()\n",
    "final_train_preds.count() # Materialize\n",
    "\n",
    "final_metrics = {\n",
    "    \"final_train_f1\": f1_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_f2\": f2_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_f2_label\": f2_label_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_precision\": precision_evaluator.evaluate(final_train_preds),\n",
    "    \"final_train_recall\": recall_evaluator.evaluate(final_train_preds)\n",
    "}\n",
    "\n",
    "print(f\"Final Model Training F2 (Delay Class): {final_metrics['final_train_f2_label']:.4f}\")\n",
    "\n",
    "# 3. Log to MLflow\n",
    "with mlflow.start_run(run_name=\"POST_PRESENTATION_FINAL_BEST_STACKED_MLP\") as run: \n",
    "    # Log Params\n",
    "    mlflow.log_param(\"hidden_layers\", best_config_str)\n",
    "    \n",
    "    # Log Metrics (NEW)\n",
    "    mlflow.log_metrics(final_metrics)\n",
    "    \n",
    "    # Log Model Artifact\n",
    "    mlflow.spark.log_model(final_model, \"model\")\n",
    "    print(f\"Final Stacked Model & Metrics saved to Run: {run.info.run_id}\")\n",
    "\n",
    "# Cleanup\n",
    "final_train_preds.unpersist()\n",
    "featurized_df.unpersist()\n",
    "full_cv_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d458a050-f3f2-4ae4-ba85-6cb75a710190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "post_presentation_5yr_stacked_NN_with_CV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
